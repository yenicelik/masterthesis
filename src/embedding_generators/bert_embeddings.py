"""
    Implements word-embeddings that are generated by the BERT lanauge model.
    We will use the standard BERT language model (not distilled), as this is better captured by language

    # TODO: Later we can perhaps expand this to any language model
    (assuming we can generate embeddings with every language model)

    # What is the difference between BertMaskedModel and BertModel

    Seems to be a good resource on how to extract word-embeddings
        https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#23-segment-id
"""
import torch

from src.language_models.model_wrappers.bert_wrapper import BertWrapper
from src.resources.corpus import Corpus


class BertEmbedding:

    @property
    def sentence_sample(self):
        """
            The number of sentences to sample for each word,
            s.t. we can sample a probability distribution from the context-word

            We do this for a few language only (i.e. German, English, Turkish??),
            and we use the Wikipedia corpus, as this is available in many language "for free"

            # TODO: ->
                Following Artetxe, can we sample a few sentences (using multinomial sampling),
                and then use this for the sample-embedding?

            # TODO: ->
                How do embeddings change based on corpus?

            NOTE: I will start with a small corpus first, and then slowly scale up.
            A larger corpus may be included on Leonhard or so (or on my private cluster lol)

            Will download the corpus from here:
                https://www.english-corpora.org/
                https://www.corpusdata.org/

            # Find out what the stupid paper used as a corpora..
        :return:
        """
        pass

    def __init__(self):
        self.corpus = Corpus()
        self.max_samples = 10

        self.wrapper = BertWrapper()
        self.bert_layer = 1  # Which BERT layer to take the embeddings from

    def _sample_sentence_including_word_from_corpus(self, word):
        """
            The Corpus is some corpus that
            Probably better ways to parse this
        :return:
        """
        out = ["[CLS] " + x for x in self.corpus.sentences if word in x][:self.max_samples]
        # Must not allow any words that happen less than 5 times!
        assert len(out) == self.max_samples, ("Not enough examples found for this word!", out, word)
        return out

    def get_embedding(self, word):
        """
            For a given word (or concept), we want to generate an embedding.
            The question is also, do we generate probabilistic embeddings or static ones (by pooling..?)
            TODO: Perhaps the probabilistic embeddings can overcome the static ones?
        :param token:
        :return:
        """
        assert isinstance(word, str), ("Word is not of type string", word)

        word = word.lower()

        # 1. Sample k sentences that include this word w
        print("Sample the sentences which are included in this corpus")
        sample_sentences = self._sample_sentence_including_word_from_corpus(word)
        print("Sample sentences are: ")
        print(sample_sentences)
        print(len(sample_sentences))

        # Tokenize the word, and find it in the array
        tokenized_word = self.wrapper.tokenizer.tokenize(word)


        # 2. Tokenize the sentences
        # Perhaps can be accelerated if needed
        # word_idx = None
        # window = None
        # new_sample_sentences = []
        # for sentence in sample_sentences:
        #     for idx, x in enumerate(sample_sentences):
        #         tokenized_word = self.wrapper.tokenizer.tokenize(x)
        #         new_sample_sentences.append(tokenized_word)
        #         print("X and word are: ", x, word)
        #         if x == word:
        #             word_idx = idx
        #             window = len(tokenized_word)
        #             print("Word and Window is: ", word_idx, window, word, tokenized_word)

        # assert window
        # assert word_idx
        #
        # sample_sentences = new_sample_sentences

        sample_sentences = [self.wrapper.tokenizer.tokenize(x) for x in sample_sentences]
        print(sample_sentences)

        # Find the index at which the corresponding word occurs...

        # 3. Run through language model, look at how the other paper reprocuded generating embeddings for word using BERT
        for sentence in sample_sentences:
            # We only sample per sentence, so it is always the same segment...
            segments_ids = [0, ] * len(sentence)
            indexed_tokens = self.wrapper.tokenizer.convert_tokens_to_ids(sentence)

            # TODO: Do I have to include the token that I want to predict, or mask it out...

            # print("segment ids and indexed tokens")
            # print(segments_ids)
            # print(indexed_tokens)

            # Now convert to pytorch tensors..
            tokens_tensor = torch.tensor([indexed_tokens])
            segments_tensors = torch.tensor([segments_ids])

            # Retrieve the embeddings at layer no `self.bert_layer`

            # TODO: Somehow not all layers are returned... check this out lol
            # Take the outputs of the forward body lol
            outputs = self.wrapper.forward(
                tokens_tensor=tokens_tensor,
                segments_tensors=segments_tensors
            )

            # Pick the one with output at position MASK ...

            # print("Outputs are: ", outputs)

        # Now query the embeddings for the predicted word (do we just mask that word..?)

        # Now


if __name__ == "__main__":
    print("Starting to generate embeddings from the BERT model")
    embeddings = BertEmbedding()

    example_words = ["the", "heart", "bank"]
    for word in example_words:
        embeddings.get_embedding(word)
