"""
    Implements word-embeddings that are generated by the BERT lanauge model.
    We will use the standard BERT language model (not distilled), as this is better captured by language

    # TODO: Later we can perhaps expand this to any language model
    (assuming we can generate embeddings with every language model)
"""
import re

from src.language_models.model_wrappers.bert_wrapper import BertWrapper
from src.resources.corpus import Corpus

class BertEmbedding:

    @property
    def sentence_sample(self):
        """
            The number of sentences to sample for each word,
            s.t. we can sample a probability distribution from the context-word

            We do this for a few language only (i.e. German, English, Turkish??),
            and we use the Wikipedia corpus, as this is available in many language "for free"

            # TODO: ->
                Following Artetxe, can we sample a few sentences (using multinomial sampling),
                and then use this for the sample-embedding?

            # TODO: ->
                How do embeddings change based on corpus?

            NOTE: I will start with a small corpus first, and then slowly scale up.
            A larger corpus may be included on Leonhard or so (or on my private cluster lol)

            Will download the corpus from here:
                https://www.english-corpora.org/
                https://www.corpusdata.org/

            # Find out what the stupid paper used as a corpora..
        :return:
        """
        pass

    def __init__(self):
        self.corpus = Corpus()
        self.max_samples = 10

        self.wrapper = BertWrapper()

    def _sample_sentence_including_word_from_corpus(self, word):
        """
            The Corpus is some corpus that
            Probably better ways to parse this
        :return:
        """
        out = [x for x in self.corpus.sentences if word in x][:self.max_samples]
        assert len(out) == self.max_samples, ("Not enough examples found for this word!", out, word)
        return out

    def get_embedding(self, word):
        """
            For a given word (or concept), we want to generate an embedding.
            The question is also, do we generate probabilistic embeddings or static ones (by pooling..?)
            TODO: Perhaps the probabilistic embeddings can overcome the static ones?
        :param token:
        :return:
        """
        assert isinstance(word, str), ("Word is not of type string", word)

        word = word.lower()

        # 1. Sample k sentences that include this word w
        print("Sample the sentences which are included in this corpus")
        sample_sentences = self._sample_sentence_including_word_from_corpus(word)
        print("Sample sentences are: ")
        print(sample_sentences)
        print(len(sample_sentences))

        # 2. Tokenize the sentences
        sample_sentences = [self.wrapper.tokenizer.tokenize(x) for x in sample_sentences]
        print(sample_sentences)

        # 3. Run through language model, look at how the other paper reprocuded generating embeddings for word using BERT

        # Now query the embeddings for the predicted word (do we just mask that word..?)



if __name__ == "__main__":
    print("Starting to generate embeddings from the BERT model")
    embeddings = BertEmbedding()

    example_words = ["the", "heart", "bank"]
    for word in example_words:
        embeddings.get_embedding(word)
