\relax 
\providecommand \oddpage@label [2]{}
\citation{mikolov13}
\citation{mikolov13b}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Motivation}{3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{harris54}
\citation{harris54}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Background}{5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Word Embeddings}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Word-Embeddings}{5}\protected@file@percent }
\newlabel{map:embedding_mapping}{{3.1}{6}}
\@writefile{toc}{\contentsline {paragraph}{Distance}{6}\protected@file@percent }
\newlabel{def:distance}{{3.1}{6}}
\citation{bengio03}
\@writefile{toc}{\contentsline {paragraph}{Learning a distance}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Going from the distributional structure of sentences to learning distances between words.}{7}\protected@file@percent }
\newlabel{eq:naive_sequential_probability}{{3.1}{7}}
\citation{bengio03}
\newlabel{eq:naive_sequential_probability}{{3.1}{8}}
\@writefile{toc}{\contentsline {paragraph}{However, we do not need to only look at only the previous words}{8}\protected@file@percent }
\newlabel{eq:basic_equation_log_maximization}{{3.1}{8}}
\citation{harris54}
\citation{mikolov13}
\citation{mikolov13}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Figure taken from \cite  {mikolov13}. The CBOW architecture predicts the current word based on the context. The Skip-gram predicts surrounding words given the current word.}}{9}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{3.1}{9}}
\newlabel{eq:basic_equation_log_maximization_skipgram}{{3.1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Static Word Embeddings}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Basic Model}{10}\protected@file@percent }
\citation{mikolov13}
\citation{mikolov13b}
\citation{mikolov13b}
\citation{mikolov13b}
\citation{pennington14}
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Figure taken from \cite  {mikolov13b}. A 2-dimensional PCA projection of the 1000-dimensional skip-gram vectors of countries and their capital cities. The proposed model is able to automatically organize concepts and learn implicit relationships between them. No supervised information was provided about what a capital city means.}}{12}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{3.2}{12}}
\@writefile{toc}{\contentsline {subsubsection}{GloVe}{12}\protected@file@percent }
\citation{vilnis14}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Embeddings}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Symmetric similarity: expected likelihood or probability product kernel}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Asymmetric divergence: KL-Divergence}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Uncertainty calculation:}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Context Embeddings}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Transformer Architecture}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ELMo}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{BERT}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{GPT and GPT-2}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Other methods}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generating "static" word-embeddings through contextual embeddings}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Resources and Datasets}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{WordNet}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{SemCor dataset}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{News dataset}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{GLUE benchmark dataset}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Related Work}{21}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Structure inside BERT}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Metric Learning and Disentanglement}{23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Zero shot and One shot learning }{23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Clustering Algorithms}{23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Applications of word vector}{23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{24}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Analysis of the current state of the art}{27}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}On the Linear Separability of meaning within sampled BERT vectors}{27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Motivation}{27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Experiment setup}{28}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces How to write algorithms}}{28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Results}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}On the Clusterability of meaning within sampled BERT vectors}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Motivation}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Experiment setup}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Results}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Correlation between Part of Speech and Context within BERT}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Motivation}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Experiment setup}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Results}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Our Method}{31}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.1}BERnie PoS}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.2}BERnie Meaning}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.3}BERnie Meaning with additional pre-training}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.4}Compressing the non-lexical out}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Further Work}{33}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Evaluation}{35}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibstyle{unsrt}
\bibdata{dissertation}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Conclusion}{37}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
