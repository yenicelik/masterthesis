\relax 
\providecommand \oddpage@label [2]{}
\citation{mikolov13}
\citation{mikolov13b}
\citation{colahLSTM}
\citation{vaswani17}
\citation{devlin18}
\citation{devlin18}
\citation{devlin18}
\citation{devlin18}
\citation{miller90}
\citation{kudugunta18}
\citation{suarez19}
\citation{kaya19}
\citation{conneau17}
\citation{wang19}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.0.1}Main Contributions}{4}\protected@file@percent }
\citation{harris54}
\citation{harris54}
\citation{hegel17}
\citation{wittgenstein53}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{spacyb}
\citation{spacy}
\citation{wang19}
\citation{wang19}
\citation{wang19}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Linguistic Features}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Polysemy}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Part of Speech \textit  {PoS}}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Other linguistic features}{7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Taken from \cite  {wang19} Table to test captions and labels\relax }}{7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:1}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Tokens and n-grams}{7}\protected@file@percent }
\citation{sutskever11}
\citation{lee17}
\citation{gage94}
\citation{sennrich16}
\citation{wu16}
\@writefile{toc}{\contentsline {paragraph}{Character}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Byte pair encodings}{8}\protected@file@percent }
\citation{ganea18}
\@writefile{toc}{\contentsline {paragraph}{WordPiece tokenizer}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Word Embeddings}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Word-Embeddings}{9}\protected@file@percent }
\newlabel{map:embedding_mapping}{{2.2}{9}}
\@writefile{toc}{\contentsline {paragraph}{Distance:}{10}\protected@file@percent }
\newlabel{def:distance}{{2.2}{10}}
\@writefile{toc}{\contentsline {paragraph}{Learning a distance}{10}\protected@file@percent }
\citation{bengio03}
\@writefile{toc}{\contentsline {paragraph}{Exploiting the distributional structure of language:}{11}\protected@file@percent }
\newlabel{eq:naive_sequential_probability}{{2.2}{11}}
\newlabel{eq:naive_sequential_probability_markovian}{{2.2}{11}}
\citation{harris54}
\newlabel{eq:naive_sequential_probability}{{2.2}{12}}
\@writefile{toc}{\contentsline {paragraph}{Loss Objective}{12}\protected@file@percent }
\newlabel{eq:basic_equation_log_maximization}{{2.2}{12}}
\citation{mikolov13}
\citation{mikolov13}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Figure taken from \cite  {mikolov13}. The CBOW architecture predicts the current word based on the context. The Skip-gram predicts surrounding words given the current word.\relax }}{13}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{2.1}{13}}
\newlabel{eq:basic_equation_log_maximization_skipgram}{{2.2}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Static Word Embeddings}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Basic Model}{15}\protected@file@percent }
\citation{mikolov13}
\citation{mikolov13b}
\citation{mikolov13b}
\citation{mikolov13b}
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{16}\protected@file@percent }
\citation{pennington14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Figure taken from \cite  {mikolov13b}. A 2-dimensional PCA projection of the 1000-dimensional skip-gram vectors of countries and their capital cities. The proposed model is able to automatically organize concepts and learn implicit relationships between them. No supervised information was provided about what a capital city means.\relax }}{17}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{2.2}{17}}
\@writefile{toc}{\contentsline {subsubsection}{GloVe}{17}\protected@file@percent }
\citation{bojanowski17}
\citation{bengio06}
\citation{vilnis14}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Embeddings}{18}\protected@file@percent }
\citation{vilnis14}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Context Embeddings}{22}\protected@file@percent }
\newlabel{map:context_embedding_mapping}{{2.2.2}{22}}
\newlabel{eq:transformer_probability}{{2.2.2}{22}}
\citation{peters17b}
\citation{hochreiter97}
\citation{rumelhart85}
\citation{colahLSTM}
\citation{colahLSTM}
\citation{hochreiter97}
\@writefile{toc}{\contentsline {subsubsection}{ELMo}{23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Figure taken from \cite  {colahLSTM}. The internals of the LSTM cell, and the recurrent flow which is repeated for three consecutive timesteps $t-1, t, t+1$. The LSTM produces an hidden representation at every steep $h_{t-1}, h_{t}, h_{t+1}$ given some inputs $x_{t-1}, x_t, x_{t+1}$. \relax }}{23}\protected@file@percent }
\newlabel{fig:lstm_internals}{{2.3}{23}}
\citation{vaswani17}
\@writefile{toc}{\contentsline {subsubsection}{The Transformer Architecture}{25}\protected@file@percent }
\citation{vaswani17}
\citation{vaswani17}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Figure taken from \cite  {vaswani17}. The transformer module architecture. The transformer encapsulates multiple attention layers.\relax }}{26}\protected@file@percent }
\newlabel{fig:attention_is_all_you_need}{{2.4}{26}}
\citation{devlin18}
\citation{devlin18}
\citation{devlin18}
\@writefile{toc}{\contentsline {subsubsection}{BERT}{27}\protected@file@percent }
\citation{devlin18}
\citation{devlin18}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Figure taken from \cite  {devlin18}. BERT uses a bidirectional transformer, which is not limited to reading in all the input from only left to right or right to left. OpenAI GPT (next section) uses a left-to-right Transformer, while ELMo is using a bidirectional LSTM which naturally captures a direction. \relax }}{28}\protected@file@percent }
\newlabel{fig:attention_is_all_you_need}{{2.5}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Example from \cite  {devlin18}. An input sentence which where 15\% of the tokens are replaced with the [MASK] token. During pre-training, the weights of the BERT model are optimized in such a way to predict the true underlying words. The word to be predicted is \textit  {the} \relax }}{28}\protected@file@percent }
\citation{devlin18}
\citation{devlin18}
\citation{devlin18}
\citation{devlin18}
\citation{sanh19}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Example from \cite  {devlin18}. Two input sequences which where 15\% of the tokens are replaced with the [MASK] token. During pre-training, the weights of the BERT model are optimized in such a way to predict the true underlying words. In this case, the second sentence is a continuation of the first one, and thus the label would be \textit  {isNext}. \relax }}{29}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Figure taken from \cite  {devlin18}. BERT takes as input multiple tokens, including a position embedding, the token embedding and the segment embedding. This allows BERT to distinguish between the location of the word within a sentence, and which word token was provided and which sentence the word token is a part of.\relax }}{29}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{2.8}{29}}
\citation{shen19}
\citation{radford18}
\citation{radford19}
\citation{radford18}
\citation{Liu18}
\citation{vaswani17}
\@writefile{toc}{\contentsline {subsubsection}{GPT and GPT-2}{30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Unsupervised pre-training}{30}\protected@file@percent }
\citation{Rei17}
\citation{peters17a}
\citation{sennrich15}
\citation{ba16}
\@writefile{toc}{\contentsline {paragraph}{Supervised fine-tuning}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Other methods}{32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generating "static" word-embeddings through contextual embeddings}{32}\protected@file@percent }
\citation{miller90}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Resources and Datasets}{34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{WordNet}{34}\protected@file@percent }
\citation{miller90}
\citation{miller90}
\citation{miller90}
\@writefile{toc}{\contentsline {paragraph}{Lexical matrix:}{35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Figure taken from \cite  {miller90}. Word-forms $F_1$, and $F_2$ are synonyms of each other, as they share one word meaning $M_1$. Word-form $F_2$, as it entails more than one meaning, namely $M_1$ and $M_2$.\relax }}{36}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Example output for WordNet 3.1 noun propositions for the word "bank". In total, 18 different concepts are recorded.\relax }}{37}\protected@file@percent }
\citation{francis64}
\citation{miller94}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Example output for WordNet 3.1 noun propositions for the word "was". In total, 18 different concepts are recorded.\relax }}{38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{SemCor dataset}{38}\protected@file@percent }
\citation{wang19}
\citation{wang19b}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Shows that the SemCor data is biased. Words with a low WordNet sense index (i.e. close to 0) occur much more often than words that have a high WordNet sense index (i.e. above 5). The x-axis shows the WordNet sense index for a chosen word, while the y-axis shows the log-frequency within SemCor. This is a cumulative plot over all words with WordNet senses within SemCor 3.0. The skew could be a natural effect of how word lower WordNet indecies are assigned to more commonly used words. \relax }}{39}\protected@file@percent }
\newlabel{fig:embeddings_by_language}{{2.12}{39}}
\@writefile{toc}{\contentsline {subsubsection}{GLUE benchmark dataset}{39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Hello\relax }}{40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Single Sentence Tasks}{40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Similarity and paraphrase tasks}{41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inference Tasks}{42}\protected@file@percent }
\citation{levesque12}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{45}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Structure inside BERT}{45}\protected@file@percent }
\citation{peters18}
\citation{may19}
\citation{jentzsch19}
\citation{hu19}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Attention mechanism}{47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}From token-vectors to word-vectors}{47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Bias in BERT vectors}{47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Change of meanings over time}{47}\protected@file@percent }
\citation{zhu18}
\citation{chen19}
\citation{kudugunta18}
\citation{vaswani17}
\citation{kudugunta18}
\citation{kudugunta18}
\citation{lample18}
\citation{ma19}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Clinical concept extration}{48}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Discovering for semantics}{48}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.7}Embeddings for translation}{48}\protected@file@percent }
\citation{moutafis17}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces From \cite  {kudugunta18}, visualizing clustering of the encoder representations of all languages, based on ther SVCCA similarity.\relax }}{49}\protected@file@percent }
\newlabel{fig:embeddings_by_language}{{3.1}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Metric Learning and Disentanglement}{49}\protected@file@percent }
\citation{mahalanobis36}
\citation{chechik10}
\citation{moutafis17}
\citation{suarez19}
\@writefile{toc}{\contentsline {subsubsection}{Non-Deep Metric Learning}{51}\protected@file@percent }
\citation{suarez19}
\citation{suarez19}
\citation{suarez19}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Taken from \cite  {suarez19}. The individual tiles show the kNN prediction regions by color for every point in the image. Using the unmodified euclidean distancee, this would result in classification regions on the left. The reader can see, learning an appropriate distance, the classification is much more effective (middle). Finally, the dimensionality is also reducable with this dimension while still matching the classification accuracy (right).\relax }}{52}\protected@file@percent }
\newlabel{fig:BERT_vanilla_pipeline}{{3.2}{52}}
\citation{kaya19}
\citation{kaya19}
\citation{kaya19}
\citation{bromley94}
\citation{chopra05}
\citation{hadsell06}
\@writefile{toc}{\contentsline {paragraph}{Dimensionality Reduction}{53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Metric Learning}{53}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Taken from \cite  {kaya19}. An illustration of deep metric learning. The space is transformed in such a way, that similar object are closer to each other, and dissimilar objects are moved away from each other.\relax }}{53}\protected@file@percent }
\newlabel{fig:muse_translation}{{3.3}{53}}
\citation{hoffer14}
\citation{ustinova16}
\citation{song16}
\citation{sohn16}
\citation{rippel16}
\citation{wang17}
\citation{ni17}
\citation{song17}
\citation{ge18}
\citation{wang19c}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Clustering Algorithms}{55}\protected@file@percent }
\citation{conneau17}
\citation{conneau17}
\citation{conneau17}
\citation{alvarez18}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Applications of word vector}{56}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Taken from \cite  {conneau17}. Each \relax }}{56}\protected@file@percent }
\newlabel{fig:muse_translation}{{3.4}{56}}
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{59}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Analysing the current state of the art}{61}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}On the Linear Separability of meaning within sampled BERT vectors}{61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Motivation}{61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Experiment setup}{61}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Checks sampled BERT vectors for linear interpretability by meaning\relax }}{62}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Results}{63}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Mean and standard deviation of the accuracy of a linear classifier trained on the 2 most common classes of WordNet meanings for the word \textit  {was}.\relax }}{63}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Mean and standard deviation of the accuracy of a linear classifier trained on the 2 most common classes of WordNet meanings for the word \textit  {is}.\relax }}{64}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Mean and standard deviation of the accuracy of a linear classifier trained on the 2 most common classes of WordNet meanings for the word \textit  {one}.\relax }}{64}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Mean and standard deviation of the accuracy of a linear classifier trained on the the 4 most common classes of WordNet meanings for the word \textit  {was}.\relax }}{64}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}On the Clusterability of meaning within sampled BERT vectors}{65}\protected@file@percent }
\newlabel{experiment_BERT_clusterability}{{4.2}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Motivation}{65}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Experiment setup}{65}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Checks sampled BERT vectors for clusters by meaning\relax }}{66}\protected@file@percent }
\citation{rand71}
\citation{hubert85}
\newlabel{eq:adjustedrandomindex}{{4.2.2}{67}}
\citation{pelevina16}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Results}{68}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Qualitative evaluation}{69}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A famous equation\relax }}{70}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A famous equation\relax }}{71}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces A famous equation\relax }}{71}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces A famous equation\relax }}{72}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces A famous equation\relax }}{72}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces A famous equation\relax }}{73}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces A famous equation\relax }}{73}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces A famous equation\relax }}{74}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces A famous equation\relax }}{74}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces A famous equation\relax }}{75}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces A famous equation\relax }}{75}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces A famous equation\relax }}{76}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces A famous equation\relax }}{76}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces A famous equation\relax }}{77}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces A famous equation\relax }}{77}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces A famous equation\relax }}{78}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces A famous equation\relax }}{78}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Correlation between Part of Speech and Context within BERT}{79}\protected@file@percent }
\newlabel{correlation_pos_context}{{4.3}{79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Motivation}{79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Experiment setup}{79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Results}{79}\protected@file@percent }
\newlabel{fig:sfig1}{{4.18a}{80}}
\newlabel{sub@fig:sfig1}{{a}{80}}
\newlabel{fig:sfig2}{{4.18b}{80}}
\newlabel{sub@fig:sfig2}{{b}{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces plots of....\relax }}{80}\protected@file@percent }
\newlabel{fig:fig}{{4.18}{80}}
\newlabel{fig:sfig1}{{4.19a}{80}}
\newlabel{sub@fig:sfig1}{{a}{80}}
\newlabel{fig:sfig2}{{4.19b}{80}}
\newlabel{sub@fig:sfig2}{{b}{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces plots of....\relax }}{80}\protected@file@percent }
\newlabel{fig:fig}{{4.19}{80}}
\newlabel{fig:sfig1}{{4.20a}{80}}
\newlabel{sub@fig:sfig1}{{a}{80}}
\newlabel{fig:sfig2}{{4.20b}{80}}
\newlabel{sub@fig:sfig2}{{b}{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces plots of....\relax }}{80}\protected@file@percent }
\newlabel{fig:fig}{{4.20}{80}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Exploiting subspace organization of semantics of BERT embeddings}{81}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The BERT model takes as input a sentence $s$. The sentence $s$ is converted to a sequence of BERT tokens $t_1$, \ldots  , $t_m$ as defined in a given vocabulary $V$. Each item in the vocabulary $V$ has a corresponding embedding vector inside the embedding layer of the transformer. This embedding vector is used by the intermediate layers of the transformer, and thus affects the downstream pipeline of the transformer for any subsequent layers of the transformer. \relax }}{82}\protected@file@percent }
\newlabel{fig:BERT_vanilla_pipeline}{{5.1}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}BERnie PoS}{82}\protected@file@percent }
\newlabel{bernie_pos}{{5.0.1}{82}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{82}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{82}\protected@file@percent }
\citation{spacyb}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The modified pipeline. The BERnie model takes as input a sentence $s$. The sentence $s$ is converted to a sequence of BERT tokens $t_1$, \ldots  , $t_m$ as defined in a given vocabulary $V$. For each target token $t_{\text  {target}}$, we make the token more specific by converting the token to a more specialized token-representation, which specifies the part-of-speech information as part of the token. In this case, $run$ becomes $run\_ VERB$. Again, each item in the vocabulary $V$ has a corresponding embedding vector inside the embedding layer of the transformer. This embedding vector is used by the intermediate layers of the transformer, and thus affects the downstream pipeline of the transformer for any subsequent layers of the transformer. \relax }}{83}\protected@file@percent }
\newlabel{fig:BERnie_POS_pipeline}{{5.2}{83}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.2}BERnie Meaning}{83}\protected@file@percent }
\newlabel{experiment_bernie_meaning}{{5.0.2}{83}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{83}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Inside the embedding layer of BERT, we introduce more specific embeddings \textit  {run\_ VERB} and \textit  {run\_ NOUN}. The BERT model should intuitively now capture more expressiveness, as the model size increased. The original \textit  {run} embedding is removed.\relax }}{84}\protected@file@percent }
\newlabel{fig:BERnie_POS_pipeline}{{5.3}{84}}
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{84}\protected@file@percent }
\newlabel{fig:sfig1}{{5.4a}{85}}
\newlabel{sub@fig:sfig1}{{a}{85}}
\newlabel{fig:sfig2}{{5.4b}{85}}
\newlabel{sub@fig:sfig2}{{b}{85}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces plots of....\relax }}{85}\protected@file@percent }
\newlabel{fig:fig}{{5.4}{85}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces \relax }}{85}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{5.5}{85}}
\newlabel{fig:sfig1}{{5.6a}{86}}
\newlabel{sub@fig:sfig1}{{a}{86}}
\newlabel{fig:sfig2}{{5.6b}{86}}
\newlabel{sub@fig:sfig2}{{b}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces plots of....\relax }}{86}\protected@file@percent }
\newlabel{fig:fig}{{5.6}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces The BERT model takes as input a sentence $s$. The sentence $s$ is converted to a sequence of BERT tokens $t_1$, \ldots  , $t_m$ as defined in a given vocabulary $V$. Each item in the vocabulary $V$ has a corresponding embedding vector inside the embedding layer of the transformer. This embedding vector is used by the intermediate layers of the transformer, and thus affects the downstream pipeline of the transformer for any subsequent layers of the transformer.\relax }}{86}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{5.7}{86}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{86}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces asj\relax }}{87}\protected@file@percent }
\citation{devlin18}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.3}BERnie Meaning with additional pre-training}{88}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{88}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{88}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Mean and standard deviation of the accuracy of a linear classifier trained on the the 4 most common classes of WordNet meanings for the word \textit  {was}.\relax }}{88}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Mean of mulitple experiments for BERnie with addiitonally trained embeddings and weights.\relax }}{89}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Compressing the non-lexical out}{89}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{89}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{89}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Results}{90}\protected@file@percent }
\bibstyle{plain}
\bibdata{dissertation}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{91}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{spacyb}{1}
\bibcite{alvarez18}{2}
\bibcite{ba16}{3}
\bibcite{bengio03}{4}
\bibcite{bromley94}{5}
\bibcite{chechik10}{6}
\bibcite{chen19}{7}
\bibcite{chopra05}{8}
\bibcite{conneau17}{9}
\bibcite{devlin18}{10}
\bibcite{francis64}{11}
\bibcite{gage94}{12}
\bibcite{ganea18}{13}
\bibcite{ge18}{14}
\bibcite{hadsell06}{15}
\bibcite{harris54}{16}
\bibcite{hegel17}{17}
\bibcite{hochreiter97}{18}
\bibcite{hoffer14}{19}
\bibcite{spacy}{20}
\bibcite{hu19}{21}
\bibcite{hubert85}{22}
\bibcite{jentzsch19}{23}
\bibcite{kaya19}{24}
\bibcite{kudugunta18}{25}
\bibcite{lample18}{26}
\bibcite{lee17}{27}
\bibcite{levesque12}{28}
\bibcite{Liu18}{29}
\bibcite{ma19}{30}
\bibcite{mahalanobis36}{31}
\bibcite{may19}{32}
\bibcite{mikolov13}{33}
\bibcite{mikolov13b}{34}
\bibcite{miller90}{35}
\bibcite{miller94}{36}
\bibcite{moutafis17}{37}
\bibcite{ni17}{38}
\bibcite{pelevina16}{39}
\bibcite{pennington14}{40}
\bibcite{peters17a}{41}
\bibcite{peters17b}{42}
\bibcite{peters18}{43}
\bibcite{radford18}{44}
\bibcite{radford19}{45}
\bibcite{rand71}{46}
\bibcite{Rei17}{47}
\bibcite{rippel16}{48}
\bibcite{sanh19}{49}
\bibcite{sennrich15}{50}
\bibcite{sennrich16}{51}
\bibcite{shen19}{52}
\bibcite{sohn16}{53}
\bibcite{song17}{54}
\bibcite{song16}{55}
\bibcite{suarez19}{56}
\bibcite{sutskever11}{57}
\bibcite{ustinova16}{58}
\bibcite{vaswani17}{59}
\bibcite{vilnis14}{60}
\bibcite{wang19b}{61}
\bibcite{wang19}{62}
\bibcite{wang17}{63}
\bibcite{wang19c}{64}
\bibcite{wittgenstein53}{65}
\bibcite{wu16}{66}
\bibcite{zhu18}{67}
