\relax 
\providecommand \oddpage@label [2]{}
\citation{mikolov13}
\citation{mikolov13b}
\citation{devlin18}
\citation{vaswani17}
\citation{devlin18}
\citation{devlin18}
\citation{devlin18}
\citation{wang19}
\citation{miller90}
\citation{pelevina16}
\citation{levine19}
\citation{wiedmann19}
\citation{coenen19}
\citation{si19}
\citation{si19}
\citation{coenen19}
\citation{jawahar19}
\citation{peters18}
\citation{jawahar19}
\citation{colahLSTM}
\citation{suarez19}
\citation{kaya19}
\citation{kudugunta18}
\citation{conneau17}
\citation{pelevina16}
\citation{wang19}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Our main contributions are the following:}{2}\protected@file@percent }
\citation{harris54}
\citation{harris54}
\citation{hegel17}
\citation{wittgenstein53}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{section:Background}{{2}{4}}
\citation{spacyb}
\citation{spacy}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Linguistic Features}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Polysemy}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Part of Speech \textit  {PoS}}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Tokens and n-grams}{5}\protected@file@percent }
\citation{sutskever11}
\citation{lee17}
\citation{gage94}
\citation{sennrich16}
\@writefile{toc}{\contentsline {paragraph}{Characters}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Byte pair encodings}{6}\protected@file@percent }
\citation{wu16}
\newlabel{tokenizer:WordPiece}{{1}{7}}
\@writefile{toc}{\contentsline {paragraph}{WordPiece tokenizer}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Word Embeddings}{7}\protected@file@percent }
\citation{ganea18}
\@writefile{toc}{\contentsline {paragraph}{Word-Embeddings}{8}\protected@file@percent }
\newlabel{map:embedding_mapping}{{2.1}{8}}
\@writefile{toc}{\contentsline {paragraph}{Distance:}{8}\protected@file@percent }
\newlabel{def:distance}{{2.2}{8}}
\citation{harris54}
\citation{bengio03}
\@writefile{toc}{\contentsline {paragraph}{Learning a distance}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Exploiting the distributional structure of language:}{9}\protected@file@percent }
\newlabel{eq:naive_sequential_probability}{{2.4}{9}}
\newlabel{eq:naive_sequential_probability_markovian}{{2.2}{10}}
\newlabel{eq:naive_coocurrence_probability}{{2.6}{10}}
\@writefile{toc}{\contentsline {paragraph}{Loss Objective}{10}\protected@file@percent }
\citation{harris54}
\citation{mikolov13}
\citation{mikolov13}
\newlabel{eq:basic_equation_log_maximization}{{2.2}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Figure taken from \cite  {mikolov13}. The CBOW architecture predicts the current word based on the context. The Skip-gram predicts surrounding words given the current word.\relax }}{11}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cbow_skipgram}{{2.1}{11}}
\citation{cil_slides}
\newlabel{eq:basic_equation_log_maximization_skipgram}{{2.2}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Static Word Embeddings}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Basic Model}{12}\protected@file@percent }
\citation{ethayarajh19}
\citation{mikolov13}
\citation{mikolov13b}
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{13}\protected@file@percent }
\citation{mikolov13b}
\citation{mikolov13b}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Figure taken from \cite  {mikolov13b}. A 2-dimensional PCA projection of the 1000-dimensional skip-gram vectors of countries and their capital cities. The proposed model is able to automatically organize concepts and learn implicit relationships between them. No supervised information was provided about what a capital city means.\relax }}{14}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{2.2}{14}}
\citation{pennington14}
\citation{bojanowski17}
\citation{brazinskas19}
\citation{yan19}
\@writefile{toc}{\contentsline {subsubsection}{GloVe}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Contextual Word Embeddings}{16}\protected@file@percent }
\newlabel{map:context_embedding_mapping}{{2.2.2}{16}}
\citation{devlin18}
\citation{devlin18}
\citation{peters17b}
\newlabel{eq:transformer_probability}{{2.2.2}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Figure taken from \cite  {devlin18}. BERT uses a bidirectional transformer, which is not limited to reading in all the input from only left to right or right to left. OpenAI GPT (next section) uses a left-to-right Transformer, while ELMo is using a bidirectional LSTM which naturally captures a single direction per LSTM. \relax }}{17}\protected@file@percent }
\newlabel{fig:attention_is_all_you_need}{{2.3}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}ELMo}{17}\protected@file@percent }
\citation{jozefowicz16}
\citation{vaswani17}
\citation{bahdanau16}
\citation{vaswani17}
\citation{vaswani17}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}The Transformer Architecture}{19}\protected@file@percent }
\citation{devlin18}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Figure taken from \cite  {vaswani17}. The architecture of the transformer module which encapsulates multiple attention modules.\relax }}{20}\protected@file@percent }
\newlabel{fig:attention_is_all_you_need}{{2.4}{20}}
\citation{devlin18}
\citation{devlin18}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}BERT: Bidirectional Encoder Representations from Transformers}{21}\protected@file@percent }
\newlabel{section:BERT}{{2.2.5}{21}}
\citation{devlin18}
\citation{devlin18}
\citation{devlin18}
\citation{devlin18}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Example from \cite  {devlin18}. An sentence where 15\% of the tokens are replaced with the [MASK] token. During the first phase of pre-training, the weights of the BERT model are optimized in such a way to predict the true underlying words. The word to be predicted is \texttt  {the}. \relax }}{22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Example from \cite  {devlin18}. Two input sequences which where 15\% of the tokens are replaced with the [MASK] token. During pre-training, the weights of the BERT model are optimized in such a way to predict the true underlying words. In this case, the second sentence is a continuation of the first one, and thus the label would be \textit  {isNext}. \relax }}{22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Figure taken from \cite  {devlin18}. Ways to fine-tune BERT on different GLUE tasks.\relax }}{23}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{2.7}{23}}
\@writefile{toc}{\contentsline {paragraph}{The pipeline of BERT}{23}\protected@file@percent }
\citation{radford18}
\citation{radford19}
\citation{ba16}
\citation{lan20}
\citation{sanh19}
\citation{moradshahi19}
\citation{sun19}
\citation{liu19}
\citation{joshi19}
\citation{wang19d}
\citation{shen19}
\citation{wang19e}
\citation{wang19}
\citation{wang19b}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces The BERT model takes as input a sentence $s$. The sentence $s$ is converted to a sequence of BERT tokens $t_1$, \ldots  , $t_m$ as defined in a given vocabulary $V$. Each item in the vocabulary $V$ has a corresponding embedding vector inside the embedding layer of the transformer. This embedding vector is used by the intermediate layers of the transformer, and thus affects the downstream pipeline of the transformer for any subsequent layers of the transformer. \relax }}{24}\protected@file@percent }
\newlabel{fig:BERT_vanilla_pipeline}{{2.8}{24}}
\@writefile{toc}{\contentsline {subsubsection}{Other language models}{24}\protected@file@percent }
\citation{wang19}
\citation{wang19}
\citation{N18-1101}
\citation{bowman2015}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}GLUE benchmark dataset}{25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Table taken from \cite  {wang19}. Listing of all GLUE tasks including the linguistic phenomenon benchmarked, as well as the domain that the benchmarking data contains. Training and test set sizes are also provided.\relax }}{25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inference Tasks}{25}\protected@file@percent }
\citation{rajpurkar2016}
\citation{dagan2005}
\citation{bar2006}
\citation{bentivogli2009}
\citation{giampiccolo2007}
\citation{levesque2012}
\citation{miller90}
\citation{miller90}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}WordNet}{28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Lexical matrix:}{28}\protected@file@percent }
\citation{miller90}
\citation{miller90}
\citation{francis64}
\citation{miller94}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Table taken from \cite  {miller90}. Word-forms $F_1$, and $F_2$ are synonyms of each other, as they share one word meaning $M_1$. Word-form $F_2$, as it entails more than one meaning, namely $M_1$ and $M_2$.\relax }}{29}\protected@file@percent }
\newlabel{fig:wordnet_table}{{2.10}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Example output for WordNet 3.1 noun propositions for the word \texttt  {bank}. In total, 18 different concepts are recorded.\relax }}{30}\protected@file@percent }
\newlabel{fig:bank_synset}{{2.11}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Example output for WordNet 3.1 noun propositions for the word \texttt  {was}. In total, 18 different concepts are recorded.\relax }}{30}\protected@file@percent }
\newlabel{fig:was_synset}{{2.12}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}SemCor dataset}{31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Shows that the SemCor data is biased towards words with low WordNet class IDs. Words with a low WordNet sense index (i.e. close to 0) occur much more often than words that have a high WordNet sense index (i.e. above 5). The x-axis shows the WordNet sense index for a chosen word, while the y-axis shows the log-frequency within SemCor. This is a cumulative plot over all words with WordNet senses within SemCor 3.0. The skew could be a natural effect of how word lower WordNet indecies are assigned to more commonly used words. \relax }}{32}\protected@file@percent }
\newlabel{fig:embeddings_by_language}{{2.13}{32}}
\citation{pelevina16}
\citation{biemann13}
\citation{remus18}
\citation{arefyev19}
\citation{denkowski09}
\citation{pelevina16}
\citation{biemann13}
\citation{biemann06}
\citation{biemann13}
\citation{denkowski09}
\citation{pelevina16}
\citation{pelevina16}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{33}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{section:RelatedWork}{{3}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Creating Synsets from Static Word Embeddings}{33}\protected@file@percent }
\newlabel{section_clustering}{{3.1}{33}}
\citation{pelevina16}
\citation{pelevina16}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Figure taken from \cite  {pelevina16}. An ego-network of the word \texttt  {table} is created. Then, clustering is applied on the ego-network, to identify different semantic sets \textit  {synsets}.\relax }}{34}\protected@file@percent }
\newlabel{fig:ego_network}{{3.1}{34}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Table taken from \cite  {pelevina16}. Neighbors of the word \texttt  {table} and its senses produced. The first row belongs to both senses, while the second and third row are distinct synsets.\relax }}{34}\protected@file@percent }
\citation{remus18}
\citation{reimers19}
\citation{bommasani19}
\citation{akbik19}
\citation{may19}
\citation{bruni13}
\citation{hill15}
\citation{pilehvar19}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Quantifying word sense disambiguation}{35}\protected@file@percent }
\citation{camachocollados18}
\citation{liebeskind19}
\citation{navigli19}
\citation{camachocollados18}
\citation{ethayarajh19}
\citation{mickus19}
\citation{ethayarajh19}
\citation{ethayarajh19}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Semantic subspace inside BERT}{36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Geometric analysis}{36}\protected@file@percent }
\citation{ethayarajh19}
\citation{ethayarajh19}
\citation{mickus19}
\citation{mickus19}
\citation{rousseeuw87}
\citation{wiktionary}
\citation{mickus19}
\citation{mikolov13}
\citation{shi19}
\citation{levine19}
\citation{wiedmann19}
\citation{shi19}
\citation{carreras04}
\citation{pradhan13}
\citation{levine19}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Probing for semantics}{39}\protected@file@percent }
\citation{levine19}
\citation{levine19}
\citation{wiedmann19}
\citation{kageback16}
\citation{miller91}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Figure from \cite  {levine19}. Standard BERT (left) and the SenseBERT adaptations (right), which include an embedding-encoder and an embedding-decoder specifically for WordNet senses.\relax }}{40}\protected@file@percent }
\newlabel{fig:embeddings_by_language}{{3.2}{40}}
\citation{wiedmann19}
\citation{wiedmann19}
\citation{coenen19}
\citation{coenen19}
\citation{coenen19}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces From \cite  {wiedmann19}. T-SNE plots of different senses of \texttt  {bank} and their contextual word embeddings. The legend shows a short description of the different WordNet sensees and the frequency of occurrence in the training data. \relax }}{41}\protected@file@percent }
\newlabel{fig:bert_wsd}{{3.3}{41}}
\citation{si19}
\citation{lee19}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces From \cite  {coenen19}. Embeddings for the word \texttt  {die} in different contexts, visualized through UMAP. The blue text describes general annotations. Notice that in this case, the two semantic classes \texttt  {die} also have two distinct part-of-speech tags. \relax }}{42}\protected@file@percent }
\newlabel{fig:BERT_plurality}{{3.4}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Word sense disambiguation in specialized domains}{42}\protected@file@percent }
\citation{si19}
\citation{si19}
\citation{si19}
\citation{si19}
\citation{coenen19}
\citation{jawahar19}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Figure taken from \cite  {si19}. \textbf  {Fictional} embedding vector points and clusters of \texttt  {cold}. This is one of the results that we want to arrive at. Specifically, we desire distinct word-embeddings that capture the modes of the underlying probability distribution. \relax }}{43}\protected@file@percent }
\newlabel{fig:cold_fictional_desired}{{3.5}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Figure taken from \cite  {si19}. PCA visualizations using embedding vector of \texttt  {cold} from BERT. The blue data points refer to \texttt  {cold} as a temperature, whereas the red data points refer to \texttt  {cold} as a symptom.\relax }}{43}\protected@file@percent }
\newlabel{fig:cold_fictional_desired}{{3.6}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Syntactic subspace in BERT}{43}\protected@file@percent }
\citation{coenen19}
\citation{jawahar19}
\citation{coenen19}
\citation{jawahar19}
\citation{coenen19}
\citation{marcus93}
\citation{hewitt19}
\citation{coenen19}
\citation{coenen19}
\citation{jawahar19}
\citation{jawahar19}
\citation{jawahar19}
\citation{peters18}
\citation{peters18}
\citation{peters18}
\citation{jawahar19}
\citation{joshi19b}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Extracting Parse-Trees}{44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces From \cite  {coenen19}. Visualizing the embeddings of two sentences after applying the Hewitt Manning probe. Parse tree (left) in comparison to the PCA projection of the contextual word embeddings (right). Small deviations are apparently, but an obvious resemblance exists. The color of an edge determines the squared Euclidean distance.\relax }}{45}\protected@file@percent }
\newlabel{fig:BERT_tree}{{3.7}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces From \cite  {jawahar19}. Dependency parse tree induced from attention head in layer 11 in layer 2 using labelled root \texttt  {are} as starting node with the maximum spanning tree algorithm\relax }}{45}\protected@file@percent }
\newlabel{fig:cold_fictional_desired}{{3.8}{45}}
\citation{ribeiro19}
\citation{biemann06}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces From \cite  {peters18}. The authors show different similarity intensities (cosine similarity) between token-pairs using the output of a 4-layer LSTM (left), and the output after the first layer (right).\relax }}{46}\protected@file@percent }
\newlabel{fig:embeddings_by_language}{{3.9}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Subspace representing part-of-speech, verbs and arguments}{46}\protected@file@percent }
\citation{jawahar19}
\citation{jawahar19}
\citation{baker98}
\citation{peters18}
\citation{tan19}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces From \cite  {jawahar19}. 2D t-SNE plot of span embeddings computed from the first and last two layers of BERT.\relax }}{47}\protected@file@percent }
\newlabel{fig:cold_fictional_desired}{{3.10}{47}}
\citation{may19}
\citation{caliskan19}
\citation{moradshahi19}
\citation{may19}
\citation{jentzsch19}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Sentiment subspace in BERT}{48}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Bias}{48}\protected@file@percent }
\citation{martinc20}
\citation{hu19}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Change of language over time}{49}\protected@file@percent }
\citation{coenen19}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}How is semantics captured through contextual word embeddings produced by BERT?}{50}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{section:Understand_BERT}{{4}{50}}
\newlabel{section:UnderstandingBERT}{{4}{50}}
\citation{news_corpus}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}On the Linear Separability of meaning within sampled BERT vectors}{51}\protected@file@percent }
\newlabel{sec:linear_separability_experiment}{{4.1}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Experiment setup}{51}\protected@file@percent }
\citation{scikit-learn}
\citation{Wolf2019}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Checks sampled BERT vectors for linear interpretability by meaning\relax }}{53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Results}{53}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Mean and standard deviation of the accuracy of a linear classifier trained on the 2 most common classes of WordNet meanings for the word \textit  {was}.\relax }}{54}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Mean and standard deviation of the accuracy of a linear classifier trained on the 2 most common classes of WordNet meanings for the word \textit  {is}.\relax }}{54}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Mean and standard deviation of the accuracy of a linear classifier trained on the 2 most common classes of WordNet meanings for the word \textit  {one}.\relax }}{54}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Mean and standard deviation of the accuracy of a linear classifier trained on the 4 most common classes of WordNet meanings for the word \textit  {was}.\relax }}{55}\protected@file@percent }
\citation{ackerman09}
\citation{mccarthy16}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}On the Clusterability of meaning within sampled BERT vectors}{56}\protected@file@percent }
\newlabel{experiment_BERT_clusterability}{{4.2}{56}}
\citation{lloyd57}
\citation{macqueen67}
\citation{bergstra12}
\citation{wang13}
\citation{pyax}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Experiment setup}{57}\protected@file@percent }
\citation{pelevina16}
\citation{pelevina16}
\citation{hagberg04}
\citation{conneau17}
\citation{conneau17}
\@writefile{toc}{\contentsline {paragraph}{We will use a modified version of the Chinese Whispers algorithm.}{58}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Checks sampled BERT vectors for clusters by meaning\relax }}{58}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Checks sampled BERT vectors for clusters by meaning\relax }}{60}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Algorithm to find the best model with the best fitting parameter configuration.\relax }}{61}\protected@file@percent }
\citation{rand71}
\citation{hubert85}
\newlabel{eq:adjustedrandomindex}{{4.2.1}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Results}{62}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces The maximum ARI score achieved during hyperparameter optimization for the derivative models as described by experiment for $k=100$ and $n=500$. \relax }}{63}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces The maximum ARI score achieved during hyperparameter optimization for the different models as described by experiment for $k=100$ and $n=1000$. \relax }}{63}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces The maximum ARI score achieved during hyperparameter optimization for the different models as described by experiment for $k=20$ and $n=1000$. \relax }}{64}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces The maximum ARI score achieved during hyperparameter optimization for the different models as described by experiment for $k=20$ and $n=1000$. \relax }}{64}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Qualitative evaluation}{65}\protected@file@percent }
\newlabel{fig:example_arms_negative_handcuffed}{{4.8}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Partition 1 for the word \texttt  {arms}. The context is about handcuffed arms. The sentiment is usually one of negative surprise.\relax }}{65}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Partition 2 for the word \texttt  {arms}. The context is about a persons arms, where one person hugs or loves another (positive sentiment)\relax }}{65}\protected@file@percent }
\newlabel{fig:example_arms_negative_crossed}{{4.2}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Partition 3 for the word \texttt  {arms}. This cluster contains \texttt  {arms} in the context of strong arms, usually in a competitive context.\relax }}{66}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Partition 4 for the word \texttt  {arms}. This cluster contains \texttt  {arms} in the context of individuals and countries. This uses the semantic definition of arms which is analogous to \texttt  {weaponry}.\relax }}{66}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Partition 5 for the word \texttt  {arms}. This cluster contains \texttt  {arms} in the context of countries (no individuals). This uses the semantic definition of arms which is analogous to \texttt  {weaponry}. One can notice that these sentences usually refer to nuclear arms.\relax }}{67}\protected@file@percent }
\newlabel{fig:example_arms_negative_crossed}{{4.8}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Partition 6 for the word \texttt  {arms}. This cluster again refers to the arms of a person, however usually with a positive / optimistic sentiment.\relax }}{67}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Correlation between Part of Speech and Context within BERT}{68}\protected@file@percent }
\newlabel{correlation_pos_context}{{4.3}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Experiment setup}{68}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Analyzing dominance of part-of-speech within WordNet meaning clusters.\relax }}{69}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Results}{70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Quantitative Evaluation}{70}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Cumulative dominance of the most occurring cluster. Dominance of a part of speech tag is measured by the percentage cover that the majority class intakes.\relax }}{70}\protected@file@percent }
\newlabel{fig:POS_dominance}{{4.7}{70}}
\@writefile{toc}{\contentsline {subsubsection}{Qualitative Evaluation}{71}\protected@file@percent }
\newlabel{fig:sfig1}{{4.8a}{71}}
\newlabel{sub@fig:sfig1}{{a}{71}}
\newlabel{fig:sfig2}{{4.8b}{71}}
\newlabel{sub@fig:sfig2}{{b}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces PCA and UMAP visualizations for contextual word embeddings $X$ sampled for the word $w=$\texttt  {run}. \relax }}{71}\protected@file@percent }
\newlabel{fig:run_tensorboard}{{4.8}{71}}
\newlabel{fig:sfig1}{{4.9a}{71}}
\newlabel{sub@fig:sfig1}{{a}{71}}
\newlabel{fig:sfig2}{{4.9b}{71}}
\newlabel{sub@fig:sfig2}{{b}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces PCA and UMAP visualizations for contextual word embeddings $X$ sampled for the word $w=$\texttt  {block}. \relax }}{71}\protected@file@percent }
\newlabel{fig:was_tensorboard}{{4.9}{71}}
\newlabel{fig:sfig1}{{4.10a}{72}}
\newlabel{sub@fig:sfig1}{{a}{72}}
\newlabel{fig:sfig2}{{4.10b}{72}}
\newlabel{sub@fig:sfig2}{{b}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces PCA and UMAP visualizations for contextual word embeddings $X$ sampled for the word $w=$\texttt  {cold}. \relax }}{72}\protected@file@percent }
\newlabel{fig:cold_tensorboard}{{4.10}{72}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Exploiting subspace organization of semantics of BERT embeddings}{73}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{section:ExploitingBERT}{{5}{73}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces For each word in the SemCor dataset, the number of WordNet senses, and the mean variance of $n=500$ sampled embedding vectors, across all embedding dimensions. The table shows a subset of the words with highest and lowest variance. There seems to be a strong correlation between the number of WordNet senses and the variance amongst sampled BERT vectors.\relax }}{74}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces  For each word $w$, we sample both the number of WordNet semantic classes that are recorded in the WordNet dataset. We also calculate the dimension-wise mean variance between $n=500$ sampled vectors for the word $w$. This constitutes a point on this plot. We repeat this procedure for the most 20'000 most frequent words which consist of a single token (i.e. are not split up into further subtokens when the tokenizer is applied). We show that although the variance is relatively high, especially if only few WordNet classes are present, there is a correlation between the number of WordNet classes and the variance of sampled BERT contextual word embeddings. The right and top distributions show histograms of how occurrent the variance and WordNet classes are respectively. Our assumption is that introducing additional embedding-vectors inside BERT for certain words allows to capture mode complex distributions, i.e. a more complex distribution for words that have a higher number of WordNet classes. \relax }}{75}\protected@file@percent }
\newlabel{fig:BERT_variance}{{5.1}{75}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}BERNIE: Equalizing variances across BERT embeddings}{75}\protected@file@percent }
\citation{spacyb}
\citation{spacyb}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}BERNIE PoS}{76}\protected@file@percent }
\newlabel{bernie_pos}{{5.1.1}{76}}
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{76}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The part-of-speech modified pipeline. The BERNIE PoS model takes as input a sentence $s$. The sentence $s$ is converted to a sequence of BERT tokens $[t_1$, \ldots  , $t_m]$ as defined in a given vocabulary $V$. This vocabulary $V$ is extended with the additional tokens for each of the split-words. For each target token $t_{\text  {target}}$, we make the token more specific by converting the token to a more specialized token-representation, which specifies the part-of-speech information as part of the token. In this case, $run$ becomes the more specialized $run\_ VERB$. Again, each item in the vocabulary $V$ has a corresponding embedding vector inside the embedding layer of the transformer. \relax }}{77}\protected@file@percent }
\newlabel{fig:BERnie_POS_pipeline}{{5.2}{77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}BERNIE Cluster}{77}\protected@file@percent }
\newlabel{experiment_bernie_meaning}{{5.1.2}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Inside the embedding layer of the transformer which occurs at each layer of BERT, we introduce more specific embeddings \texttt  {run\_VERB} and \texttt  {run\_NOUN}. The BERT model should now capture more expressiveness, as more weight got introduced for a part of the model which results in a probability distribution with high variance. The original \texttt  {run} embedding is removed.\relax }}{78}\protected@file@percent }
\newlabel{fig:BERnie_POS_initialization}{{5.3}{78}}
\newlabel{fig:sfig1}{{5.4a}{78}}
\newlabel{sub@fig:sfig1}{{a}{78}}
\newlabel{fig:sfig2}{{5.4b}{78}}
\newlabel{sub@fig:sfig2}{{b}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The resulting, fully modified BERNIE PoS pipeline.\relax }}{78}\protected@file@percent }
\newlabel{fig:BERNIE_POS_full_pipeline}{{5.4}{78}}
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Results}{79}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The semantic modified pipeline. The BERNIE Cluster model takes as input a sentence $s$. The sentence $s$ is converted to a sequence of BERT tokens $[t_1$, \ldots  , $t_m]$ as defined in a given vocabulary $V$. This vocabulary $V$ is extended with the additionally introduced tokens for each of the split-words. For each target token $t_{\text  {target}}$, we make the token more specific by converting the token to a more specialized token-representation, which specifies the clustereing information as part of the token. In this case, $bank$ becomes the more specialized $bank\_FINANCE$. Again, each item in the vocabulary $V$ has a corresponding embedding vector inside the embedding layer of the transformer. This embedding vector is used by the intermediate layers of the transformer, and thus affects the downstream pipeline of the language model for any subsequent layers of the transformer.\relax }}{80}\protected@file@percent }
\newlabel{fig:BERT_Cluster_pipeline}{{5.5}{80}}
\newlabel{fig:sfig1}{{5.6a}{80}}
\newlabel{sub@fig:sfig1}{{a}{80}}
\newlabel{fig:sfig2}{{5.6b}{80}}
\newlabel{sub@fig:sfig2}{{b}{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The resulting, fully modified BERNIE Cluster pipeline.\relax }}{80}\protected@file@percent }
\newlabel{fig:fig}{{5.6}{80}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces GLUE Benchmark performance measures for the BERT model, BERNIE PoS and BERNIE Cluster. Higher scores are better. The task-wide best-performers are marked in bold. All values are the average scores of two runs. \relax }}{81}\protected@file@percent }
\citation{devlin18}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}BERNIE Cluster with additional pre-training}{82}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Experiment setup}{83}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Results}{83}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces GLUE Benchmark performance measures for the BERNIE Cluster model without any additional pre-training, the BERNIE Cluster with full pre-training and BERNIE with partial pre-training. Higher scores are better. The task-wide best-performers are marked in bold. All values are the average scores of two runs.\relax }}{84}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{85}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{section:Conclusion}{{6}{85}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Main Contributions}{85}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Future Work}{86}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing the WiC benchmark:}{86}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Larger benchmarking dataset:}{87}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear subspace still not apparent:}{87}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Formalizing semantics:}{87}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limited coherence in lines of work}{87}\protected@file@percent }
\bibstyle{plain}
\bibdata{dissertation}
\bibcite{cil_slides}{1}
\bibcite{pyax}{2}
\bibcite{spacyb}{3}
\bibcite{news_corpus}{4}
\bibcite{colahLSTM}{5}
\bibcite{wiktionary}{6}
\bibcite{ackerman09}{7}
\bibcite{akbik19}{8}
\bibcite{alvarez18}{9}
\bibcite{arefyev19}{10}
\bibcite{ba16}{11}
\bibcite{bahdanau16}{12}
\bibcite{baker98}{13}
\bibcite{bengio03}{14}
\bibcite{bengio06}{15}
\bibcite{bentivogli2009}{16}
\bibcite{bergstra12}{17}
\bibcite{biemann06}{18}
\bibcite{biemann13}{19}
\bibcite{bojanowski17}{20}
\bibcite{bommasani19}{21}
\bibcite{bowman2015}{22}
\bibcite{brazinskas19}{23}
\bibcite{bromley94}{24}
\bibcite{bruni13}{25}
\bibcite{caliskan19}{26}
\bibcite{camachocollados18}{27}
\bibcite{campello13}{28}
\bibcite{carreras04}{29}
\bibcite{cer2017}{30}
\bibcite{chechik10}{31}
\bibcite{chopra05}{32}
\bibcite{coenen19}{33}
\bibcite{comaniciu02}{34}
\bibcite{conneau17}{35}
\bibcite{dagan2005}{36}
\bibcite{denkowski09}{37}
\bibcite{devlin18}{38}
\bibcite{dolan2005}{39}
\bibcite{ester96}{40}
\bibcite{ethayarajh19}{41}
\bibcite{francis64}{42}
\bibcite{frey07}{43}
\bibcite{gage94}{44}
\bibcite{ganea18}{45}
\bibcite{ge18}{46}
\bibcite{giampiccolo2007}{47}
\bibcite{hadsell06}{48}
\bibcite{hagberg04}{49}
\bibcite{bar2006}{50}
\bibcite{harris54}{51}
\bibcite{hegel17}{52}
\bibcite{hewitt19}{53}
\bibcite{hill15}{54}
\bibcite{hochreiter97}{55}
\bibcite{hoffer14}{56}
\bibcite{spacy}{57}
\bibcite{hu19}{58}
\bibcite{hubert85}{59}
\bibcite{shankar17}{60}
\bibcite{jawahar19}{61}
\bibcite{jentzsch19}{62}
\bibcite{joshi19}{63}
\bibcite{joshi19b}{64}
\bibcite{jozefowicz16}{65}
\bibcite{kageback16}{66}
\bibcite{kaya19}{67}
\bibcite{kudugunta18}{68}
\bibcite{lample18}{69}
\bibcite{lan20}{70}
\bibcite{lee17}{71}
\bibcite{lee19}{72}
\bibcite{levesque2012}{73}
\bibcite{levine19}{74}
\bibcite{liebeskind19}{75}
\bibcite{liu19}{76}
\bibcite{lloyd57}{77}
\bibcite{ma19}{78}
\bibcite{macqueen67}{79}
\bibcite{mahalanobis36}{80}
\bibcite{marcus93}{81}
\bibcite{martinc20}{82}
\bibcite{may19}{83}
\bibcite{mccarthy16}{84}
\bibcite{mickus19}{85}
\bibcite{mihael99}{86}
\bibcite{mikolov13}{87}
\bibcite{mikolov13b}{88}
\bibcite{miller90}{89}
\bibcite{miller91}{90}
\bibcite{miller94}{91}
\bibcite{moradshahi19}{92}
\bibcite{moutafis17}{93}
\bibcite{navigli19}{94}
\bibcite{ni17}{95}
\bibcite{scikit-learn}{96}
\bibcite{pelevina16}{97}
\bibcite{pennington14}{98}
\bibcite{peters17b}{99}
\bibcite{peters18}{100}
\bibcite{pilehvar19}{101}
\bibcite{pradhan13}{102}
\bibcite{radford18}{103}
\bibcite{radford19}{104}
\bibcite{rajpurkar2016}{105}
\bibcite{rand71}{106}
\bibcite{reimers19}{107}
\bibcite{remus18}{108}
\bibcite{ribeiro19}{109}
\bibcite{rippel16}{110}
\bibcite{rousseeuw87}{111}
\bibcite{rumelhart85}{112}
\bibcite{sanh19}{113}
\bibcite{sennrich16}{114}
\bibcite{shen19}{115}
\bibcite{shi19}{116}
\bibcite{shin18}{117}
\bibcite{si19}{118}
\bibcite{socher2013}{119}
\bibcite{sohn16}{120}
\bibcite{song17}{121}
\bibcite{song16}{122}
\bibcite{suarez19}{123}
\bibcite{sun19}{124}
\bibcite{sutskever11}{125}
\bibcite{tan19}{126}
\bibcite{tang19}{127}
\bibcite{tsai19}{128}
\bibcite{tshitoyan19}{129}
\bibcite{ustinova16}{130}
\bibcite{vaswani17}{131}
\bibcite{vilnis14}{132}
\bibcite{wang19e}{133}
\bibcite{wang19b}{134}
\bibcite{wang19}{135}
\bibcite{wang17}{136}
\bibcite{wang19d}{137}
\bibcite{wang19c}{138}
\bibcite{wang13}{139}
\bibcite{warstadt2018}{140}
\bibcite{whitaker19}{141}
\bibcite{wiedmann19}{142}
\bibcite{N18-1101}{143}
\bibcite{wittgenstein53}{144}
\bibcite{Wolf2019}{145}
\bibcite{wu16}{146}
\bibcite{yan19}{147}
\citation{wang19}
\citation{wang19}
\citation{wang19}
\citation{bengio06}
\citation{vilnis14}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Background}{100}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Linguistic features}{100}\protected@file@percent }
\newlabel{appendix:linguistic_features}{{A.1}{100}}
\@writefile{toc}{\contentsline {paragraph}{Other linguistic features}{100}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Table from \cite  {wang19}. Types of linguistic phenomena organized under four major categories.\relax }}{100}\protected@file@percent }
\newlabel{table:1}{{A.1}{100}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Gaussian Embeddings}{100}\protected@file@percent }
\newlabel{appendix:GaussianEmbeddings}{{A.2}{100}}
\citation{vilnis14}
\citation{hochreiter97}
\citation{rumelhart85}
\citation{colahLSTM}
\citation{colahLSTM}
\citation{hochreiter97}
\citation{warstadt2018}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Long Short-Term Memory}{103}\protected@file@percent }
\newlabel{appendix:LSTM}{{A.3}{103}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Figure taken from \cite  {colahLSTM}. The internals of the LSTM cell, and the recurrent flow which is repeated for three consecutive timesteps $t-1, t, t+1$. The LSTM produces an hidden representation at every steep $h_{t-1}, h_{t}, h_{t+1}$ given some inputs $x_{t-1}, x_t, x_{t+1}$. \relax }}{103}\protected@file@percent }
\newlabel{fig:lstm_internals}{{A.1}{103}}
\citation{socher2013}
\citation{dolan2005}
\citation{shankar17}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}GLUE}{104}\protected@file@percent }
\newlabel{appendix:GLUE}{{A.4}{104}}
\@writefile{toc}{\contentsline {subsubsection}{Single Sentence Tasks}{104}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Similarity and paraphrase tasks}{104}\protected@file@percent }
\citation{cer2017}
\citation{frey07}
\citation{ester96}
\citation{campello13}
\citation{mihael99}
\citation{comaniciu02}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Other lines of work}{106}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Clustering}{106}\protected@file@percent }
\newlabel{appendix:Clustering}{{B.1}{106}}
\@writefile{toc}{\contentsline {paragraph}{Affinity Propagation}{106}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{DBScan}{106}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MeanShift}{106}\protected@file@percent }
\citation{tang19}
\citation{tsai19}
\citation{sanh19}
\citation{shen19}
\citation{shin18}
\citation{whitaker19}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Static word embeddings and contextual word embeddings}{107}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Metric Learning and Disentanglement}{107}\protected@file@percent }
\citation{moutafis17}
\citation{mahalanobis36}
\citation{chechik10}
\citation{moutafis17}
\citation{suarez19}
\citation{suarez19}
\citation{suarez19}
\@writefile{toc}{\contentsline {subsubsection}{Non-Deep Metric Learning}{109}\protected@file@percent }
\citation{suarez19}
\citation{kaya19}
\citation{kaya19}
\citation{kaya19}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Taken from \cite  {suarez19}. The individual tiles show the kNN prediction regions by color for every point in the image. Using the unmodified euclidean distancee, this would result in classification regions on the left. The reader can see, learning an appropriate distance, the classification is much more effective (middle). Finally, the dimensionality is also reducable with this dimension while still matching the classification accuracy (right).\relax }}{110}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dimensionality Reduction}{110}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Metric Learning}{110}\protected@file@percent }
\citation{bromley94}
\citation{chopra05}
\citation{hadsell06}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Taken from \cite  {kaya19}. An illustration of deep metric learning. The space is transformed in such a way, that similar object are closer to each other, and dissimilar objects are moved away from each other.\relax }}{111}\protected@file@percent }
\newlabel{fig:muse_translation}{{B.2}{111}}
\citation{hoffer14}
\citation{ustinova16}
\citation{song16}
\citation{sohn16}
\citation{rippel16}
\citation{wang17}
\citation{ni17}
\citation{song17}
\citation{ge18}
\citation{wang19c}
\citation{kudugunta18}
\citation{vaswani17}
\citation{kudugunta18}
\citation{kudugunta18}
\citation{lample18}
\citation{ma19}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.1}Embeddings for translation}{113}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces From \cite  {kudugunta18}, visualizing clustering of the encoder representations of all languages, based on ther SVCCA similarity.\relax }}{113}\protected@file@percent }
\newlabel{fig:embeddings_by_language}{{B.3}{113}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Compressing the non-lexical out}{114}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{114}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{114}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Results}{114}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {C}More Results}{115}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Finding the best clustering model}{115}\protected@file@percent }
\newlabel{section:more_clustering_results}{{C.1}{115}}
\@writefile{toc}{\contentsline {subsubsection}{Quantiative Evaluation}{115}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {C.1}{\ignorespaces The maximum ARI score achieved during hyperparameter optimization for the different models as described by experiment for $k=50$ and $n=500$. \relax }}{115}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {C.2}{\ignorespaces The maximum ARI score achieved during hyperparameter optimization for the different models as described by experiment for $k=50$ and $n=1000$. \relax }}{115}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {C.3}{\ignorespaces The maximum ARI score achieved during hyperparameter optimization for the different models as described by experiment for $k=100$ and $n=1000$. \relax }}{116}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Qualitative Evaluation}{116}\protected@file@percent }
\newlabel{section:more_clustering_results_qualitative}{{C.3}{116}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces Partition 1 for the word \texttt  {bank}. This cluster contains \texttt  {bank} in the context of national banks as financial institutions. Notice that the sentiment is usually a negative one.\relax }}{116}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.2}{\ignorespaces Partition 2 for the word \texttt  {bank}. This cluster contains \texttt  {bank} as a sequence of objects.\relax }}{116}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.3}{\ignorespaces Partition 3 for the word \texttt  {bank}. This cluster contains \texttt  {bank} as a local, consumer bank (in contrast to institutional, national banks).\relax }}{117}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.4}{\ignorespaces Partition 4 for the word \texttt  {bank}. This cluster contains \texttt  {bank} in the context of national banks as financial institutions. Notice that here, the context is usually about interest rates.\relax }}{117}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.5}{\ignorespaces Partition 5 for the word \texttt  {bank}. This cluster contains \texttt  {bank} in the context of investment banks (in contrast to consumer, and institutional banks).\relax }}{117}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.6}{\ignorespaces Partition 1 for the word \texttt  {key}. This cluster contains \texttt  {key} as a synonym for the word \texttt  {main}, which characterizes an important subject. The context is one between large institutions in politics and finance. The sentiment is generally positive.\relax }}{118}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.7}{\ignorespaces Partition 2 for the word \texttt  {key}. This cluster contains \texttt  {key} as a synonym for the word \texttt  {main}, which characterizes an important subject. The context is one of regulations and elections. The sentiment is serious.\relax }}{118}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.8}{\ignorespaces Partition 3 for the word \texttt  {key}. This cluster contains \texttt  {key} as a synonym for the word \texttt  {main}, which characterizes an important subject. The context is one of groups of people. The sentiment is neutral to positive.\relax }}{118}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.9}{\ignorespaces Partition 4 for the word \texttt  {key}. This cluster contains \texttt  {key} as a synonym for the word \texttt  {main}, which characterizes an important subject. The context is one of sports and baseball. The sentiment is mixed.\relax }}{119}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.10}{\ignorespaces Partition 5 for the word \texttt  {key}. This cluster contains \texttt  {key} as a synonym for the word \texttt  {main}, which characterizes an important subject. The context is mixed. The sentiment is mixed .\relax }}{119}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {C.11}{\ignorespaces Partition 6 for the word \texttt  {key}. This cluster contains \texttt  {key} as a synonym for the word \texttt  {main}, which characterizes an important subject. The context is mixed. The sentiment is mixed.\relax }}{119}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C.2}Nominalised Verbs}{119}\protected@file@percent }
\newlabel{section:frozen_verbs}{{C.2}{119}}
\citation{tshitoyan19}
\citation{conneau17}
\citation{conneau17}
\citation{conneau17}
\@writefile{toc}{\contentsline {chapter}{\numberline {D}Applications}{121}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{AppendixApplication}{{D}{121}}
\@writefile{toc}{\contentsline {section}{\numberline {D.1}Using embeddings in other domains}{121}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D.2}Using embeddings in translations}{121}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {D.1}{\ignorespaces Taken from \cite  {conneau17}. Toy illustration of the embedding matching methodology using the MUSE model.\relax }}{121}\protected@file@percent }
\newlabel{fig:muse_translation}{{D.1}{121}}
\citation{alvarez18}
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{124}\protected@file@percent }
