\relax 
\providecommand \oddpage@label [2]{}
\citation{mikolov13}
\citation{mikolov13b}
\citation{vaswani17}
\citation{devlin18}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Motivation}{3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{harris54}
\citation{harris54}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Background}{5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Word Embeddings}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Word-Embeddings}{5}\protected@file@percent }
\newlabel{map:embedding_mapping}{{3.1}{6}}
\@writefile{toc}{\contentsline {paragraph}{Distance}{6}\protected@file@percent }
\newlabel{def:distance}{{3.1}{6}}
\citation{bengio03}
\@writefile{toc}{\contentsline {paragraph}{Learning a distance}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Going from the distributional structure of sentences to learning distances between words.}{7}\protected@file@percent }
\newlabel{eq:naive_sequential_probability}{{3.1}{7}}
\citation{bengio03}
\newlabel{eq:naive_sequential_probability}{{3.1}{8}}
\@writefile{toc}{\contentsline {paragraph}{However, we do not need to only look at only the previous words}{8}\protected@file@percent }
\newlabel{eq:basic_equation_log_maximization}{{3.1}{8}}
\citation{harris54}
\citation{mikolov13}
\citation{mikolov13}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Figure taken from \cite  {mikolov13}. The CBOW architecture predicts the current word based on the context. The Skip-gram predicts surrounding words given the current word.}}{9}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{3.1}{9}}
\newlabel{eq:basic_equation_log_maximization_skipgram}{{3.1}{10}}
\citation{mikolov13}
\citation{mikolov13b}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Static Word Embeddings}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Basic Model}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{11}\protected@file@percent }
\citation{mikolov13b}
\citation{mikolov13b}
\citation{pennington14}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Figure taken from \cite  {mikolov13b}. A 2-dimensional PCA projection of the 1000-dimensional skip-gram vectors of countries and their capital cities. The proposed model is able to automatically organize concepts and learn implicit relationships between them. No supervised information was provided about what a capital city means.}}{12}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{3.2}{12}}
\citation{vilnis14}
\@writefile{toc}{\contentsline {subsubsection}{GloVe}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Embeddings}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Symmetric similarity: expected likelihood or probability product kernel}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Asymmetric divergence: KL-Divergence}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Uncertainty calculation:}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Context Embeddings}{17}\protected@file@percent }
\newlabel{eq:transformer_probability}{{3.1.2}{17}}
\citation{peters17}
\citation{hochreiter97}
\@writefile{toc}{\contentsline {subsubsection}{ELMo}{18}\protected@file@percent }
\citation{vaswani17}
\citation{vaswani17}
\citation{vaswani17}
\citation{devlin18}
\@writefile{toc}{\contentsline {subsubsection}{The Transformer Architecture}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{BERT}{19}\protected@file@percent }
\citation{devlin18}
\citation{devlin18}
\citation{devlin18}
\citation{radford18}
\citation{radford19}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Figure taken from \cite  {vaswani17}. The transformer module architecture. The transformer encapsulates multiple attention layers.}}{20}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{3.3}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Figure taken from \cite  {devlin18}. BERT takes as input multiple tokens, including a position embedding, the token embedding and the segment embedding. This allows BERT to distinguish between the location of the word within a sentence, and which word token was provided and which sentence the word token is a part of.}}{21}\protected@file@percent }
\newlabel{fig:cbow_skipgram}{{3.4}{21}}
\@writefile{toc}{\contentsline {subsubsection}{GPT and GPT-2}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Other methods}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generating "static" word-embeddings through contextual embeddings}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Resources and Datasets}{24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{WordNet}{24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{SemCor dataset}{24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{News dataset}{24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{GLUE benchmark dataset}{24}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Related Work}{25}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Structure inside BERT}{25}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Metric Learning and Disentanglement}{27}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Zero shot and One shot learning }{27}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Clustering Algorithms}{27}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Applications of word vector}{27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{28}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Analysis of the current state of the art}{31}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}On the Linear Separability of meaning within sampled BERT vectors}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Motivation}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Experiment setup}{32}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Checks sampled BERT vectors for linear interpretability by meaning}}{32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Results}{33}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}On the Clusterability of meaning within sampled BERT vectors}{34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Motivation}{34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Experiment setup}{34}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Checks sampled BERT vectors for clusters by meaning}}{34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Results}{35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Correlation between Part of Speech and Context within BERT}{35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Motivation}{35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Experiment setup}{35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Results}{35}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Our Method}{37}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.1}BERnie PoS}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.2}BERnie Meaning}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.3}BERnie Meaning with additional pre-training}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.4}Compressing the non-lexical out}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiment setup}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Further Work}{39}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Evaluation}{41}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibstyle{unsrt}
\bibdata{dissertation}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Conclusion}{43}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
