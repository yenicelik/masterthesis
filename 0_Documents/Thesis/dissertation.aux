\relax 
\providecommand \oddpage@label [2]{}
\citation{mikolov13}
\citation{dropping_perf1}
\citation{dropping_perf2}
\citation{density_matching}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Scope of Work}{1}\protected@file@percent }
\citation{gaussian_embedding}
\citation{density_matching}
\citation{density_matching}
\citation{density_matching}
\citation{flowpp}
\citation{neuralsplineflow}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Motivation}{5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{harris54}
\citation{harris54}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Background}{7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Word Embeddings}{7}\protected@file@percent }
\citation{bengio03}
\citation{bengio03}
\citation{variational_inference_using_normalized_flows}
\citation{nvp}
\newlabel{eq:previous_words}{{3.1}{8}}
\newlabel{eq:previous_words}{{3.1}{8}}
\citation{mikolov13}
\citation{mikolov13}
\citation{mikolov13}
\citation{harris54}
\citation{mikolov13}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Figure taken from \cite  {mikolov13}. The CBOW architecture predicts the current word based on the context. The Skip-gram predicts surrounding words given the current word.}}{9}\protected@file@percent }
\newlabel{fig:boat1}{{3.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Static Word Embeddings}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{9}\protected@file@percent }
\citation{vilnis14}
\@writefile{toc}{\contentsline {subsubsection}{GLoVe}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Embeddings}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Symmetric similarity: expected likelihood or probability product kernel}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Asymmetric divergence: KL-Divergence}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Uncertainty calculation:}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Context Embeddings}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Transformer Architecture}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ELMo}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{BERT}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{GPT and GPT-2}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Other methods}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generating "static" word-embeddings through contextual embeddings}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Related Work}{17}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Structure inside BERT}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Metric Learning and Disentanglement}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Zero shot and One shot learning }{19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Clustering Algorithms}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Applications of word vector}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Word2Vec}{20}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Related Work}{23}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Pseudo-Cross-Label}{25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{online data}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{offline data}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Discrete Methods }{26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Loss in Translation}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}MUSE Facebook using a Min-Max objective}{27}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Methods using Normalising Flows}{30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Density Matching for Bilingual Word Embeddings (Zhou 2019)}{30}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Methods viewing this problem as an Optimal Transport (OT) Problem}{33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Gromov-Wasserstein Alignment of Word Embedding Spaces}{33}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces How to write algorithms}}{34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Supervised Case: Procrustes}{34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Unsupervised Maps: Optimal Transport}{35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Graph translation using normalizing flows}{36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Using monolingual word-embeddings to map one to another}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cross-lingual word-mappings (Artetxe 2018)}{37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bilingual Lexicon Induction through Unsupervised Machine Translation (Artetxe 2019)}{38}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Gaussian Embeddings and Token matching in other applications}{38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Creating Gaussian Embeddings to represent Graphs}{38}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Metric learning on BERT embedding}{39}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Analysis of the current state of the art}{41}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Our Method}{43}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Further Work}{45}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Evaluation}{47}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibstyle{unsrt}
\bibdata{dissertation}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Conclusion}{49}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
