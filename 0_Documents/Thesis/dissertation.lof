\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Figure taken from \cite {mikolov13}. The CBOW architecture predicts the current word based on the context. The Skip-gram predicts surrounding words given the current word.}}{9}% 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Figure taken from \cite {mikolov13b}. A 2-dimensional PCA projection of the 1000-dimensional skip-gram vectors of countries and their capital cities. The proposed model is able to automatically organize concepts and learn implicit relationships between them. No supervised information was provided about what a capital city means.}}{12}% 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Figure taken from \cite {vaswani17}. The transformer module architecture. The transformer encapsulates multiple attention layers.}}{20}% 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Figure taken from \cite {devlin18}. BERT takes as input multiple tokens, including a position embedding, the token embedding and the segment embedding. This allows BERT to distinguish between the location of the word within a sentence, and which word token was provided and which sentence the word token is a part of.}}{21}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
