%% 
%% ACS project dissertation template. 
%% 
%% Currently designed for printing two-sided, but if you prefer to 
%% print single-sided just remove ",twoside,openright" from the 
%% \documentclass[] line below. 
%%
%%
%%   SMH, May 2010. 


\documentclass[a4paper,12pt,twoside,openright]{report}


%%
%% EDIT THE BELOW TO CUSTOMIZE
%%

\def\authorname{Understanding and exploiting subspace organization in contextual word embeddings \xspace}
\def\authorcollege{David Yenicelik \\ ETH ZÃ¼rich\xspace}
\def\authoremail{yedavid@ethz.ch}
\def\dissertationtitle{}
\def\wordcount{14,235}


%\usepackage[dvips]{epsfig,graphics} 
\usepackage{epsfig,graphicx,verbatim,parskip,tabularx,setspace,xspace}
\usepackage{amsmath, amsfonts}
\usepackage{bbm}
\usepackage{amsfonts}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{empheq}
\usepackage{float}
\usepackage{url}
\usepackage{fancyvrb} % for "\Verb" macro


\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tabularx,booktabs}


%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing

% Definition of macros
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
%\newcommand{\bracket}[1]{\left[#1\right]}
\newcommand{\bracket}[1]{\left|#1\right|}
\newcommand{\absdet}[1]{\left|#1\right|}
\newcommand{\bftab}{\fontseries{b}\selectfont}
%% START OF MAIN TEXT 

\pagenumbering{arabic} 
\setcounter{page}{1} 

\newpage
\chapter{Introduction}
 
One of the goals of Natural Language Understanding \textit{NLU} is to formalize written text in a way, such that linguistic features can be captured through computational models, which can then again be used for downstream machine learning tasks.
The most popular methodology in NLU is the use of vectors that represent written text, including word-vectors, sentence-vectors and more. 
These vectors are often referred to as \textit{embedding} vectors, as they embed more complex concepts into a vector representation.
Optimally, the algebra of the space the we operate in, including the various operations such as multiplication and addition should have meaningful relations to the concepts captured by these embeddings. \\

Language models \textit{LM} are a generalization of NLU models that can both generate word-embeddings, sentence embeddings, but also intermediate representations which can be used for downstream tasks.
Most commonly, language models take into consideration the context that a word token appears and, and includes this in the calculation when creating the embedding. \\

Because embeddings - due to their strong usage in downstream tasks - can be considered as the fundamental unit of machine learning in the domain of natural language, any shortcomings will propagate to the performance of the target task.
Thus, improving the way these embeddings have strong implications for any subtasks in NLU, including but not limited to named entity recognition \textit{NER}, sentiment analysis and translation between languages. 
However, most of the modern LMs are so complex that these are considered black-box models.
\\

Although this work does not try to imminently push the performance of these language models, we believe that providing a better understanding of the underlying principles will pave the way for future research to better track these issues.
For the sake of focus, and because we believe that meaning is the most important linguistic feature for communication, we will focus on investigating how modern LMs capture semantics. \\

\subsection{Main Contributions}

Our main contribution lies in conducting experiments which test how semantics is captured in one of the most popular language models.
Because discriminative tasks are simpler than generative ones, first we analyse the linear separability of semantics within embedding vectors produced by BERT. 
Then we check how natural this separability is entailed, by trying to match an implicit generative distribution onto it through clustering.
We then investigate to what effect introducing additional embedding vectors inside modern language models has.
We aim to use simple and easy to understand models for best interpretability of the results.

In conclusion, we show that semantics in modern language models is not interpretable using simple linear models, implying that model complex language models are still required for downstream tasks.
We also show that linguistic features such as sentiment are much more strongly captured in modern language models, allowing for language stereotypes to easily manifest itself, leading to stronger cases of bias in language models.

%TODO Do a final "conclusion", maybe based on semantics

%- Some properties include that static word vectors like word2vec form a bijection between discrete vectors and word tokens. 
%However, because a single word can entail multiple meanings, such as the polysemous word "bank" ( (1) financial institution, (2) a sitting bench ), this results in a lossy compression
%- Other language models like context embeddings entail too much information, and also include other linguistic features such as semantic information, relatenedness to unrelated concepts.
%These properties can easily introduce bias into any downstream tasks.
%- We conjecture that many language tasks incl. translation will benefit most from meaning information
%
%- Our general approach is to start with a complex language model that outputs context embeddings, and find signals / vectors that entail meaning.
%- Although this work is dedicated to the domain of natural language understanding, the principles analysed in this work should generalize to other domains with similar structural properties as well, where we want to denoise some embedding space to some select properties.

%TODO Do a polytope intersection experiment?

%\chapter{Background} 
\chapter{Background} 

% TODO: Take some more from here
%TODO http://da.inf.ethz.ch/teaching/2019/CIL/lecture/CIL2019-05-Word-Embeddings.pdf

\cite{harris54} was one of the early works that argue that that there is inherent structure in language, and that this structure can be formalized.
They also mention that the relation between the linguistic representation in terms of sounds and tokens are related to the meaning that the representation entail. 
This representation is often captured in form of a word token $w$, such as \Verb#bank# or \Verb#cat#.
However, despite the obvious relationship, the distinction between distributional structure of the language and semantics is not a formal one. 
For the above case, the token \Verb#bank# may have different meanings $m_\text{bank-financial instution}$, $m_\text{bank-sea bank}$, whereas the token \Verb#cat# will have a more straight-forward semantic interpretation of $m_\text{cat-animal}$.
\cite{harris54} argues that there is a parallel semantic structure, and argues that this is not a one-to-one relation between vocabulary and different semantic classes.
Generally, one of the man views of this paper is that the meaning of a word is defined by the context it carries.
The formalization of language is not a trivial problem to consider, as this also touches base on a philosophical level \cite{hegel17}, \cite{wittgenstein53}, posing the question on the nature of the relation between though and (language)-representation.

\section{Linguistic Features}

There is a vast number of linguistic features, going from phonological features, morphological and syntactic features to semantic features.
This is not an exhaustive summary, but focusing on the properties of languages relevant to this work. \\

We will first look at language, specifically at properties that are manifested within languages, called linguistic features.
These make a formal analysis of our topic easier.

%TODO perhaps talk about word-tokens
%TODO perhaps talk about character n-grams

\paragraph{Polysemy} describes the phenomenon that a word token $w$ may have multiple meanings.
For the above example the number of meanings that \Verb#cat# can entail is $ | M_\text{cat} | = | \{ m_\text{cat-animal} \} |  = 1$, whereas the number of meanings that \Verb#bank# can entail is $ | M_\text{bank} | = | \{ m_\text{bank-financial instution},  m_\text{bank-sea bank}\} |  = 2$.
These are just examples, but the true numbers vary depending on the granularity chosen by humans (which again is a non-trivial question of when one concept stops, and another distinct concept starts) and the language viewed.
This definition and investigation is often left to linguists to decide. \\

\paragraph{Part of Speech \textit{PoS}} implies the category of syntactic function of a word.
These categories include nouns, verbs and adjectives, which follow certain grammatical rules.
There is a number of such classes, including adjectives \textit{ADJ}, adposition \textit{ADP}, adverbs \textit{ADV}, auxiliary \textit{AUX}, conjunction \textit{CONJ}, coordinating conjunction \textit{CCONJ}, determiner \textit{DET}, interjection noun \textit{INTJ}, noun \textit{NOU}, numeral \textit{NUM}, particle \textit{PART}, pronoun\textit{PRON}, proper noun \textit{PROPN}, punctuation \textit{PUNCT}, subordinating conjunction \textit{SCONJ}, symbol \textit{SYM}, verb \textit{VERB}, as is defined by \cite{spacyb}.
In this work, we will be using the the spaCy python package  \cite{spacy} whenever we need to apply computation on word-tokens to identify the underlying part of speech class.
Within this work, we focus on intuitively interpretable aspects, and thus limit our work on nouns, adjectives, adverbs and verbs.

\paragraph{Other linguistic features} are recorded in \cite{wang19} and include

\begin{table}[h!]
\centering
\begin{tabular}{l l} 
 \hline
 Coarse-Grained Categories & Fine-Grained Categories \\ [0.5ex] 
 \hline
 Lexical Semantics & Lexical Entailment, Morphological Negation, Factivity, \\
&  Symmetry/Collectivity, Redundancy, Named Entities, \\
& Quantifiers \\ 
Predicate Argument Structure & Core Arguments, Prepositional \\ & Phrases, Ellipsis \/ Implicits, \\
& Anaphora/Coreference Active/Passive, Nominalization, \\
& Genitives/Partitives, Datives, Relative Clauses, \\
& Coordination Scope, Intersectivity, Restrictivity \\
Logic & Negation, Double Negation, Intervals/Numbers, \\
& Conjunction, Disjunction, Conditionals, Universal, \\
& Existential Temporal, Upward Monotone \\
& Downward Monotone, Non-Monotone \\
Knowledge & Common Sense, World Knowledge \\ [1ex] 
\hline
\end{tabular}
\caption{Taken from \cite{wang19} Table to test captions and labels}
\label{table:1}
\end{table}


\subsection{Tokens and n-grams}

The most trivial basic unit of language is considered to be a token $w$.
In the above sections, we have implicitly made the assumption that this token $w$ is always a word. 
However, there is a variety of ways that sentences can be encoded into a sequence of tokens.
In a sentence such as

\begin{verbatim}
The man travelled down the road.
\end{verbatim}\label{sentence:man_travelled}

this would constitute the basic units of computation to be the set of 

$$
\{ \text{the}, \text{man}, \text{travelled}, \text{down}, \text{road} \}
$$

The way a sentence is split up into indiviudal basic units of language is referred to as \textit{tokenization}.
As such, the sentence would be tokenized into the sequence (assuming lower-casing is part of the tokenization)

$$
[the, man, travelled, down, the, road]
$$

It is important to note that there are also other ways to interpret language tokens. 
I will provide insights on different formats, as the language models that we will be using use other tokenization methods.

\paragraph{Character}s can be considered as another base unit of language.
For the sentence \eqref{sentence:man_travelled}, the set of basic units would correspond to the latin alphabet plus whitespace $a - z" "$ (" " denotes whitespace), and the above sentence would be tokenized into the sequence 

$$
[t, h e, " ", m, a, n, " ", t, r, a, v, e, l, l, e, d, " ", d, o, w, n, " ", t, h, e, " ", r, o, a, d]
$$

Choosing characters as the basic token levels has shown considerable success and popularity in language modelling \cite{sutskever11} and translation \cite{lee17}.

\paragraph{Byte pair encodings} \cite{gage94} are another popular mechanism to tokenize sentences, and also shows success in the case of language modelling and translation \cite{sennrich16}. 
Here, the most frequent pair of bytes in a sequence is replaced with the most frequent pair of bytes in a sequence with a single, unused byte.

Depending on what corpus this tokenizer is trained on, the above sentence could be tokenized into the following sequence

$$
[the., " ", ma., n., " ", t., rav., e., lled., " ", do., w., n., " ", the., " ", r., oa., d.]
$$

The tokenization of the sentence above implies that the corpus that the tokenizer was trained on includes as a majority of word-occurences the sequence of characters \textit{the}, as well as \textit{lled} and \textit{do}. 
The $.$ (dot) at the end of each character denotes the end of the byte-pair.
The main idea behind this tokenization is to construct a vocabulary of frequent subwords.

\paragraph{WordPiece tokenizer} \cite{wu16} follows a similar idea as the byte-pair encoding, and constructs a vocabulary given a corpus by maximizing the entropy.
The main difference to the byte-pair encoding is that the vocabulary naturally includes a set of the most commonly appearing words in the respective language, and is then aggregated by introducing additional subword units to cover unseen character sequences.

The tokenization of \eqref{sentence:man_travelled} would then look as follows (depending on which implementation and version of the WordPiece tokenizer is used)

$$
[the, man, travel, \#\#led, down., the, ro,  \#\#ad.]
$$

where we can see that words such as \Verb#the# and \Verb#down# are not further divided into tokens, and that suffixes such as \Verb!##led! are introduced, because these are rather unregular patterns, which however allow for better generalization.






\section{Word Embeddings}

Because in the last few decades, computing power has gradually increased, and popularity in using computational methods have surged, we will go over the different word-representations through which we allow computers to execute operations on.

\paragraph{Word-Embeddings}
In general, we want to find a mapping 

\begin{equation}
w \mapsto (x_w, b_w) \in \mathcal{X}^{d + 1}
\end{equation}{\label{map:embedding_mapping}}

where $w$ is a token representation from a vocabulary $w \in V$ and where this token representation is transformed into a $d+1$-dimensional vector representation $x_w$, and a bias-term $b_w$ that is specific to the token $w$.
Whenever we talk about \textit{word-vectors}, \textit{(word)-embeddings}, or \textit{feature-vectors}, we will refer to the image of the above map. 
For convenience and unless stated otherwise, we will assume that $x_w$ absorbs the bias term $b_w$ as an additional vector-element.
Also, for simplicity and unless otherwise stated, we will use the euclidean real-valued vector-space $\mathbb{R}^{d+1}$ to described the resulting embedding vectors.
Please note, however, that the choice of the embedding space $\mathcal{X}$ is not fixed in general, and as such, work in other spaces such as hyperbolic spaces  \cite{ganea18} have also been conducted. 

\paragraph{Distance:} Our goal is to build an embedding space where the distance between word-embeddings correspond to the relation between words (i.e. the semantics entailed by their tokens).
Formally, we introduce the concept of \textit{distance} $d : \mathcal{X}  \times \mathcal{X} \mapsto [ 0, \inf )$, which should capture the relation between different elements. 
For a set of elements $x, y, z \in \mathcal{X}$, following properties must hold for a mapping to be a valid distance metric:

\begin{enumerate}
\item $d(x, y) \geq 0$ (non-negativity)
\item $d(x, y) = 0 \iff x = y$ (identity, i.e. if two embedding vectors are identitical, they must capture the same underlying word instance)
\item $d(x, y) \leq d(x, z) + d(z, y)$ (triangle inequality)
\end{enumerate}{\label{def:distance}}

One consequence of the above rules is that for words $a, b, c$, each having a word embedding $x, y, z$ respectively, $d(x, y) < d(x, z) \iff $ word instance $b$ is conceptually closer to word $a$ than word $c$.
For convenience, whenever I input $a, b, c$ into the distance function $d$, the word-embeddings for the corresponding word-tokens shall be used.
Also, please notice that I left out the notion of symmetry for distance measures.

\paragraph{Learning a distance}
A popular paradigm in machine learning is to maximize a certain probability distribution given some data $\mathbf{X}$.
In the context of word vectors, we want to maximize the probability that $w$ occurs in the context window of $w \prime$ through some parameters $\theta$.
Implicitly, this corresponds to minimizing the distance between $w$ and $w^{\prime}$, while keeping the distance between $w$ and all other words constant. 
We call $w \prime$ a \textit{context word} for $w$.

\begin{equation}
p \left(w | w^{\prime}\right)
\end{equation}

and in the work we focus at, the probabilistic maximization problem corresponds to linear minimization problem

\begin{equation}
\forall w, w^{\prime} : \hspace{20pt} \text{max } p \left(w | w^{\prime}\right) \iff \text{min } d(w | w^{\prime})
\end{equation}

Please note that we do not generalize to formalizing a dual-relationship between the two problem-statements, however.

\paragraph{Exploiting the distributional structure of language:} Our goal is to learn distance between words by exploiting the distributional structure of a set of sentences, which make up a \textit{corpus}. 
One of the early formalizations of representing the distributional structure of words through was expressed in \cite{bengio03}, which argues that a sequential statistical model can be constructed to estimate this true posterior.  
In it's most naive form, this would imply that we can estimate the probability of a word $w^{(t)}$ after words $w^{(t-1)}, \ldots, w^{(1)}$ as

\begin{equation}
p(\mathbf{w}) = \prod_{t=1}^T p\left( w^{(t)} | w^{(t -1)}, \ldots, w^{(1)} \right)
\end{equation}{\label{eq:naive_sequential_probability}}

%TODO check the underlying equation again!!

where $\mathbf{w} = w^{(1)}, \dots, w^{(T)} $ is the sequence of words, $p(\mathbf{w})$ the probability of this sentence occurring.
This corresponds to a simple autoregressive model.
One could for example use (hidden) markov models to model this relation, as we could fix the \textit{context size} $n$ (in this case, only looking at previously occurring words) and include the markov assumption of

\begin{equation}
p\left( w^{(t)} | w^{(t -1)}, \ldots, w^{(1)} \right) = p\left( w^{(t)} | w^{(t -1)}, \ldots, w^{(t - n + 1)} \right)
\end{equation}{\label{eq:naive_sequential_probability_markovian}}

Fixing the context window to $n$ words for each computation (i.e. convolution), introduces the concept of \textit{n-grams}. 
\textit{n-grams} can also occur in relation to character, where $n$ characters are fed in to some model at a certain timestep $t$.

Next to a context which only consists of the \textit{previous} $n$ words, one can also regard a context which includes both the previous \textit{and subsequent} $R$ words. 

\begin{equation}
p(\mathbf{w})=\prod_{t=1}^{T} \prod_{\triangle \in \mathcal{I} } p\left( w^{(t)} | w^{(t +\Delta)}\right)
\end{equation}{\label{eq:naive_sequential_probability}}

whose probability we wish to estimate (and maximize if this is in the given training dataset), $\mathcal{I}=\{-R, \ldots,-1,1, \ldots, R\}$ is the context window. 

\paragraph{Loss Objective} The above function is a probability estimate of the true underlying distribution.
However, given that computational power is limited, we are usually interested in learning a parametrized model which captures this underlying distribution.
One way to learn this parametrized model is to minimize a loss by backpropagating the gradients of the parameters.
In that case, the loss function would look as follows.

%TODO I guess this is the skip-gram approach, the bag-of-words is again a different one

\begin{equation}
\mathcal{L}(\theta ; \mathbf{w})= - \prod_{t=1}^{T} \prod_{\triangle \in \mathcal{I}} p_{\theta}\left(w^{(t)} | w^{(t +\Delta)}\right)
\end{equation}{\label{eq:basic_equation_log_maximization}}

By minimizing the mean loss over all sentences in a big-enough corpus $\mathcal{C}$, we can arrive at a reliable estimator model with parameters $\theta$, given the model is complex enough and generalizes well.

Specifically, if $p_\theta$ is a parametric probability density function, one can then optimize for the most optimal $\hat{\theta} = \text{argmin}_\theta \mathcal{L}(\theta ; \mathbf{w})$ using a maximum likelihood estimation approach.
One would usually minimize the log-loss as this allows for thee gradients to be more easily computed and avoids numerical issues.

%TODO is this only training, or does this also have other implications..?
Intuitively the above models flow the idea expressed by \cite{harris54} very well, speaking that the meaning of a word is captured by its neighborhood.
However, the above equations follow a \textit{continuous bag of words CBOW} approach, which aims at predicting a target word $w^t$ given some context words $w^{\prime}$.
However, it is also possible to follow a \textit{skip-gram SG} approach, where the aim is to predict context words $w^{\prime}$ given a target word $w^t$.
Both methods result in the same type of model, which is merely trained differently.
The skip-gram model is found to work better with rare words, whereas the CBOW model seems to have slightly better accuracy on words that are occuring more frequently.


\begin{figure}[h]
	\center
  \includegraphics[width=0.6\linewidth]{./assets/background/cbow_and_skipgram.png}
  \caption{Figure taken from \cite{mikolov13}. The CBOW architecture predicts the current word based on the context. The Skip-gram predicts surrounding words given the current word.}
  \label{fig:cbow_skipgram}
\end{figure}

Using a skip-gram approach, \eqref{eq:basic_equation_log_maximization} for example would be reformulated into 
\begin{equation}
\mathcal{L}(\theta ; \mathbf{w})=\prod_{t=1}^{T} \prod_{\triangle \in \mathcal{I}} p_{\theta}\left(w^{(t +\Delta)} | w^{(t)}\right)
\end{equation}{\label{eq:basic_equation_log_maximization_skipgram}}

Although we have mentioned that the probability density distribution $p$ is often modelled through a parametrized function (due to the sheer amount of data within corpora), we have not presented methods on how this parametrization can be achieved.
In the following section, we will show how vectors for $x_w$ and $b_w$ can be found, and other ways to parametrize the probability density function $p$.
We will not go into too much detail as to how these models are trained, as all of them can be trained by applying a gradient-based optimization on the loss term.

\newpage
\subsection{Static Word Embeddings}

Here we will talk about word-embeddings where each word-token only has a single embedding $x_w$, i.e. there is a bijection between word-tokens and word-embeddings. 
Specifically, the mapping \eqref{map:embedding_mapping} is not a probabilistic function with a latent random factor, but rather a deterministic one.
We will go over the basics of static word embeddings only, as the focus of this work lays in context embeddings which are presented in the subsequent section.

\subsubsection{Basic Model}

The first model we are going to look at is a most basic model which fulfills the properties of the distance metrics shown in \eqref{def:distance}.

Here, we can introduce a \textit{log-bilinear} model where the log-probability is define as

\begin{equation}
\text{log} p_{\theta}(w | w^{\prime}) = \left\langle\mathbf{x}_{w}, \mathbf{x}_{w^{\prime}}\right\rangle+b_{w} + \text { const. }
\end{equation}

To arrive at the actual probability, we can exponentiate the log-probability as such

$$
p_{\theta}\left(w | w^{\prime}\right)=\frac{\exp \left[\left\langle\mathbf{x}_{w}, \mathbf{x}_{w^{\prime}}\right\rangle+b_{w}\right]}{Z_{\theta}\left(w^{\prime}\right)}
$$

where $Z_{\theta}\left(w^{\prime}\right):=\sum_{v \in \mathcal{V}} \exp \left[\left\langle\mathbf{x}_{v}, \mathbf{x}_{w^{\prime}}\right\rangle+b_{v}\right]$ is a normalization constant such that the probability mass sums to $1$, and the model parameters entail the word-embeddings $\theta = (x_w, b_w) \in \mathbb{R}^{d+1}$

%TODO CIL lecture slides
% http://da.inf.ethz.ch/teaching/2019/CIL/lecture/CIL2019-05-Word-Embeddings.pdf

Because we have $\theta = \forall w \in \mathcal{V} : \{ x_w, b_w \}$, we can use a gradient-based loss-minimizing method which minimizes the cost function for $\theta$, for a word $w$ and all their possible context words $w^{\prime} = w^{(t+\Delta)}$ within a context size $R$.

$$
\mathcal{L}(\theta ; \mathbf{w})=\sum_{t=1}^{T} \sum_{\Delta \in \mathcal{I}} \log p_{\theta}\left(w^{(t+\Delta)} | w^{(t)}\right)
$$.

This model basic model comes with certain drawbacks. 
Amongst others the distance would be minimized if all word-embeddings would collapse onto a single point. 
%TODO is this true?
There is no term that forces unrelated words to move away from each other, a property that we are interested in as unrelated words should form embedding vectors that are far from each other.


\subsubsection{Word2Vec}

%TODO look at this more closely again
One of the most prominent of word vector models is proposed by \cite{mikolov13, mikolov13b}.
Here, a neural network with a single embedding layer can be trained to transform one-hot-vectors $\in \{ 0, 1 \}^{| \mathcal{V} |}$ which represents a word $w$ in vocabulary $\mathcal{V}$
into a latent vector representation $x_w \in \mathbf{R}^{d + 1}$ using a neural network - which boils down to a linear embedding matrix - $W$ both a continuous bag of word and also a continuous skip-gram approach.
The skip-gram approach is preferred in practice.

Specifically, the loss-function that is optimized looks as follows

\begin{align} 
\mathcal{L}(\theta ; \mathbf{w}) & =\sum_{t=1}^{T} \sum_{\Delta \in \mathcal{I}} [\\ 
& b_{w^{t+\Delta}} +\left\langle \mathbf{x}_{w^{(t+\Delta)}}, \mathbf{x}_{w^{(t)}} \right\rangle \\
& -\log \sum_{v \in \mathcal{V}} \exp \left[\left\langle\mathbf{x}_{v}, \mathbf{x}_{w^{(t)}}\right\rangle+b_{v} \right] 
\end{align}

As one can see, the loss function takes as input the bilinear loss from the basic model, and complements this by adding a regularizing term , such that random samples are not put next to each other.
This idea is referred to as \textit{negative sampling}.

\begin{figure}[h]
	\center
  \includegraphics[width=0.6\linewidth]{./assets/background/word2vec_cities.png}
  \caption{Figure taken from \cite{mikolov13b}. A 2-dimensional PCA projection of the 1000-dimensional skip-gram vectors of countries and their capital cities. The proposed model is able to automatically organize concepts and learn implicit relationships between them. No supervised information was provided about what a capital city means.}
  \label{fig:cbow_skipgram}
\end{figure}

%TODO include difference between target vocabulary V and context vocabulary C

Backpropagation is used to optimize over the networks weights. 
Also, highly frequent words are optionally subsampled and their frequency is re-weighted using the formula $$
P\left(w_{i}\right)=1-\sqrt{\frac{t}{f\left(w_{i}\right)}}
$$

where $f( \dot )$ is the function that returns the frequency of a word $w_i$.

\subsubsection{GloVe}

For the global vectors for word representation \textit{GloVe} \cite{pennington14}, the authors follow a more straight-forward matrix factorization approach.

First, a global co-occurence matrix is created $\mathbf{N} = (n_{ij}) \in \mathbb{N}^{|\mathcal{V}| \dot |\mathcal{C}|}$ where each entry $n_{ij}$ is determined by the number of occurrences of word $w_i \in \mathcal{V}$ in context $w_j \in \mathcal{C}$.
Given that the vocabulary size can exceed multiple thousand items, this practically results in a sparse matrix.

%TODO do i have to cite work i take from lecture slides?

\begin{align}
\mathcal{H}(\theta ; \mathbf{N}) &=
\sum_{i, j} f\left(n_{i j}\right)(\underbrace{\log n_{i j}}_{\text {target }}-\underbrace{\log \tilde{p}_{\theta}\left(w_{i} | w_{j}\right)}_{\text {model }})^{2} \\
&= \sum_{i, j} f\left(n_{i j}\right)(\log n_{i j} - \left\langle x_i, y_j \right\rangle )^{2} \\
\end{align}

where $f(n_{ij})$ is the weighting function which assigns a weight for each entry in the co-occurence matrix based on the co-occurence of words $w_i$ and $w_j$. 
In the second line, we also again use a bilinear probability density model $\tilde{p}_{\theta}\left(w_{i} | w_{j}\right)=\exp \left[\left\langle\mathbf{x}_{i}, \mathbf{y}_{j}\right\rangle+b_{i}+c_{j}\right]$.
The constants $b_i, c_j$ are left out and are assumed to be absorbed in the embedding vectors.

A popular choice for the weighting function is 

$$
f(n) = \min \left\lbrace 1, \left(\frac{n}{n_{\max}}\right)^{\alpha} \right\rbrace
$$

with $\alpha \in (0, 1]$.
The motivation behind this is that frequent words do not receive a word-frequency which is considered too high (there is a cutoff at some point) and small counts are considered noise and slowly cancelled out.

Further extensions which include other basic units of include for example the fastText embeddings \cite{bojanowski17}.

\subsubsection{Gaussian Embeddings}

Gaussian embeddings is the only type of distribution representation that we cover here. 
\cite{bengio06} already used distributional representations to embed concepts from natural language processing into embeddings.

\cite{vilnis14} considers creating a Gaussian representation for each word token, resulting in Gaussian word embeddings.
This can be interpreted as a probabilistic and continuous extension to the otherwise common discrete point vectors.
Each word is represented by a Gaussian distribution in high-dimensional space, with the aim to better capture uncertainty when doing algebraic operations on the word-embeddings, resulting in a model which expresses the relation between words and their representations better.
Because the probabilistic framework is more flexible, it can also express asymmetric relations more naturally than dot-products or cosine similarities do.  
Specifically, the newly emerging embedding for word $w$ can be modelled by the tuple 

$$
\theta_w = ( \mu_w,  \Sigma_w)
$$

and the probability density function of the word embedding $x_w$ is expressed by

\begin{equation}
p(x; w) = \mathcal{N}\left(x ; \mu_{w}, \Sigma_{w}\right)
\end{equation}

where $x$ is a point in the embedding space. \\

Given that the probability density function is given by a gaussian distribution, the loss functions which is minimized is adapted accordingly:

\begin{equation}
L_m(w, c_p, c_n) = - min(0, m - E_\theta(w, c_p) + E_\theta(w, c_n)
\end{equation}

where $E$ is the energy function which calculates the distance measure between word $w$, a positive context word $c_p$ (a word that co-occurs with the word $w$, and $c_n$), and a negative-sampled context word $c_n$ (a word that does not co-occur with the word $w$, usually randomly sampled from the set of all words in the vocabulary $\mathcal{V}$).

\cite{vilnis14} proposes two different ways to compute the energy function, using a symmetric similarity, and an asymmetric similarity measure.


% Checkpoint

The energy function can be \textbf{symmetric}.
In this case, the authors use an inner product of two gaussian distributions defined as:

\begin{align}
E_\theta(w, c) &= \int_{x \in \mathcal{R}^d} f(x)g(x) dx \\
&= \int_{x \in \mathcal{R}^d} \mathcal{N}(x; \mu_w, \Sigma_w) \mathcal{N}(x; \mu_c, \Sigma_c) dx \\
&= \mathcal{N}(0; \mu_w - \mu_c, \Sigma_w + \Sigma_c)
\end{align}

which results in a nice closed form representation. \\

For numerical feasibility and easy of differentiation, usually the log-loss $\text{log} E_\theta(w, c_p, c_n)$ is optimized the for a given dataset with $w \in \mathcal{V}, c_p, c_n \in \mathcal{C}$ (usually $\mathcal{V} = \mathcal{C}$, unless the set of context words is different.

%TODO check if all minimize optimize maximize are correctly termed

The energy function can also be \textbf{asymmetric}.
In this case, the authors refer to the KL-divergence, resulting in the following energy function minimized.

\begin{align}
-E(w_i, c_j) & = D_{KL}(c_j || w_i) \\
&= \int_{x \in \mathcal{R}^d} \mathcal{N}(x; \mu_{w_i}, \Sigma_{w_i}) \text{log} \frac{\mathcal{N}(x; \mu_{c_j}, \Sigma_{c_j})}{\mathcal{N}(x; \mu_{w_i}, \Sigma_{w_i})} dx \\
&= \frac{1}{2}\left(\operatorname{tr}\left(\Sigma_{i}^{-1} \Sigma_{j}\right)+\left(\mu_{i}-\mu_{j}\right)^{\top} \Sigma_{i}^{-1}\left(\mu_{i}-\mu_{j}\right)-d-\log \frac{\operatorname{det}\left(\Sigma_{j}\right)}{\operatorname{det}\left(\Sigma_{i}\right)}\right)
\end{align}

Because of the loss function, this can entail information such as "y entails x", without necessarily implying "x entails y" (a property that symmetric loss functions cannot model).

Assuming training was conducted by one of the two presented models,  one can use the resulting model parameters for two words $x_{w_1}, x_{w_2}$ to calculate the uncertainty - which intuitively models the similarity between the two words $w_1$ and $w_2$.
This is analogous to calculating the dot product between the two distributions $P(z=x^T y)$ through the close-form formulation of

\begin{align}
\mu_z &= \mu_x^T \mu_y \\
\Sigma_z &= \mu_{x}^T \Sigma_{x} \mu_{x}+\mu_{y}^T \Sigma_{y} \mu_{y}+\operatorname{tr}\left(\Sigma_{x} \Sigma_{y}\right)
\end{align}

We then get an uncertainty bound, where $c$ denotes the number of standard deviations away from the mean.
This uncertainty bound can be interpreted as a hard cutoff to where one concept end, allowing to make calculations between embeddings.
\begin{equation}
\mu_{x}^{\top} \mu_{y} \pm c \sqrt{\mu_{x}^{\top} \Sigma_{x} \mu_{x}+\mu_{y}^{\top} \Sigma_{y} \mu_{y}+\operatorname{tr}\left(\Sigma_{x} \Sigma_{y}\right)}
\end{equation}

We can learn the parameters $\Sigma$ and $\mu$ for each of these embeddings using a simple gradient-based approach, where we set constraints on 

\begin{align}
\forall i \quad & \norm{ \mu_i }_2  \leq C\\
\forall i \quad & m I <  \Sigma_i < M I
\end{align}

These constraints can be used by using a laplacian regularizer in the loss function which is then optimized using a gradient based approach. 
The method shows competitive scores to the Skip-Gram model, although usually only with minor improvements depending on the benchmark-dataset.

\newpage
\subsection{Context Embeddings}

Context embeddings follow the idea of the static word embeddings.
In contrast to static word embeddings, while context embeddings also produce embeddings for words, the produced embedding vector vary depending on the context $\mathbf{c}$ that a word $w$ carries.

Specifically, the mapping for word embeddings \eqref{map:embedding_mapping} is adapted in such a way to also take as input the previous context words $c_\text{previous} = w^{(t)}, \ldots, w^{(t-d + 1)}$, and subsequent context word $c_\text{subsequent} = w^{(t-d - 1)}, \ldots, w^{(1)}$ for a given word of interest $w$.

\begin{equation}
(c_\text{previous}, w, c_\text{subsequent}) \mapsto (x_w) \in \mathcal{X}^{d + 1}
\end{equation}{\label{map:context_embedding_mapping}}

As an example, while static word embeddings would produce the same vector $x_1$ for all occurrences of the word $bank$, the context embedding would produce different embeddings $x_1$ and $x_2$ respectively for the two sentences:

\begin{verbatim}
I withdrew some cash at the bank's ATM.
I walked down the sea bank.
\end{verbatim}

Up to some exceptions, any deviation of the context will result in a deviation of the resulting context embedding.

%TODO Make a better transition to the log-probability argument

The underlying model is a many-to-one sequence estimator, in general modelling the probability of 

%TODO Is this joint probability given, or is t-d taken out?
\begin{align}
p(w^{(t-d)}; c_\text{previous}, w^{t-d}, c_\text{subsequent}) &= p(w^{(t-d)}; w^{(t)}, \ldots, w^{(t-d + 1)}, w^{(t-d)}, w^{(t-d - 1)}, \ldots, w^{(1)})
\end{align}{\label{eq:transformer_probability}}

instead of independently querying the probability $p(w^{(t-d)})$ without any context words.

There are different ways to achieve modelling this probability, from simple markov models, to LSTMs to transformers.
We will go over the most prominent models of the past few years, discussing their underlying mechanism.

\subsubsection{ELMo}

%TODO Is ELMo pre-trained???

The simplest idea of context embeddings, which take into account more than just a single word, but all tokens from the context $\mathbf{c} = (c_\text{previous}, c_\text{subsequent}) $is the \textit{Embeddings from Language Models} (ELMo) model proposed by \cite{peters17b}.

The most basic architecture of ELMo is an extension of the Long Short-Term Memory \textit{LSTM} architecture.
LSTMs are sequential recurrent neural networks \cite{hochreiter97}.
LSTMs are an extension of the recurrent neural network \textit{RNN} \cite{rumelhart85}, but introduces a mechanism to solve the problem of error in the backpropagating gradient, and a "sotrage" mechanism which allows part of the RNN cell to be non-changed throughout backpropagation update mechanisms.
%TODO how do you best explain LSTMs?

The internal cell of the LSTM includes the following mechanisms

\begin{figure}[h]
	\center
  \includegraphics[width=\linewidth]{./assets/background/LSTM.png}
  \caption{Figure taken from \cite{colahLSTM}. The internals of the LSTM cell, and the recurrent flow which is repeated for three consecutive timesteps $t-1, t, t+1$. 
  The LSTM produces an hidden representation at every steep $h_{t-1}, h_{t}, h_{t+1}$ given some inputs $x_{t-1}, x_t, x_{t+1}$.
  }
  \label{fig:lstm_internals}
\end{figure}

Notice that the flow has a direction, and that in this case, the flow moves towards a positive direction of the sequence (i.e. from $t-1 = 0$ to $t + 1 = 2$ for example).

Specifically, LSTMs \cite{hochreiter97} are good at estimating the probability of word $w^{(t)}$ occurring given a previous history of words $w^{(t-1)}, \ldots w^{(1)}$.
In practice, the LSTM is a popular choice for sequence modeling tasks, as it is able to capture very well the following probability distribution.

%TODO This formula does not seem very correct ...
\begin{equation}
p\left(w_{1},  w_{2}, \ldots, w_{T} \right)=\prod_{k=1}^{T} p\left(w_{k} | w_{k-1}, \ldots, w_{2}, w_{1}\right)
\end{equation}

When we apply two LSTM layers, one that reads the sequence in the positive direction - as decpicted in \eqref{lstm_internals} - and another one that reads the sequence in a negative direction (i.e. reads in the sequence $[\ldots, x_{t+1}, x_t, x_{t-1}, \ldots]$ in this particular direction), the resulting architecture is called bidirectional LSTM.
In its underlying architecture, ELMo is using bidirectional Long Short-Term Memory architectures \textit{bi-LSTM}.
Specifically, a possible loss function results in 

\begin{align} 
\sum_{k=1}^{T} &\left(\log p\left(w_{k} | w_{k-1}, \ldots, w_{1} ; \Theta_{x}, \vec{\Theta}_{L S T M}, \Theta_{s}\right)\right.\\
+&\left.\log p\left(w_{k} | w_{k+1}, \ldots, w_{T}; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right) 
\end{align}

where $p$ is an LSTM model parametrized by it's weights $\theta$, including softax heads to arrive at a valid probability distribution amongst words in the vocabulary $\mathcal{V}$.

Given that the LSTM is a sequential model, it outputs as many hidden representations as there are timesteps $T$ that are given as input to the model. 
Each LSTM layer produces one set of such hidden representations.
ELMo concatenates the hidden representations for both of the bidireectional LSTM layers to arrive at a single hidden representation.
Within ELMo, each one of these hidden representations are interpreted as one of the context embeddings, which can then be used for downstream tasks.
As such, we retrieve an embedding vector $x_{w^i} = [h^\text{backward}_{w^i}, h^\text{forward}_{w^i}]$ for word $w^i$
Each context embeddings outputs a vector representing $x_{k}$ for the word token $w^{(k)}$, given its context $ w_{1}, \ldots, w_{k-1}  w_{k+1}, \ldots, w_{N}$.

%TODO Mention how training is done here?

Formally, one would pre-train this model on a huge corpus in an unsupervised fashion.
This pre-trained language model would then be extended by another linear layer, for example, to be adapted to a task at hand which has less data.
This downstream task could be a named entity recognition, part of speech tagging etc.
The gradients of the ELMo biLSTM weights are backpropagated and updated for the final fine-tuning process.

%CHECKPOINT

%TODO model a probability model which is 1-step markovian, and can only cover 

\subsubsection{The Transformer Architecture}

Although ELMo is able to crerate context embeddings, the performance is limited to LSTMs which can only capture timestep-directed sequences.

Specifically, the LSTM used within ELMo is a sequential timestep model which covers a storage mechanism within the weights it uses, resulting  in a 1-step markovian probability-assumption

\begin{align}
p(w_1, \ldots, w_T) &= p(w_T | w_{T-1}; h_{T-1}, \theta) \nonumber  \\
& \ldots p(w_t | w_{t-1}; h_{t-1}, \theta) p(w_{t-2} | w_{t-2}; h_{t-2}, \theta) \nonumber \\
& \ldots p(w_0 ; \mathbf{0}, \theta) 
\end{align}

where $h_{i}$ is the forward-propagation vector outputted by the LSTM cell at timestep $i$, which are supposed to capture some concept of storage together with the models weights $\theta$.
Although intuitively $h_{T-1}$ is supposed to capture information from many timesteps back, there is no explicit information flow between the LSTM cell at timestep $t+1$ and $t-1$ (i.e. any two cells which are longer than 2 steps away).

This is something where the transformer architecture comes in.
The first transformer architecture for sequence modelling of text was introduced in \cite{vaswani17}.
The transformer architecture introduces the concept of attention for modern language models.
% A single transformer takes as input a sequence $\mathbf{w} = [w_1, \ldots, w_T]$ and outputs a set of hidden.
It takes the full sample sentence including all timesteps, and calculates a correlation between the hidden representations of each timestep.
Internally, it calculates hidden representations, uses the attention mechanism to rank similarity between the hidden representations, and then outputs a sample predicted sentence using the output probabilities.

%TODO I am skipping the explanaion of attention. Should I do this?

\begin{figure}[H]
	\center
  \includegraphics[width=0.5\linewidth]{./assets/background/transformer_module.png}
  \caption{Figure taken from \cite{vaswani17}. The transformer module architecture. The transformer encapsulates multiple attention layers.}
  \label{fig:attention_is_all_you_need}
\end{figure}

In the context of sentences, it is able to take as input a full sequence of words.
For each word, a vector representation is computed using an embedding layer.
%TODO what is the key, and what is the value
For the input sequence, this results in a set of vectors $V$ which represents the values, and $K$ which represents the keys.
One can then take set of query vectors $Q$ and calculate the inner product $\langle K, Q \rangle$.
One then applies a softmax this set of dot-products to arrive at a notion of which two vector are similar to each other.
In the case of self-attention, the query vectors $Q$ are equivalent to $K$.
This allows for the model to be more attentive between two elements, and allows the model to focus exploit relationships between different tokens within a token-sequence better.

%TODO Not sure if this is well enough written

This has implications for the complexity of the model.
Instead of calculating the raw probability of \eqref{naive_sequential_probability}, and even using a markovian assumption because computation power is expensive, the transformer architecture implicitly calculates the joint probability of an entire sequence of words, thus decreasing the step of computations through which information flows. \\

We will not go into further detail of how and why the transformer architecture works better than the LSTM to keep focus.
All of the below presented models use the transformer architecture as a building block instead of the LSTM used in ELMo.
The following architectures are all based on the transformer module.

\subsubsection{BERT}
 
As introduced in \cite{devlin18}, the Bidirectional Encoder Representations from Transformers model (BERT) combines the above concepts of forward and backwards sequential models with attention, as marked in the transformer architecture above.

Although there are different version of BERT published, the base model of BERT consists of 12 attention layers, which output a 768-dimensional hidden representation of the input sequence, which on the final layer is outputted to work with any target task.

The difference between BERT and ELMo (and GPT which is introduced in the next section) is depicted in the following figure.

\begin{figure}[H]
	\center
  \includegraphics[width=\linewidth]{./assets/background/BERT_GPT_ELMo.png}
  \caption{Figure taken from \cite{devlin18}. BERT uses a bidirectional transformer, which is not limited to reading in all the input from only left to right or right to left. OpenAI GPT (next section) uses a left-to-right Transformer, while ELMo is using a bidirectional LSTM which naturally captures a direction. }
  \label{fig:attention_is_all_you_need}
\end{figure}

%TODO Talk about what kind of input these take-in
%TODO Look at bit more closely into the attention mechanism?
%TODO Can draw some more inspiration from this here

As stated in the transformed architecture, the main advantage in BERT is that the information flow between any two tokens is shortened because there is not a intermediate latent representation laying between the flow of hidden representations of any two timesteps.
Specifically, the attention mechanism allows for a direct comparison of hidden states $h_t$ and $h_k$ with ($|t - k| > 1$), whereas the paradigm of the LSTM would be to capture any relation between the two hidden representations in the weights of the LSTM, as well as the ongoing hidden representation which is passed on at every timestep (i.e. $h_{t-1}$ would have to contain the information about $h_k$ where $k < t - 1$).
by using multiple attention layers of on top each other, BERT is a deep model, allowing for more complex patterns to be trained. \\

BERT is trained using two phases. 
The first phase is BERT \textbf{pre-training}, where the model is trained using a masked language model approach. 
Here, about 15\% of words in a sample are replaced with the "mask" token, and the goal of the model is to predict the word which was replaced by the \Verb#[MASK]# token.

\begin{figure}[H]
\begin{verbatim}
[CLS] The man went to [MASK] store. [SEP]
\end{verbatim}
\caption{Example from \cite{devlin18}. 
An input sentence which where 15\% of the tokens are replaced with the [MASK] token. 
During pre-training, the weights of the BERT model are optimized in such a way to predict the true underlying words.
The word to be predicted is \textit{the}
}
\end{figure}

The second pre-training task which is trained analogously to the masked language model prediction is the \textbf{next sentence prediction} task.
Here, the language model is given two sentences, and is supposed to predict whether the second sentence is the continuation of the first one.
50 \% of the training set here consists of randomly sampled sentences, and the other 50 \% of the training set consists of the actual next sentence for a corpus.

\begin{figure}[H]
\begin{verbatim}
[CLS] the man went to [MASK] store [SEP]
he bought a gallon [MASK] milk [SEP]
\end{verbatim}
\caption{Example from \cite{devlin18}. 
Two input sequences which where 15\% of the tokens are replaced with the [MASK] token. 
During pre-training, the weights of the BERT model are optimized in such a way to predict the true underlying words.
In this case, the second sentence is a continuation of the first one, and thus the label would be \textit{isNext}.
}
\end{figure}



%TODO Story cloze task.

BERT is also a pre-trained model which is trained on the scraped english wikipedia corpus.

BERT introduces itself as a pre-trained language model.
Specifically, it shall be used to fine-tune on specific downstream tasks.

%TODO talk about masked training


\begin{figure}[h]
	\center
  \includegraphics[width=\linewidth]{./assets/background/BERT_multiple_input_tokens.png}
  \caption{Figure taken from \cite{devlin18}. BERT takes as input multiple tokens, including a position embedding, the token embedding and the segment embedding. This allows BERT to distinguish between the location of the word within a sentence, and which word token was provided and which sentence the word token is a part of.}
  \label{fig:cbow_skipgram}
\end{figure}


Multiple versions of BERT are provided, including $BERT_{LARGE}$ which includes %TODO input number of parameters
, and $BERT_{BASE}$ which includes %TODO number of parameters

Ever since BERT came out, a wide variety of similar models came out that mimick the main logic of BERT, and improve upon it, such as DistillBERT \cite{sanh19}.

Numerous works have been published on making BERT representations more compact, for it to be used for on-device text-processing applications, such as more compact sentence-representations \cite{shen19}.

%TODO talk about bert tokenizers..

%TODO Hint at the hubness problem \cite{conneau17}

\subsubsection{GPT and GPT-2}

First proposed in \cite{radford18} and further extended in \cite{radford19}.

\cite{radford18} introduces the concept of \textit{generative pre-training} and discriminative fine-tuning \textit{generative pre-training}.

\paragraph{Unsupervised pre-training} consists of an unsupervised corpus of tokens $\mathcal{U} = \{ u_1, \ldots, u_n \}$ and use the common objective of 

\begin{equation}
L(\mathcal{U}) = \sum_i \log P(u_i | u_{i-k}, \ldots, u_{u-1}; \theta )
\end{equation}

where $k$ describes a context window, and the conditional probability $P$ is modeled using a neural network wth parameters $\theta$. 
The transformer decoder \cite{Liu18} is used which is a variant of the transformer \cite{vaswani17}, which introduces a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens as such:

\begin{align}
h_0 & = UW_e + W_p \\
h_l & = \text{transformer\_block}(h_{l-1}) \forall i \in [1, n] \\
P(u) & = \text{softmax}(h_n {W_e}^T)
\end{align}

where $U=\left(u_{-k}, \ldots, u_{-1}\right)$ is the context vector of tokens, $n$ is the number of layers, $W_e$ is the token embedding matrix, and $W_p$ is the position embedding matrix.

\paragraph{Supervised fine-tuning} consists of leveraging a labeled dataset $\mathcal{C}$, where each instance consists of a sequence of input tokens, $x^1, \ldots, x^m$ along with a label $y$.
The inputs are passed through the pre-trained model to obtain the final transformer block's activation ${h_l}^m$ which is then fed into an added linear output layer with parameters $W_y$ to predict $y$.

\begin{equation}
P\left(y | x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)
\end{equation}

which then gives the following objective to maximize:

\begin{equation}
L_{2}(\mathcal{C})=\sum_{(x, y)} \log P\left(y | x^{1}, \ldots, x^{m}\right)
\end{equation}

As suggested in \cite{Rei17} and \cite{peters17a}, an auxiliary language modeling loss (unsupervised training) is added which accelerates convergence and improves generalization

\begin{equation}
L_{3}(\mathcal{C})=L_{2}(\mathcal{C})+\lambda * L_{1}(\mathcal{C})
\end{equation}

with a regularization parameter $\lambda$.


%TODO read the two papers and take out anything significantly different to BERT.

GPT-2 improves on the first version of GPT.
Firs, the training dataset uses a crawled text of all outbound texts from Reddit resulting in the WebText corpus. 
The crawling als prioritizes documents by quality.

The second major modification is in input representations.
GPT-2 uses a Byte Pair Encoding (BPE) scheme,
\cite{sennrich15}, which operates on unicode code points as the most fundamentla unit of language.
The BPE tokenization is modified by avoiding merging across character categories for any byte sequence (i.e. for words that occur in many variations, such as \Verb#dog.#, \Verb#dog!#, and \Verb#dog?#)
This is similar to the character level rnns (cite jason).

The base model of GPT is modified by moving the layer normalization \cite{ba16} layer to the input of each sub-block and an additional layer normalization after the final self-attention block. 
Also, initialization was modified to account for the accumulation of the residual path with model depth.
Also, vocabulary is expanded to 50257 tokens.


\subsection{Other methods}

Although the above presented methods are the prevailent methods for word-embeddings, there are also other methods which do not clearly fit into one of the above categories.

\subsubsection{Generating "static" word-embeddings through contextual embeddings}

Some work has been done in extracting word-embeddings from contextual language models like BERT or ELMo.

CITE (BERT WEARS GLOVES: DISTILLING STATIC EMBEDDINGS FROM PRETRAINED CONTEXTUAL REPRESENTATIONS)

(1) Uses \textit{pooling} between BERT tokens to arrive at a single representation between words.

Here, sentences are split by space (tokenized).
Words are tokenized further into a subword as defined by WordPiece (Wu et al. 2016).

The defined pooling operations looks as follows to arrive at the word from the individual subwords:

$$
\mathbf{w}_{c}=f\left(\mathbf{w}_{c}^{1}, \ldots, \mathbf{w}_{c}^{k}\right) ; f \in\{\min , \max , \text { mean, last }\}
$$

where we have subwords $w^{1},  \ldots, w^{k}$ such that $\operatorname{cat}\left(w^{1}, \ldots, w^{k}\right)=w$

Why would any of these pooling operations result in a meaninigful source-word? 
This is just squishing tokens together! \\

-> This is a major limitation for which we may need to use ELMo
-> However this may be needed for "unseen concepts" (which are unseen words...)
-> Perhaps check what fasttext does...?


(2) Uses \textit{context combination} to map from different contexts $c_1, \ldots, c_n$ to a single static embedding $w$ that is agnostic of context.

Proposed are two ways to represent context.

\textbf{Decontextualization} For a single word-context, we siimply feed-in the word by itself to the model.

\textbf{Aggregated} combine $w$ in multiple contexts.
n sentences are sampled from the dictionary $\mathcal{D}$.
From the multiple sampled words, we then apply pooling to arrive at a single representation that aggregates the different tokens into one.

$$
\mathbf{w}=g\left(\mathbf{w}_{c_{1}}, \dots, \mathbf{w}_{c_{n}}\right) ; g \in\{\min , \max , \text { mean }\}
$$


This post extracts (token?) word-embeddings: 
(https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b)

This seems to be a way to extract embeddings for tokens from BERT
(https://github.com/imgarylai/bert-embedding)

(->How can we create a (parametric) probability density from a point-cloud distribution?)

perhaps not necessarily interpretable in standard euclidean space
(https://www.kdnuggets.com/2019/02/bert-features-interbertible.html)
original (https://medium.com/thelocalminima/are-bert-features-interbertible-250a91eb9dc)

Perhaps we can mask all but the target token to arrive at one vector per token (and then combine them somehow...).
But how do they extract the singular word-embeddings...?

(-> you could be like "acquiring bedeutung" is a big problem in many tasks. especially useful when we try to map one concept to another. we look at the NLP task for concreteness)

Generally, really good critique on this paper:

(https://openreview.net/forum?id=SJg3T2EFvr)

usually, we have sentence-embeddings, and do not look at word-embeddings.

(-> we don't want to add more and more context. we want a model which contains the polysemy of different contexts, which could allow for probability maximization..., otherwise we have to look at bigger and bigger documents to build more accurate language models, which becomes infeasible at some point. (although this would be the way humans work, because they live in context as well)

This blog aims to generate word-embeddings (and sentence-embeddings) from the BERT model.
(https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)

create word-vectors by taking out what BERT predicts at the nth token.
create word-vectors by concatenating or summing multiple layer's outputs.

the cosine similarity between these vectors seem pretty well-done!


(-> Does it make sense to use BERT and then on calculate word-embeddings through an extended fully-connected model)

-> ELMo may provide a better tokenizer, maybe better ot use this? What about GPT? ELMo uses moses tokenizer which seems word-level enough

-> How to solve this tokenization problem....

-> Can also analyze only words that exist.


\section{Resources and Datasets}

\subsubsection{WordNet}

The online lexical database WordNet was original introduced in \cite{miller90}.
WordNet is a semantic reference system similar to a thesaurus whose design is inspired by current psycholinguistic theories of human lexical memory.
English nouns, verbs and adjectives are organized into synonym sets, each representing one underlying lexical concept.
Different relations link the synonym sets.
WordNet 1 contains of 95,600 different word forms (51,500 simple words and 44,100 collocations), and include a total of 70,100 word meanings, or sets of synonyms.
Compared to a standard dictionary, WordNet divides the lexicon into five categories, namely nouns, verbs, adjectives, adverbs and function words. 
The authors argue that the rest of language is probably stored separately as part of the syntactic component of language.

The most ambitious feature of WordNet, however, is to attempt to organize lexical information in terms of word meanings.

The problem with alphabetical thesaurus is redundant entries. If Word $w_a$ and word $w_b$ are synonyms, the pair should be entered twice.
The problem with a topical thesaurus is that two look-ups are required, first on an alphabetical list and again in the thesaurus proper.

\paragraph{Lexical matrix:} 
Word are often referred to both the utterance and to its associated concept.
As such, \cite{miller90} specify the difference between the two concepts as "word form", which refers to the physical utterance or inscription, and "word meaning", which refers to the lexicalized concept that a form can be used to express.

The following table captures the concept of a lexical matrix, which encodes word-forms (columns), word-meanings (rows), and the existence of the expression of the word-meanings through the corresponding word-form (cell).
If multiple entries exist for a single column, then the single word-form encode mulitple meanings, implying that the word-form is polysemous (it encodes multiple word-forms).
If multiple entries exist for a single row, the two words express the same underlying concept, and thus the two words-forms are synonoms.

%TODO Throughouly read miller90

\begin{figure}[H]
\begin{center}
\begin{tabular}{ | c | c  c  c  c  c | } 
 \hline
 Word        &  &  & Word & Forms &  \\ 
 Meanings & $F_1$ & $F_2$ & $F_3$ & \ldots & $F_n$ \\ 
 \hline
 $M_1$     & $E_{1,1}$ & $E_{1,2}$ &  &  &  \\ 
 $M_2$     &  & $E_{2,2}$ &  &  &  \\ 
 $M_3$     &  &  & $E_{3, 3}$ &  &  \\ 
 $\vdots$ &  &  &  & $\ddots$ &  \\ 
 $M_m$    &  &  &  &  & $E_{m, n}$ \\ 
 \hline
\end{tabular}
\end{center}
\caption{Figure taken from \cite{miller90}. Word-forms $F_1$, and $F_2$ are synonyms of each other, as they share one word meaning $M_1$. Word-form $F_2$, as it entails more than one meaning, namely $M_1$ and $M_2$.}
\end{figure}

WordNet also includes synonym sets referred to as \textit{synsets}, which are a collection of word-forms that together determine a single meaning.
Thus, a meaning is represented as a synset.
WordNet is organized by semantic relations.
Thus, WordNet includes a set of semantic relations.
WordNet defines synonyms as a set of words, where one word is interchangable for another.
This implies that in terms of substituatbiliy, WordNet clusters words into nouns, verbs, adjectives and adverbs, as the part of speech must be valid at the position of the sentence.

Although the authors mention that synonyms should be best thought of a continuum along which similarity of meaning can be graded, the authors decide to determine similarity in terms of a binary event, something which is either present, or not present.

This is mainly handcrafted by linguistics over the last 2 decades.
Meanings have different levels of detail.
The two tables depicted below show examples of how WordNet 3.1 introduces different semantic classes for the words \Verb#bank# and \Verb#was#

\begin{figure}[H]
\begin{center}
\begin{tabular}{ | c | p{11cm} | } 
 \hline
 Part of Speech & Definition \\  
 \hline
noun     & (sloping land (especially the slope beside a body of water)) "they pulled the canoe up on the bank"; "he sat on the bank of the river and watched the currents"\\ 
noun     & depository financial institution, bank, banking concern, banking company (a financial institution that accepts deposits and channels the money into lending activities) "he cashed a check at the bank"; "that bank holds the mortgage on my home" \\ 
 noun     & (an arrangement of similar objects in a row or in tiers) "he operated a bank of switches"  \\ 
verb & (tip laterally) "the pilot had to bank the aircraft" \\ 
verb    & (do business with a bank or keep an account at a bank) "Where do you bank in this town?"  \\ 
 \hline
\end{tabular}
\end{center}
\caption{Example output for WordNet 3.1 noun propositions for the word "bank". In total, 18 different concepts are recorded.}
\end{figure}

\begin{figure}[H]
\begin{center}
\begin{tabular}{ | c | p{11cm} | } 
 \hline
 Part of Speech & Definition \\ 
 \hline
noun     & Washington, Evergreen State, WA, Wash. (a state in northwestern United States on the Pacific) \\ 
verb     & (have the quality of being; (copula, used with an adjective or a predicate noun)) "John is rich"; "This is not a good answer"
 bank"; "that bank holds the mortgage on my home" \\ 
 verb     & (an arrangement of similar objects in a row or in tiers) "he operated a bank of switches"  \\ 
verb & (form or compose) "This money is my only income"; "The stone wall was the backdrop for the performance"; "These constitute my entire belonging"; "The children made up the chorus"; "This sum represents my entire income for a year"; "These few men comprise his entire army" \\ 
verb    & (work in a specific place, with a specific subject, or in a specific function) "He is a herpetologist"; "She is our resident philosopher"  \\ 
 \hline
\end{tabular}
\end{center}
\caption{Example output for WordNet 3.1 noun propositions for the word "was". In total, 18 different concepts are recorded.}
\end{figure}

In this work, we will often use WordNet as



\subsubsection{SemCor dataset}

The SemCor 3.0 corpus is a collection of the Brown corpora \cite{francis64}, where each noun, adjective and verb is tagged with the WordNet 3.0 senses.
It was part of the early WordNet project, initially introduced in \cite{miller94}.

Some example snippets look as follows, where SemCor includes sentences, and each sentence is annotated by WordNet senses.

\begin{verbatim}
A Texas halfback who does n't even know the team 's plays,
 Eldon_Moritz, ranks fourth in Soutwest_conference scoring after three games.
\end{verbatim}

The corresponding part-of-speech labels and wordnet sense ids 
are
\begin{verbatim}
DT, NN, NN, WP, VBZ, RB, RB, VB, DT, NN, 
POS, NN, NNP, VB, JJ, IN, NNP, VP, IN, JJ, NN
\end{verbatim}

\begin{verbatim}
None, 1, 1, None, 0, 1, 7, None, 1, 
None, 3, 1, 1, 1, None, 1, 1, 1, 1
\end{verbatim}

%TODO Insert distribution of number of occurences of wordnet ids.

\begin{figure}[h]
	\center
  \includegraphics[width=0.6\linewidth]{./assets/background/semcor_skew.png}
  \caption{Shows that the SemCor data is biased. Words with a low WordNet sense index (i.e. close to 0) occur much more often than words that have a high WordNet sense index (i.e. above 5).
  The x-axis shows the WordNet sense index for a chosen word, while the y-axis shows the log-frequency within SemCor. 
  This is a cumulative plot over all words with WordNet senses within SemCor 3.0.
  The skew could be a natural effect of how word lower WordNet indecies are assigned to more commonly used words.
  }
  \label{fig:embeddings_by_language}
\end{figure}
%TODO Should make this graph again, a bit better titles..

\subsubsection{GLUE benchmark dataset}

The GLUE benchmark dataset was first introduced by \cite{wang19}.
Although additional work has been done through the introduction of SuperGLUE \cite{wang19b} to train for an even bigger variety of language tasks, after most NLU models started surpassing non-expert human performance on the initially-proposed GLUE-tasks, we will only use at the standard GLUE benchmarking dataset, as our base model will be BERT, which does not perform on superhuman leves with the GLUE benchmark yet.

\begin{figure}[H]
\center
%TODO Write section for GLUE tasks
\begin{tabular}{
 l % left aligned column
 l % left aligned column
 l
 l
 l
 l
 % *{2}{S[table-format=4.0]} % three columns with numeric data       
}
\toprule
%TODO MRPC has one aligned, and one unaligned test
\textbf{Corpus} & \textbf{ $|$Train$|$ } &\textbf{$|$Test$|$} & \textbf{Task} & \textbf{Metrics} & \textbf{Domain}  \\
\midrule
%\multicolumn{6}{ c }{Helllo}
CoLA  & 8.5k  &  1k   & acceptability & Matthews corr. &  misc \\

SST-2 & 67k &  1.8k  & sentiment &  accuracy &  movie reviews \\

MRPC & 3.7k &  1.7k & paraphrase & accuracy / F1 & news \\
          
STS-B & 7k &  1.4k    & sentence similarity & Pearson/Spearman corr. &  misc. \\
      
QQP & 364k &  391k & paraphrase  & accuracy / F1  &  social QA questions\\	

MNLI & 393k &  20k  & NLI   & mis-/matched accuracy &  misc \\

QNLI & 105k &  5.4k & QA/NLI   & accuracy &  Wikipedia \\

RTE & 2.5k &  3k      & NLI   &  accuracy &  news, Wikipedia \\

WNLI & 634 &  146   & coreference/NLI & accuracy  & fiction books \\

% SNLI & Accuracy &  0.8461 &              &  0.8367 \\
      
\bottomrule
\end{tabular}
\caption{Hello}
\end{figure}

We use an accuracy score for all benchmarking datasets. Because MRPC and QQP have a dominant class, the F1-score is used here. Finally, STS-B is judged using a pearson and spearman correlation due to a ranking being produced, and CoLa is assessed using the Matthews correlation.

\subsubsection{Single Sentence Tasks}

The Corpus of Linguistic Acceptability \textbf{CoLA} consists of englihs sentences, where the task of the model is to predict whether or not the sentence is a valid english sentence.
The Matthews correlation evaluates the model from a score of -1 to 1, where 0 corresponds to uniformly random guessing.
The test set includes out of domain sentences.

\begin{verbatim}
label:1
The professor talked us into a stupor.
\end{verbatim}

\begin{verbatim}
label:0	
The professor talked us.
\end{verbatim}

%TODO Perhaps show examples?
%TODO Cite the underlying paper?

The Stanford Sentiment Treebank \textbf{SST-2} consists of sentences from moview reviews and human annotations of their sentiment.
This is a binary classificaiton (positive / negative) task.


\begin{verbatim}
label:1
comes from the brave , uninhibited performances
\end{verbatim}

\begin{verbatim}
label:0
excruciatingly unfunny and pitifully unromantic
\end{verbatim}

\subsubsection{Similarity and paraphrase tasks}

The Microsoft Research Paraphrase Corpus \textbf{MRPC} is a corpus of sentence pairs automatically extracted from online news sources, with human annotations whether the sentences in the pair are semantically equivalent.
Here, an F1-score is taken, because the class-sets are imbalanced.


\begin{verbatim}
quality:1	
Prosecutors filed a motion informing Lee they intend to seek the death penalty.	
He added that prosecutors will seek the death penalty.
\end{verbatim}

\begin{verbatim}
quality:0	
A former teammate , Carlton Dotson , has been charged with the murder.	
His body was found July 25 , and former teammate Carlton Dotson has been charged in his shooting death .
\end{verbatim}

The Quora Question Pairs \textbf{QQP} dataset is a collection of question and answer pairs from the website Quora. 
The task it to check whether a pair of questions are semantically equivalent.
Again, the F1-score is taken as a measure because the class-sets are unbalanced.


\begin{verbatim}
My Galaxy ace is hang?	
Why are the people on Staten Island are racist?	
0
\end{verbatim}

\begin{verbatim}
Where can I learn to invest in stocks?	
How can I learn more about stocks?	
1
\end{verbatim}

The Semantic Textual Similarity Benchmark \textbf{STS-B} is a corpus of pairs of news headlines, video and image captions.
Each pair is annotated with a similarity score from 1 to 5, and the task is to predict these scores.
The evaluation is done using Pearson and Spearman correlation, as the determining factor is the relative ranking amongst scores.


\begin{verbatim}
The man is riding a horse.	
A man is riding on a horse.	
5.000
\end{verbatim}

\begin{verbatim}
A man is playing a guitar.
A girl is playing a guitar.	
2.800
\end{verbatim}

\subsubsection{Inference Tasks}

The Multi-Genre Natural Language Inference Corpus \textbf{MNLI} is a crowd-sourced collection of sentence pairs with extual entailment annotations.
For eah premise and hypothesis sentence, the task is to predict whether the premise entails the hypothesis, contradit the hypothesis, or neither.
Thus, this is a 3-class classification problem.
The matched MNLI version tests on data which is in the same domain as the training set, while the mis-matched MLNI tests on a training set which is cross-domain.
The MNLI contains parse-trees, which is the reason we do not display these here.

The Stanford Question Answering Dataset is a question-answering dataset. 
The authors of GLUE use this dataset, to create the Question answering NLI \textbf{QNLI}, by removing the requirements that the model selects the exact answer.
On top of that, they also remove the simplifying assumption that the answer is always present in the input and that lexical overlap is a reliable cue.
The task is to determine whether the context sentence contains the answer to the question. 

\begin{verbatim}
Who did the children work beside?	
In many cases, men worked from home.
not_entailment
\end{verbatim}

\begin{verbatim}
How many alumni does Olin Business School have worldwide?
Olin has a network of more than 16,000 alumni worldwide.
entailment
\end{verbatim}


The Recognizing Textual Entailment \textbf{RTE} dataset comes from a series of annual textual entailment challenges. 
Data is combined from RTE1, RTE2, RTE3 and RTE5. 
The classes to be predicted by the model include \textit{neutral}, \textit{contradiction} and \textit{no entailment}.


%TODO do code styling
\begin{verbatim}
Oil prices fall back as Yukos oil threat lifted	
Oil prices rise.
not_entailment
\end{verbatim}

\begin{verbatim}
Money raised from the sale will go into a trust for Hepburn's family.
Proceeds go to Hepburn's family.	
entailment
\end{verbatim}


The Winograd NLI \textbf{WNLI} dataset uses the original Winograd Schema Challenge dataset \cite{levesque12}, which is a reading comprehension task where the reader must read a sentence with a pronoun and select the referent of that pronoun from a list of choices. 
Sentence pairs are constructed by replacing the ambiguous pronoun with each possible referent.
The task is to predict if the sentence with the pronoun substituted is entailed by the original sentence.

Some example sentences include 

%TODO do code styling
\begin{verbatim}
Bob was playing cards with Adam and was way ahead.
If Adam hadn't had a sudden run of good luck, he would have won.	Adam would have won.
label:0
\end{verbatim}

\begin{verbatim}
Mark told Pete many lies about himself, which Pete included in his book. 
He should have been more truthful.	
Mark should have been more truthful.
label:1
\end{verbatim}




\chapter{Related Work}

\section{Structure inside BERT}

(How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings)

going down the drain of "geometry" of BERT and ELMo.

could also go down the drain of bias (we would prefer to have uniform space over gender etc.)

-> does projection into some subspace which has same metric properties perhaps not make it asitropic?

pretty ok summary of what kind of properties we want from word-embeddings... (https://devopedia.org/word-embedding)


Especially in Named Entity Recognition (NER), there is a lot of use for static word-embeddings.
I guess this is because we need static embeddings which represent the individual clusters?

-> Using pooling for some 

-> Character level operation

-> Perhaps make good sense to work towards a word-embeddings where different vectors are close to each other?

-> perhaps find a metric space warping the vectors, s.t. an isotropic representation is achieved?

-> Perhaps tokenization is a big problem, but perhaps other architecture..? but retraining is too difficult.. probably best to just stick to BERT? one way or the other, we need good word-embeddings derived from good language models to form a probabilistic prediction of the concept


-> Could perhaps also try to make an adversarial autoencoder after the BERT layer (or continue training, s.t. a second loss is mimized as a downstream task?)

-> Perhaps distilling with "correct" tokens? i.e. another network which copies BERT, but instead of outputting \#\#end, it outputs one of most frequent 20k words

-> thesaurus using a (set of) words. a little like sentence-generation, but generating most-probable examples

-> Everyone just averages token-embeddings..

-> perhaps fitting a GMM to the contextualized representations of BERT may give a good probability space..?

-> Perhaps make sense to apply MUSE to this?

-> Artetxe 2019 uses language models to generate embeddings. we also do this, but do it using 1) better language models, and 2) better 

-> QUESTION: Which factors (mapping algo, embedding) is delimiting in automated embedding matching

-> perhaps create a GMM for each concept, based on how many modals we identify? how to estimate the number of clusters? by graph-clustering perhaps! (this could be very consuming)

-> Adversarial autoencoder on BERTs last models to enforce it to some better distribution


\subsection{Attention mechanism}

Some analysis has been done with looking at the similarity between the produced context-vectors for biLM \cite{peters18}.

\subsection{From token-vectors to word-vectors}

\cite{may19} analyse the social biases that are part of the context embeddings and show that depending on the language model (ELMo, BERT), the amount of social bias deviates. 
Because context embeddings capture more than just semantics, bias is a natural implication of context vectors.
A by-product of their research includes aggregating token-embeddings (i.e. aggregating the tokens $hav$ and $\#\#\#ing$ to result at the word $having$).
Although not very extensive on the topic of aggregation, the authors  use techniques of mean-pooling, max-pooling, and last-pooling to determine arrive at a single context-vector, if the given token is intrinsically split-up by the language-model tokenizer (incl. BERT, ELMo, GPT).

Finally, evaluation on the corpora also yield inclusion of human-like biases \cite{jentzsch19} by names, races, male-vs-female.

\subsection{Bias in BERT vectors}


\subsection{Change of meanings over time}

\cite{hu19} analyse how meanings quantitatvely using corpora from different historical epochs, including the 1890s, 1940s, 1960s, 1970s, 1990s and 2000s. 
Specifically, they measure the similarity scores for given context-vectors based on differently trained corpora.
They demonstrate through examples of the word \textit{gay} and \textit{alien} that the similarity between context-vectors change.

\subsection{Clinical concept extration}

The task of concept-extration is heavily applied in the field in the clinical domain.

\cite{zhu18} trains a LSTM-Conditional-Random-Field to extract concepts.
They used annotated corpora from doctors notes to train the model and model this as a named-entity-recognition task.

%TODO \cite{tenney19} Look more closely at this paper

\subsection{Discovering for semantics}


\cite{chen19} apply a co-clustering framework that discovery multiple semantic and visual senses of a given noun phrase.
They use an structure-EM-like algorithm to achive this.

\subsection{Embeddings for translation}

A lot of work is done in the field of analysing embeddings for translation tasks, to further mitigate the black-box behvaior of neural network.

This includes \cite{kudugunta18} who use singular value canonical correlation analysis to compare hidden representations of language models between different languages.
To arrive at a sentence-representation, they do mean-pooling over the outputted embedding-vectors of a sentence. 
They find interesting behavior in the arrangement of context-embedding in the Transfoerm-Big architecture proposed in the \cite{vaswani17} paper, which is also inherently used in BERT and GPT.
Interestingly, they show that different languages do not project into similar spaces, thus making zero-shot learning tasks between languages difficult.


\begin{figure}[h]
	\center
  \includegraphics[width=0.6\linewidth]{./assets/relatedwork/embeddings_by_language.png}
  \caption{From \cite{kudugunta18}, visualizing clustering of the encoder representations of all languages, based on ther SVCCA similarity.}
  \label{fig:embeddings_by_language}
\end{figure}

%TODO What language model do they use? I dont see anything about BERT


\cite{lample18} uses a cyclic loss to apply unsupervised machine translation suign monolingual corpora only. 
Here, sentence-embeddings are learned over time, which follow the cyclic loss constraint, and minimize the sunsupervised translation loss.

Other work tries to disentangle the different linguisitc, visual and audio-based features by supplying multi-modal input at once \cite{ma19}.
A translation model is learned which can not only translate between different domains, but also between different modalities (i.e. from image to audio, audio to text, and any such combination).



\section{Metric Learning and Disentanglement}

Althogh difference metric spaces can be considered, we will be refering to Euclidean spaces for simplicity and unless otherwise stated.

%TODO cite the proper definition of Metric learning
Metric learning is the concept of learning a new space in which distances between data samples 

%TODO mention the survey paper
Distance metric learning approaches the problem of learning a similarity metric.

We introduce two sets of data-sample pairs, $S$ and $D$:

\begin{align}
S &= \{ (x_i, x_j) : x_i and x_j should be similar\} \\
D &= \{ (x_i, x_j) : x_i and x_j should be dissimilar\}
\end{align}

As is also mentioned in \cite{moutafis17}, the proximity relation can also be captured through \textit{relation triplets}

\begin{align}
R &= \{ (x_i, x_j, j_l) : x_i is more similar to x_j than x_l \} \\
\end{align}

The general notion of a distance in the euclidean space can be defined as 
\begin{equation}
d_{E}\left(\boldsymbol{x}_{\boldsymbol{i}}, \boldsymbol{x}_{\boldsymbol{j}}\right)=\sqrt{\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right)^{\top}\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right)}
\end{equation}
or equivalents as the $l_2$-norm

\begin{equation}
d_{E}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{2}
\end{equation}

Most distance learning techniques propose different ways of learning a Mahalanobis distance \cite{mahalanobis36}

\begin{equation}
d_{M}\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right)=\sqrt{\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right)^{\top} \boldsymbol{A}^{-1}\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right)}
\end{equation}

Because calculating $d^2$ is computationally simpler, and $d^2$ maintains most mathematical properties as calculating $d$, often $d^2$ is used for downstream calculations (i.e. optimization, distance measure), rather than $d$.

In the case of the Mahalanobis distance, $A$ is a linear operator such as a matrix.
However, $A$ can also be generalized to a nonlinear operator by using the following formulation of the distance metric

\begin{align}
d_{M}^{2}\left(x_{i}-x_{j}\right) &=\left(x_{i}-x_{j}\right)^{\top} A^{-1}\left(x_{i}-x_{j}\right) \\
d_{M}^{2}\left(x_{i}-x_{j}\right) &= (L\left(x_{i}-x_{j}\right))^{\top} (L\left(x_{i}-x_{j}\right)) \\
d_{M}\left(x_{i}-x_{j}\right) &=\left\|L\left(x_{i}-x_{j}\right)\right\|_{2} \\
d_{M}\left(x_{i}-x_{j}\right) &=\left\|Lx_{i}-Lx_{j}\right\|_{2}
\end{align}

where $A = L^T L$ and $L$ can now be any function $L \in \mathbf{R}^n -> \mathbf{R}^d$. 
Whenever, $d < n$, the data is projected to a lower dimensional space $d$, which results in dimensionality reduction, which however breaks the convexity property if $L$ is not invertible.
In comparison to a dimensionality reduction such as PCA, it has been obsereved that learning a similarity measure can sometimes be more effective \cite{chechik10}.

Specifically, the data is projected into another  \
%TODO specify this further
The authors in \cite{moutafis17} argue that due to this property of measuring similarity between pairs, the metric learning models are able to generalize better across classes.

\subsubsection{Non-Deep Metric Learning}

Although we focus on deep metric learning during our experiments, due to the additional flexibility that deep neural networks provide, a good understanding of the underlying mechanisms is useful.

\cite{suarez19} provides a good introduction into the literature of metric learning algorithms.

We first define by a proper definition of a distance

\newtheorem{mydef}{Definition}
\begin{mydef}
Let $X$ be a non empty set. A \textit{distance} or \textit{metric} over $X$ is a map $d: X \times X -> \mathbb{R}$ that satisfies the following properties:

\begin{enumerate}
\item Coincidence: $d(x, y) = 0 \iff x = y,$ for all $x, y \in X$
\item Symmetry: $d(x, y) = d(y, x)$ for all $x, y \in X$
\item Triangle inequality: $d(x, z) \leq d(x, y) + d(y, z) $ for all $x, y \in X$
\end{enumerate}
The ordered pair $(X, d)$ is called a metric space. 
We will be assuming a euclidean metric space unless otherwise specified.
\end{mydef}

Because the coincidence property is not always important to us, but because word-embeddings are more interested in measuring relative distances, we will also consider mappings konwn as \textit{pseudoinstances} which relax the coincidence property to merely fulfill $d(x, x) = 0$.
In the $d$ dimensional euclidean space, the set of \textit{Mahalanobis distances}, which is parametrized by positive semidefinite matrices are useful.

Using above similarity, dissimilarity and relation triplets, an optimal distance $d$ from a set of distances $\mathcal{D}$ can be found

\begin{equation}
\min_{d \in \mathcal{D}} l (d, S, D, R)
\end{equation}

where $l$ is one of the loss metrics shown below.



\begin{figure}[h]
	\center
  \includegraphics[width=\linewidth]{./assets/relatedwork/metric_learning_example.png}
  \caption{Taken from \cite{suarez19}. The individual tiles show the kNN prediction regions by color for every point in the image. Using the unmodified euclidean distancee, this would result in classification regions on the left. The reader can see, learning an appropriate distance, the classification is much more effective (middle). Finally, the dimensionality is also reducable with this dimension while still matching the classification accuracy (right).}
  \label{fig:BERT_vanilla_pipeline}
\end{figure}

Although we will not go into too much detail, we will outline some algorithms that \cite{suarez19} mentions in their work.

We will first start with dimensionality reduction techniques, then move to nearest neighbors classifiers, follow with nearest centroids classifiers and finally go over some information theory based algorithms.
Notice that for almost each one of these methods, kernelized versions are available.



%TODO Insert some texts from table I

\paragraph{Dimensionality Reduction} techniques include principal component analysis \textit{PCA}, LDA ANMM



\subsubsection{Deep Metric Learning}

Besides the non-deep proposed in the work above models , in\cite{kaya19} also introduces the deep generalization to these frameworks.

\begin{figure}[H]
	\center
  \includegraphics[width=\linewidth]{./assets/relatedwork/metric_learning.png}
  \caption{Taken from \cite{kaya19}. An illustration of deep metric learning. The space is transformed in such a way, that similar object are closer to each other, and dissimilar objects are moved away from each other.}
  \label{fig:muse_translation}
\end{figure}

The most prominent model is the siamese network \cite{bromley94}, \cite{chopra05}, \cite{hadsell06}, where the network consists of two identical sub-networks joined at their outputs. 
We provide an example the architecture looks that looks as follows:

\begin{verbatim}
class Siamese:

    def __init__(input_dim, latent_dim):
        self.fully_connected = nn.Linear(input_dim, latent_dim)
        
    def forward(input1, input2):
        latent_1 = self.fully_connected(input1)
        latent_2 = self.fully_connected(input2)
        
        distance = torch.sqrt(
            torch.sum((latent_1 - latent_2) * (latent_1 - latent_2))
        )
        
        return distance, latent_1, latent_2
\end{verbatim}

Two samples are ingested by the neural network simultaneously.
Both samples are passed through identical weights.
Training occurs by minimizing the contrastive loss.

\begin{equation}
L_{\text {Contrastive }}=(Y) \frac{1}{2}\left(D_{W}\right)^{2}+(1-Y) \frac{1}{2}\left\{\max \left(0, m-D_{W}\right)\right\}^{2}
\end{equation}

where we follow the convention that $Y=1$ whenever the given sample-pairs are of the same class, and $Y=0$ whenever the given sample-pairs are from different classes, and $D_W$ is the output of the above neural network.
Training can be highly unstable, and as such, the learning rate must be properly set, batches must have a balanced amount of both similar, and dissimilar pairs. 
Finally, training can fail if a single batch includes multiple domains of data. \\

For the triplet networks \cite{hoffer14} which makes use of relation triplets, including an input $X$, a point similar to the input $X^p$ and a point dissimilar to the input $X^n$, the following loss can be used for minimization.

\begin{equation}
L_{\text {Triplet}}=\max \left(0,\left\|G_{W}(X)-G_{W}\left(X^{p}\right)\right\|_{2}-\left\|G_{W}(X)-G_{W}\left(X^{n}\right)\right\|_{2}+\alpha\right)
\end{equation} 

where $\alpha$ forms a distance margin, similar to the length of the support vector in SVMs.

As the authors suggest, using a triplet logic results in more stable training and bigger margins between similarity classes.
Analogous logic goes for quadrupel loss etc., but it is unsure to what extend these extensions increase performance.
This general idea can also be extended to angular distance (using cosine distances), and non-euclidean spaces, however, we will not go into further detail on this as this is not the focus of this work.

Although we will not go into further detail, other loss functions include histogram loss \cite{ustinova16}, the structured loss \cite{song16} n-pair loss \cite{sohn16}, magnet loss \cite{rippel16}, angular loss \cite{wang17}, quadruple loss \cite{ni17}, clustering loss \cite{song17}, hierarchical triplet loss \cite{ge18} and the multi-similarity loss \cite{wang19c}.

%TODO do back-projection to higher dimenisonal space.
%TODO check performance on some bias prediction task (to see if sentiment is truly gone ..)

\section{Clustering Algorithms}

We will later on use different clustering algorithms to evaluate how well semantics is interpretable inside BERT vectors.
We will give a short introduction on the following clustering algorithms.

\begin{itemize}
\item Affinity Propagation
\item Chinese Whispers
\item DBScan
\item HDBScan
\item MeanShift
\item Optics
\end{itemize}

%TODO Go a bit into detail with chinese whispers, and the paper where they form EGO networks to find synsets (and thus, meanings...)

\section{Applications of word vector}

%TODO Talk about MUSE paper

Although word-vectors can be used for a variety of tasks, we will focus on some varieties of how these vectors are used for machine translation (of human languages) tasks.

Amongst the most prominent method is the MUSE model introduced by \cite{conneau17}.
Here, a mapping $W$ is found using either an unsupervised methodology by minimizing the batch-size cosine-distance between sampled word-vectors between two languages $A$ and $B$, or a supervised methodology is devised using the procrustes algorithm.

\begin{figure}[h]
	\center
  \includegraphics[width=\linewidth]{./assets/relatedwork/muse.png}
  \caption{Taken from \cite{conneau17}. Each }
  \label{fig:muse_translation}
\end{figure}

Specifically, the following loss-function optimizes over the space of possible mapping matrices $W^*$:

\begin{equation}
W^{\star}=\underset{W \in O_{d}(\mathbb{R})}{\operatorname{argmin}}\|W X-Y\|_{\mathrm{F}}=U V^{T}, \text { with } U \Sigma V^{T}=\operatorname{SVD}\left(Y X^{T}\right)
\end{equation}

Unsupervised training is conducted using a discriminator and a mapping function, where the discriminator's task is to identify the origin-distribution, and the mapping function is supposed to fool the discriminator, resulting in a mapping function which maps all word-emebddings into a common global embedding space.

A generalization of these word vectors to point-vector probability-distributions are captured by \cite{alvarez18}.
Here, the point-vectors are interpreted as point-delta-distributions in the $d+1$ dimensional space.
Common optimal transport methods are coupled with the Gromov-Wasserstein loss to find an orthogonal mapping which maps from vector-space $X$ to vector space $Y$, and thus from the word-token of one language to another.
One way to achieve this is to use the supervised loss-metric and solve the procrustes problem, finding correspondences between the columns of X and columns of Y through:

\begin{equation}
\min _{\mathbf{P} \in O(n)}\|\mathbf{X}-\mathbf{P} \mathbf{Y}\|_{F}^{2}
\end{equation}

with $O(n)=\left\{\mathbf{P} \in \mathbb{R}^{n \times n} | \mathbf{P}^{\top} \mathbf{P}=\mathbf{I}\right\}$.
The resulting algorithm achieves similar performance to MUSE while requiring only a fraction of the computational cost, being able to get competitive results on CPU.

The unsupervised approach deals with finding a transportation map $T$ that fulfills

\begin{equation}
\inf _{T}\left\{\int_{\mathcal{X}} c(\mathbf{x}, T(\mathbf{x})) d \mu(\mathbf{x}) | T_{\#} \mu=\nu\right\}
\end{equation},

where $\mu$ and $\nu$ are the point-delta distributions describing the word-embeddings in the probability space.
The relaxed Kantorovich's formulation is then finally used to reduce this problem to finding a set of transportation plans in a polytopes.

$$
\Pi(\mathbf{p}, \mathbf{q})=\left\{\Gamma \in \mathbb{R}_{+}^{n \times m} | \Gamma \mathbb{1}_{n}=\mathbf{p}, \Gamma^{\top} \mathbf{1}_{n}=\mathbf{q}\right\}
$$

Finally, the cost function

$$
\min _{\Gamma \in \Pi(\mathbf{p}, \mathbf{q})}\langle\Gamma, \mathbf{C}\rangle
$$
is minimized to find an optimal transportation plan.


Another extension is the work by  


\newpage
\subsubsection{Word2Vec}

BERT conditions on the rest of the input sentence.

BERT uses words, subwords and individual characters (in total 30'000) that are then used as tokens.

Idea is to do the following:
Concepts (and thus words), are represented across multiple contexts.
We can create probabilistic word-embeddings by sampling from a corpus the context of a specific word.
From multiple samples of the context-embedding-vector, we take the mean and stddev, or create a GMM if there are multimodal logic (we can check this multimodality by runniing some sort of rejection sampling algorithm).
Then we have a probability density function (one for each language), and can map one into another.

Perhaps we could split up too high-variance embeddings to multimodal embeddings, depending on their use-cases.

This allows for interpretability in polysemy sentences.

Not using more complex flows implies that the flow itself is not the bottleneck (they probably tried out other flows as well).

Are the individual word-embeddings going to be one big blob with high variance, or is it going to be a multi-modal distribution...?

Another task we may look at is, from a given word-embedding, sample it's context. 
Not entirely sure how to do this with BERT and co.

At what point do we overfit, and at what point do we generalize?


\textbf{Artetxe bilingual token matching through unsupervised machine translation}

- Input is cross-lignual word-embeddings
- Build an unsupervised phrase-based statistical machine translation system
- Derive phrase-based embeddings from input-word-embeddings by taking top 400'000 bigrams and 400'000 trigrams -> take arithmetic mean of word-embedding
- score top 100 close phrases using softmax cosine similarity
- generation of synthetic parallel corpus using this approach
- Then use FastAlign to use parallel corpus to align words
- 

\chapter{Analysing the current state of the art}

Upadhyay et al. argue that the choice of data is more important than the actual algorithm.

Definitely also look into this, [Analyzing the Limitations of Cross-lingual Word Embedding Mappings] seems to be an analysis of the difficulties etc. 

\section{On the Linear Separability of meaning within sampled BERT vectors}

\subsection{Motivation}

To see if there is any structure within BERT vectors w.r.t. the different meaning of one word, we ask ourselves whether or not different part of the meaning are at different locations of the embedding space produced by BERT.

\subsection{Experiment setup}

Let us regard a word $w$ in sentence $s$ is indexed by $i$.
When passed through the BERT model, the word $w$ produces an embedding $x_w$.
When we sample BERT embeddings for a single word $w$ for $n=500$ sentences. 

Let us restrict the choice of words $w$ on polysemous and ambigious words which carry multiple meanings. 
Specifically, $w$ is chosen in such a way that each meaning has more than 30 samples in each class when chosen from within the SemCor dataset. 

%TODO beautify this table

\begin{center}
% Words used to test BERT-sampled vectors for linear separability of lexical features
\begin{tabular}{ | c | c | c | }
\hline
\multicolumn{3}{ | c | }{Words used to test BERT-sampled vectors for linear separability of lexical features} \\
\hline
 was & is & be \\ 
 \hline
 are & more & one \\  
 \hline
 first & only & time \\
 \hline
\end{tabular}
\end{center}

When we sample 
Specifically, the experiment setup looks as follows.

\begin{algorithm}[H]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\Input{A target word $w_{\text{target}}$; The latent dimensionality $k$for PCA;}
\KwResult{Accuracy of a logistic regression classifier}
 $\mathbf{D}, \mathbf{y} \leftarrow $  sample up to 500 sentences (as much as available) from the SemCor corpus which include the word $w_{\text{target}}$, along with the corresponding WordNet meaning-id\;

$ \mathbf{X} \leftarrow BERT( \mathbf{D} )$ i.e. pass each sentence through BERT and retrieve the resulting word embedding $x_w$ as defined in the above section\;
 
$ \mathbf{X}, \mathbf{y} \leftarrow oversample( \mathbf{X}, \mathbf{y} )$ such that we don't have dominating classes\;
 
$ \mathbf{X_\text{train}}, \mathbf{X_\text{test}}, \mathbf{y_\text{train}}, \mathbf{y_\text{test}} \leftarrow trainTestSplit( \mathbf{X}, \mathbf{y}, testProportion=0.4 )$ \;

$ \mathbf{X_\text{train}} \leftarrow StandardScaler( \mathbf{X_\text{train}})$ such that all the data is normalized\;

$ \mathbf{X_\text{train}} \leftarrow PCA( \mathbf{X_\text{train}}, k )$ such that all the data is projected to a lower latent dimensionality $k$\;

$ model \leftarrow LogisticRegression( \mathbf{X_\text{train}}, \mathbf{y_\text{train}} )$ \;
    
$ \mathbf{\hat{y}_\text{test}} \leftarrow model.predict(\mathbf{X_\text{test}})$ \;

$ accuracy, confusionMatrix \leftarrow loss(\mathbf{y_\text{test}}, \mathbf{\hat{y}_\text{test}}) $ \;
    
return $ accuracy, confusionMatrix $\;
    
 \caption{Checks sampled BERT vectors for linear interpretability by meaning}
\end{algorithm}

We use a binary 
%TODO what is the formal name of the loss function that we are using..?

First of all, we sample $n=500$ sentences from the news corpus for a target word $\hat{w}$, which has index $i$ in sentence $s$.

Each sentence, we pass through the 

The BERT model takes a sequence of size (up to) 512 elements as input.

We sample $n=500$ vectors from BERT.
This is the vector at the output embedding layer of BERT, which has the same location as the input to the BERT model.

%TODO insert graphic of which embedding is used..

We apply t-fold cross validation, and measure the mean accuracy as well as the standard deviation of the accuracy. 
We also note down the variance kept after projecting PCA on the lower dimensionality $k$.
This is the sum of the first $k$ largest normalized eigenvalues.


\subsection{Results}

The number of words we can use to realiably estimate a planar separation of semantics within the sampled BERT vectors is small.

We run the above experiment for a set of different $k$ to build up an intuition of how well different dimensionalites still capture the meaning in different locations of the vector space. 

\begin{center}
\captionof{table}{Mean and standard deviation of the accuracy of a linear classifier trained on the 2 most common classes of WordNet meanings for the word \textit{was}.}
\begin{tabular}{SSSSSSSS} \toprule
    {dimensionality} & {variance kept} & {accuracy (mean / $\pm$ stddev)}  \\ \midrule
     10  & 0.27 & 0.82 / $\pm$ 0.03 \\ \midrule
     20  & 0.41 & 0.81 / $\pm$ 0.04  \\ \midrule
     30  & 0.50 & 0.85 / $\pm$ 0.03  \\ \midrule
     50  & 0.63 & 0.92 / $\pm$ 0.03 \\ \midrule
     75  & 0.73 & 0.94 / $\pm$ 0.02 \\ \midrule
    100 & 0.81 & 0.95 / $\pm$ 0.02  \\ \midrule
\end{tabular}
\end{center}

% -> show which two different word meanings are sampled

% Also perhaps show the confusion matrix

\begin{center}
\captionof{table}{Mean and standard deviation of the accuracy of a linear classifier trained on the 2 most common classes of WordNet meanings for the word \textit{is}.}
\begin{tabular}{SSSSSSSS} \toprule
    {dimensionality} & {variance kept} & {accuracy (mean / $\pm$ stddev)}  \\ \midrule
       2  & 0.09 & 0.57 / $\pm$ 0.02 \\ \midrule
     10  & 0.29 & 0.82 / $\pm$ 0.03  \\ \midrule
     20  & 0.42 & 0.82 / $\pm$ 0.04  \\ \midrule
     30  & 0.51 & 0.83 / $\pm$ 0.03  \\ \midrule
     50  & 0.72 & 0.85 / $\pm$ 0.04 \\ \midrule
     75  & 0.78 & 0.84 / $\pm$ 0.04 \\ \midrule
    100 & 0.79 & 0.85 / $\pm$ 0.03  \\ \midrule
\end{tabular}
\end{center}

% -> show which two different word meanings are sampled

\begin{center}
\captionof{table}{Mean and standard deviation of the accuracy of a linear classifier trained on the 2 most common classes of WordNet meanings for the word \textit{one}.}
\begin{tabular}{SSSSSSSS} \toprule
    {dimensionality} & {variance kept} & {accuracy (mean / $\pm$ stddev)}  \\ \midrule
       2  & 0.10 & 0.55 / $\pm$ 0.10 \\ \midrule
       3  & 0.14 & 0.51 / $\pm$ 0.05 \\ \midrule
     10  & 0.34 & 0.59 / $\pm$ 0.08  \\ \midrule
     20  & 0.50 & 0.76 / $\pm$ 0.03  \\ \midrule
     30  & 0.62 & 0.77 / $\pm$ 0.02  \\ \midrule
     50  & 0.76 & 0.83 / $\pm$ 0.06 \\ \midrule
     75  & 0.87 & 0.87 / $\pm$ 0.05 \\ \midrule
    100 & 0.94 & 0.87 / $\pm$ 0.05  \\ \midrule
\end{tabular}
\end{center}

There are many permutations 
We now also employ multi-class classificiation.

\begin{center}
\captionof{table}{Mean and standard deviation of the accuracy of a linear classifier trained on the the 4 most common classes of WordNet meanings for the word \textit{was}.}
\begin{tabular}{SSSSSSSS} \toprule
    {dimensionality} & {variance kept} & {accuracy (mean / $\pm$ stddev)}  \\ \midrule
       2  & 0.08 & 0.38 / $\pm$ 0.03 \\ \midrule
       3  & 0.11 & 0.38 / $\pm$ 0.04 \\ \midrule
     10  & 0.28 & 0.65 / $\pm$ 0.03  \\ \midrule
     20  & 0.43 & 0.76 / $\pm$ 0.04  \\ \midrule
     30  & 0.53 & 0.83 / $\pm$ 0.03  \\ \midrule
     50  & 0.67 & 0.93 / $\pm$ 0.01 \\ \midrule
     75  & 0.77 & 0.95 / $\pm$ 0.01 \\ \midrule
    100 & 0.83 & 0.95 / $\pm$ 0.01  \\ \midrule
\end{tabular}
\end{center}

We get very similar accuracies for "time", "made", "thought".
However, these tables are left out as we do not deem these to be statistically significant.

%TODO re-run these experiments and print out the respective ids

%TODO Inseert confusion matrix here maybe




\section{On the Clusterability of meaning within sampled BERT vectors} \label{experiment_BERT_clusterability}

\subsection{Motivation}

%TODO Talk about different ways to determine if a dataset is clusteerable, but mention that in the end we want a practical approach, thus we brute-force search
%TODO 


To see if there is any structure within BERT vectors w.r.t. the different meaning of one word, we ask ourselves whether or not different part of the meaning are at different locations of the embedding space produced by BERT.

This is an extension to linear separability.
We want to see how obvious these clusterings and differences are.


\subsection{Experiment setup}

The general algorithm to cluster the dataset is shown below.

%TODO include optimization loop over all models

%TODO Define and show what the random adjusted index is

\begin{algorithm}[H]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\Input{
$w_{\text{target}}$: The target word whose BERT vectors we want to cluster; \\ 
$DimRed$: The dimensionality reduction method; \\
$k$ : The latent dimensionality for the dimensionality reduction method; \\ 
$ClusterAlgorithm$: The clustering algorithm to use;
%TODO insert optional l1/l2 normalization
}
\KwResult{The associated cluster-id with each sampled sentence, and the adjusted random index.}

 $\mathbf{D}, \mathbf{y} \leftarrow $  sample up to 500 sentences from the SemCor corpus which include the word $w_{\text{target}}$, along with the corresponding WordNet meaning-id, and compensate more sentences by sampling from the news corpus if less than 500 sentences are available in the SemCor sentence.
We set y = -1 whenever no labeling information is available, which is the case if we don't sample data from the SemCor corpus.\;

$ \mathbf{X} \leftarrow BERT( \mathbf{D} )$ i.e. pass each sentence through BERT and retrieve the resulting word embedding $x_w$ as defined in the above section\;
 
$ \mathbf{X}, \mathbf{y} \leftarrow oversample( \mathbf{X}, \mathbf{y} )$ such that we don't have dominating classes (all except for $y = -1$).\;
 
$ \mathbf{X} \leftarrow StandardScaler( \mathbf{X})$ such that all the data is normalized\;

$ \mathbf{X} \leftarrow DimRed( \mathbf{X}, k )$ such that all the data is projected to a lower latent dimensionality $k$\;

$ model \leftarrow ClusterAlgorithm( \mathbf{X})$ \;

$ \mathbf{\hat{y}} \leftarrow model.predict(\mathbf{X}) $ \;

$ score \leftarrow AdjustedRandomIndex(\mathbf{\hat{y}}, \mathbf{y}) $ \;

return $ score, \mathbf{\hat{y}}$\;
    
 \caption{Checks sampled BERT vectors for clusters by  meaning}
\end{algorithm}

However, because some simple algor.

We had a few constraints.
When clustering, we are not given the number of cluster to be found. 
Thus, the algorithm must solve the multi-modal detection problem intrinsically, which is considered a hard problem in machine learning, as it falls in the same category as global probability density estimation.
Also, because we use the lexical distinction of semantics as defined in WordNet, our algorithm needs to adapt to the granularity that was defined by linguists.

Because we want to evaluate how well our clustering method can mimick the semantic definitions in WordNet, but also generalize to unseen words, we train on an unsupervised dataset. 
We then test our resulting clustering on a labeled datasetet using the random adjusted index \cite{rand71}, \cite{hubert85}.

The adjusted random index calculates the overlap and as such the similarity between two clustering assignments.
Specifically, 

\begin{equation}
A R I=\frac{\sum_{i j}\left(^{n_{i j}}\right)-\left[\sum_{i}\left(\begin{array}{c}a_{i} \\ 2\end{array}\right) \sum_{j}\left( \begin{array}{c}b_{j} \\ 2\end{array} \right)\right] /\left(\begin{array}{c}n \\ 2\end{array}\right)}{\frac{1}{2}\left[\sum_{i}\left(\begin{array}{c}a_{i} \\ _{2}\end{array}\right)+\sum_{j}\left(\begin{array}{l}b_{j} \\ 2\end{array}\right)\right]-\left[\sum_{i}\left(\begin{array}{c} a_{i} \\ 2\end{array}\right) \sum_{j}\left(\begin{array}{l}b_{j} \\ 2\end{array}\right)\right] /\left(\begin{array}{l}n \\ 2\end{array}\right)}
\end{equation}{\label{eq:adjustedrandomindex}}

%TODO read this up again!
where $n_{i,j}$ is the number of items that are present in both clustering assignment, $a_i = \sum_j n_{i,j}$ , $b_j = \sum_i n_{i,j}$ for two clustering $a$ and $b$.

where the resulting score is between $[-1.0, 1.0]$, where a score of $0$ implies completely assignment of cluster-labels into random buckets, $-1.0$ implies a negative correlation, and $1.0$ implies perfect similarity.
The adjusted random index is adjusted for chance, by taking all possible permutations of the labels that the clustering algorithm can have.




Although we will not go into too much detail with the algorithm descriptions here, we will 


The general algorithm to cluster the dataset is shown below.



\begin{enumerate}
\item s
\end{enumerate}

%TODO talk about the chinese whispers algorithm bcs this is custom-made

\subsection{Results}

%\begin{center}
%\captionof{table}{Adjusted Random Index between the predicted cluster labels and the true underlying cluster labels. 
%Cluster labels correspond to the different meanings in WordNet.
%BERT vectors are sampled for the word "use". 
%We sample 500 vectors and project these to 50 dimensions before clustering.
%}
%\begin{tabular}{SSSSSSSS} \toprule
%    {model} & {best adjusted random score} & {best adjusted random score)}  \\ \midrule
%       Affinity Propagation  & 0.001 & 0.38 / $\pm$ 0.03 \\ \midrule
%       DBScan                     & 0.000 & 0.38 / $\pm$ 0.04 \\ \midrule
%      HDBScan                    & 0.328 & 0.65 / $\pm$ 0.03  \\ \midrule
%     MeanShift                   & 0.004 & 0.76 / $\pm$ 0.04  \\ \midrule
%     Optics                        & 0.000 & 0.83 / $\pm$ 0.03  \\ \midrule
%\end{tabular}
%\end{center}

\newcolumntype{b}{X}
\newcolumntype{s}{>{\hsize=.5\hsize}X}

\begin{table}[htbp]
    \centering
    %\begin{tabularx}{\textwidth}{| X | X |}
    \begin{tabularx}{\textwidth}{b|ss}
    \toprule
      {model} & {ARI)}  \\ \hline
        Affinity Propagation     & 0.001     \\ \hline
        DBScan                        & 0.000      \\ \hline
        HDBScan                      & 0.328     \\ \hline
        MeanShift                    & 0.004      \\ \hline
        Optics                         & 0.000      \\ \hline
    \end{tabularx}
\end{table}

Repeating the experiment with 1000 datapoint does not change the results by much.

\begin{table}[htbp]
    \centering
    %\begin{tabularx}{\textwidth}{| X | X |}
    \begin{tabularx}{\textwidth}{b|ss}
    \toprule
      {model} & {ARI)}  \\ \hline
        Affinity Propagation     & 0.000     \\ \hline
        DBScan                        & 0.139      \\ \hline
        HDBScan                      & 0.271     \\ \hline
        MeanShift                    & 0.005      \\ \hline
        Optics                         & 0.070      \\ \hline
    \end{tabularx}
\end{table}

Repeating the experiment with 1000 items and projecting PCA to 100 results in 

\begin{table}[htbp]
    \centering
    %\begin{tabularx}{\textwidth}{| X | X |}
    \begin{tabularx}{\textwidth}{b|ss}
    \toprule
      {model} & {ARI)}  \\ \hline
        Affinity Propagation     & 0.000     \\ \hline
        DBScan                        & 0.215      \\ \hline
        HDBScan                      & 0.359     \\ \hline
        MeanShift                    & 0.003      \\ \hline
        Optics                         & 0.000      \\ \hline
    \end{tabularx}
\end{table}

We see that including more dimensions make the clustering better w.r.t. the adjusted random score.

%TODO Move to 
Here, we also introduce the chinese whispers algorithm.
The chinese whispers algorithm was used to cluster for word-senses in the context of static word embeddings, such as in
\cite{pelevina16}.

We now apply forceful clustering using Bayesian Optimization.

%TODO: Move some to Appendix (especially when it is not very successful...
100 latent dimensions, 500 samples
\begin{table}[htbp]
    \centering
    %\begin{tabularx}{\textwidth}{| X | X |}
    \begin{tabularx}{\textwidth}{b|ss}
    \toprule
      {model} & {ARI)}  \\ \hline
        Affinity Propagation     & 0.168     \\ \hline
        Chinese Whispers        & 0.298     \\ \hline
        DBScan                        & 0.201      \\ \hline
        HDBScan                      & 0.242     \\ \hline
        MeanShift                    & 0.167      \\ \hline
        Optics                         & 0.167      \\ \hline
    \end{tabularx}
\end{table}


100 components, 1000 samples
\begin{table}[htbp]
    \centering
    %\begin{tabularx}{\textwidth}{| X | X |}
    \begin{tabularx}{\textwidth}{b|ss}
    \toprule
      {model} & {ARI)}  \\ \hline
        Affinity Propagation     & 0.170     \\ \hline
        Chinese Whispers        & 0.249     \\ \hline
        DBScan                        & 0.260      \\ \hline
        HDBScan                      & 0.234     \\ \hline
        MeanShift                    & 0.167      \\ \hline
        Optics                         & 0.197      \\ \hline
    \end{tabularx}
\end{table}


Projecting to lower dimensions increases accuracy strongly.
20 components, 1000 samples
\begin{table}[htbp]
    \centering
    %\begin{tabularx}{\textwidth}{| X | X |}
    \begin{tabularx}{\textwidth}{b|ss}
    \toprule
      {model} & {ARI)}  \\ \hline
        Affinity Propagation     & 0.165     \\ \hline
        Chinese Whispers        & 0.349     \\ \hline
        DBScan                        & 0.167      \\ \hline
        HDBScan                      & 0.273     \\ \hline
        MeanShift                    & 0.226      \\ \hline
        Optics                         & 0.167      \\ \hline
    \end{tabularx}
\end{table}



Projecting to lower dimensions increases accuracy strongly.
and now we add a SEP token
20 components, 1000 samples
\begin{table}[htbp]
    \centering
    %\begin{tabularx}{\textwidth}{| X | X |}
    \begin{tabularx}{\textwidth}{b|ss}
    \toprule
      {model} & {ARI}  \\ \hline
        Affinity Propagation     & 0.316     \\ \hline
        Chinese Whispers        & 0.457     \\ \hline
        DBScan                        & 0.170      \\ \hline
        HDBScan                      & 0.298     \\ \hline
        MeanShift                    & 0.251      \\ \hline
        Optics                         & 0.167      \\ \hline
    \end{tabularx}
\end{table}

We now analyse the embeddings manually

\subsubsection{Qualitative evaluation}

Given that the above numbers likely don't mean anything, here are some clusters found for the best model configuration (chinese whispers), applied on a set of words.



Can see that this can be used for biased and insecure AI applications!

\begin{figure}[H]
\begin{align}
\text{                                                                  } & \text{- - - -} \nonumber \\
\text{Ms. Gotbaum tried to slide her handcuffed } & \text{arms from her back to her front} \nonumber \\
\text{                                                                  } & \text{- - - -} \nonumber \\
               \text{That magisterial use of the upper body and } & \text{arms is her physical signature} \nonumber \\
\text{entered the stage in pairs and forcefully stretched their } & \text{arms and legs} \nonumber \\ 
                                    \text{ripped from patientsâ } & \text{arms as they were carried away} \nonumber \\
                                    \text{                                                                  } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}


\begin{figure}[H]
\begin{align}
\text{                                              } & \text{- - - -} \nonumber \\
\text{She swooped him up into her } & \text{arms and kissed him madly} \nonumber \\
\text{                                              } & \text{- - - -} \nonumber \\
\text{I place my son in her } & \text{arms and I pray that it somehow comforts her} \nonumber \\
\text{perfect babies \ldots into the loving } & \text{arms of middle class+ Americans} \nonumber \\
\text{which he falls back into her } & \text{arms like a baby} \nonumber \\
\text{Sometimes I took it into my } & \text{arms and felt its surprising heft} \nonumber \\
\text{                                              } & \text{- - - -} \nonumber
\end{align}
\caption{A famous equation}
\end{figure}


\begin{figure}[H]
\begin{align}
\text{                              } & \text{- - - -} \nonumber \\
\text{and shuttle robotic } & \text{arms of a solar array and truss}  \nonumber\\
\text{                              } & \text{- - - -} \nonumber \\
\text{a contingent of young } & \text{arms that will allow us to win now}  \nonumber\\
\text{By and large, those } & \text{arms remained as fictional as those in "The War \ldots"}  \nonumber\\
\text{and extensive use of robotic } & \text{arms operating at their limits}  \nonumber\\
\text{staff of strong young } & \text{arms that might have tamed the National League East} \nonumber \\
\text{                              } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}



\begin{figure}[H]
\begin{align}
\text{                                                   } & \text{- - - -} \nonumber \\
\text{this country is so polarized that people spring to } & \text{arms against any proposal} \nonumber \\
\text{                                                   } & \text{- - - -} \nonumber \\
                        \text{At least they are carrying } & \text{arms to protect themselves} \nonumber \\
                 \text{His organization issued a call to } & \text{arms} \nonumber \\
      \text{he shoves it in the faces of his comrades in } & \text{arms} \nonumber \\
                           \text{people who had taken up } & \text{arms against the United States} \nonumber \\
                    \text{Mostly non-Arab rebels took up } & \text{arms in early 2003} \nonumber \\
\text{                                                   } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}


\begin{figure}[H]
\begin{align}
\text{                                                   } & \text{- - - -} \nonumber \\
                      \text{The classic years of the} & \text{arms race, the 1950s and â60s before} \nonumber \\
                      \text{                                                   } & \text{- - - -} \nonumber \\
                    \text{that concerns over nuclear} & \text{arms proliferation in the Middle East} \nonumber \\
                  \text{Russian adherence to another} & \text{arms control treaty} \nonumber \\
       \text{he will press for peace and an eventual} & \text{arms cut for the states} \nonumber \\
       \text{payments to the companies that supplied} & \text{arms to Iraq were often delayed} \nonumber \\
\text{Mr. Safar denies any wrongdoing, including any} & \text{arms dealings} \nonumber \\
\text{                                                   } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}



\begin{figure}[H]
\begin{align}
\text{                                              } & \text{- - - -} \nonumber \\
           \text{leaned back in his chair and, with} & \text{ arms crossed,} \nonumber \\
\text{                                              } & \text{- - - -} \nonumber \\
\text{God, fully in name, is at the bottom with his} & \text{ arms out wide.} \nonumber \\
    \text{If you feel yourself falling, spread your} & \text{ arms} \nonumber \\
                                   \text{Agamemnon,} & \text{ arms raised ... barely contained violence} \nonumber \\
                       \text{Mr. James sat with his} & \text{ arms folded, his head lowered} \nonumber \\
      \text{she felt tears in her eyes and held her} & \text{ arms out in simple joy} \nonumber \\
\text{                                             } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}

NOW MOVING OVER TO THE BANK EXAMPLE


\begin{figure}[H]
\begin{align}
\text{                                                 } & \text{- - - -} \nonumber \\
\text{heavy withdrawals from the British} & \text{ bank Northern Rock reignited concern} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\text{credit card and other consumer loans, forcing the} & \text{ bank to set aside \$3.4 billion} \nonumber \\
\text{by a mainland Chinese commercial bank in a U.S.} & \text{ bank} \nonumber \\
\text{Investors fear the} & \text{ bank will be forced to write down} \nonumber \\
\text{investors hoped that the} & \text{ bank had disclosed the} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}


\begin{figure}[H]
\begin{align}
\text{                                              } & \text{- - - -} \nonumber \\
\text{I would expect the} & \text{ bank by the trail to the left of the road to have been broken down} \nonumber \\
\text{                                              } & \text{- - - -} \nonumber \\
\text{The current slowed and swirled alongside a mud} & \text{ bank where cows had trodden to the water} \nonumber \\
\text{But the} & \text{ bank had positioned itself well} \nonumber \\
\text{provide plant scientists and farmers with a} & \text{ bank of genes} \nonumber \\
\text{Upstairs at the large} & \text{ bank of cashiers} \nonumber \\
\text{                                              } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}


\begin{figure}[H]
\begin{align}
\text{                                                 } & \text{- - - -} \nonumber \\
\text{of their familyâs naturalization â} & \text{ bank deposit by bank deposit} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\text{12 that the company might suffer a run on the} & \text{ bank because of mortgage concerns} \nonumber \\
\text{Third, per several} & \text{ bank managers of major national banks} \nonumber \\
\text{Local party bosses gained broad powers over state} & \text{ bank lending, taxes} \nonumber \\
\text{government contracts, and a web of} & \text{ bank accounts} \nonumber \\
\text{Prince Bandarâs Washington} & \text{ bank accounts} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}


\begin{figure}[H]
\begin{align}
\text{                                                    } & \text{- - - -} \nonumber \\
 \text{to lift some of the mystery surrounding the central} & \text{ bank and improve communications with Wall Street} \nonumber \\
\text{                                                    } & \text{- - - -} \nonumber \\
\text{many economists had predicted that the} & \text{ bank would not cut its rate} \nonumber \\
\text{expectations that the central} & \text{ bank will raise interest} \nonumber \\
\text{a hint that the central} & \text{ bank plans to hold rates} \nonumber \\
\text{Chinaâs central} & \text{ bank has stepped up its already huge purchases of dollar-denominated securities} \nonumber \\
\text{fertilizer prices in African countries, but that the} & \text{ bank itself had often failed to recognize} \nonumber \\
\text{                                                    } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}


\begin{figure}[H]
\begin{align}
\text{                                              } & \text{- - - -} \nonumber \\
\text{citing challenges for the investment} & \text{ bank and the potential for an above-average credit burden} \nonumber \\
\text{                                              } & \text{- - - -} \nonumber \\
\text{93 percent drop in profits at its investment} & \text{ bank last week} \nonumber \\
\text{UBS said it did not expect its investment} & \text{ bank to return to profitability} \nonumber \\
\text{defrauded by the investment} & \text{ bank in 1998 when} \nonumber \\
\text{said in an interview that the investment} & \text{ bank approached him last month \nonumber} \\
\text{said the leaders of Citigroupâs investment} & \text{ bank and alternative} \nonumber \\
\text{                                            } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}

EXAMPLES FOR KEY CLUSTERING

\begin{figure}[H]
\begin{align}
\text{                                                 } & \text{- - - -} \nonumber \\
\text{Many of the} & \text{ key Arab states } \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\text{in two months and Australia's} & \text{ key S\&P ASX 200 shed 1.9 percent } \nonumber \\
\text{Wall Street rebounded Wednesday after} & \text{ key earnings reports from JPMorgan Chase \& Co.} \nonumber \\
\text{The Democratic candidate hires a} & \text{ key strategist} \nonumber \\
\text{[CLS] Mr. Jones âquickly established a good rapport with} & \text{ key donorsâ } \nonumber \\
\text{able to meet the two} & \text{ key officials in the government} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}



\begin{figure}[H]
\begin{align}
\text{                                                 } & \text{- - - -} \nonumber \\
\text{former president of Trinity College, who played a} & \text{ key role in designing the test} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\text{seen in the West as a} & \text{ key yardstick of the fairness of an election} \nonumber \\
\text{treat ... as a} & \text{ key factor in its decisions about regulatory issues} \nonumber \\
\text{which policy makers have called the} & \text{ key test for deciding whether to lower interest rates} \nonumber \\
\text{9. 11. as a} & \text{ key element in pitch meetings} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}



\begin{figure}[H]
\begin{align}
\text{                                                 } & \text{- - - -} \nonumber \\
\text{Interstate 5 is a} & \text{ key route connecting Southern and Northern California} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\text{ A} & \text{ key piece of new functionality for Ops Center } \nonumber \\
\text{Youssef Squali at Jefferies \& Co. says two} & \text{ key factors are driving the stock up} \nonumber \\
\text{transforming connection with believers is a} & \text{ key element of evangelical Christianity} \nonumber \\
\text{What would you say was the} & \text{ key element of your management style that allowed you to stabilize H.P.} \nonumber \\
\text{is a} & \text{ key indicator of retailer performance} \nonumber \\
\text{in the West the} & \text{ key players were not a small group of intellectuals reading Greek sources } \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}

\begin{figure}[H]
\begin{align}
 \text{                                                 } & \text{- - - -} \nonumber \\
\text{times change and technology advances, the} & \text{ key to the city symbolizes } \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\text{And an official, five-and-three-quarters-inch-long gold-plated pewter} & \text{ key to prove it} \nonumber \\
\text{but it is small enough to fit onto a} & \text{ key chain} \nonumber \\
\text{In it lay three keys on a} & \text{ key chain in the shape of a red speedboat} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}


\begin{figure}[H]
\begin{align}
\text{                                                 } & \text{- - - -} \nonumber \\
\text{The Red Sox will now have all their} & \text{ key players from their 2007 championship team} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\text{Mike Green scored 12 points and had a} & \text{ key assist in overtime as No. 22 Butler beat Virginia Tech} \nonumber \\
\text{or taking the chance of losing a} & \text{ key player to injury} \nonumber \\
\text{But they never led, could not get a} & \text{ key basket at crucial times and played like a team } \nonumber \\
  \text{but Cam Long stole the ball near the top of the} & \text{ key and ran out the clock} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}


\begin{figure}[H]
\begin{align}
\text{                                                 } & \text{- - - -} \nonumber \\
\text{Three of their} & \text{ key players played more than 40 minutes in Sacramento} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\text{A} & \text{ key for the Giants on Sunday} \nonumber \\
\text{chemical reactions on solid surfaces, which are} & \text{ key to understanding questions like why the ozone layer is thinning} \nonumber \\
\text{but is she part of the conspiracy or the} & \text{ key to Simâs salvation?} \nonumber \\
\text{whose ability to play on a sprained ankle against the Eagles} & \text{ key to that matchup} \nonumber \\
\text{Horses have been the lifelong} & \text{ key to satisfying the real feminine needs for me and my daughter} \nonumber \\
\text{Connecticut cornerback said the} & \text{ key to defeating Louisville would be pressuring Brohm} \nonumber \\
\text{                                                 } & \text{- - - -} \nonumber \\
\end{align}
\caption{A famous equation}
\end{figure}






\section{Correlation between Part of Speech and Context within BERT} \label{correlation_pos_context}

\subsection{Motivation}

%TODO think about why you did this lol
Because there are more resources in NLP with Part of Speech (PoS) tags, as compared to semantics, we want to analyse to what extent BERT sees similarities between PoS and, and because we assume a strong correlation between PoS and semantics, we analyse to what extent this is visible within BERT vectors.

\subsection{Experiment setup}

We test the hypothesis "semantics implies PoS" by conductin the following experiment.
For a chosen target word $w_t$, we fixate one of the wordnet meanings.
We then sample $n$ sentences for the target word $w_t$ where $w_t$ has semantic meaning $m$ in the occurring sentence.
After we have sampled all the sentences, we determine the PoS for the target word $w_t$.
We then calculate the percentage occurrence of the majority PoS class and record this as a percentage.
If all of the sampled target words $w_t$ for all the sentences have the same assigned PoS tag, then the score results in a value of $1.0$.
If the dominant PoS tag occurs only half the time, this number decreases to $0.5$.
Please notice that in this experiment, we only view simple PoS tags (i.e. "noun", "verb", "adjective", "pronoun"), and not the more complex ones listed above.

\subsection{Results}

It is apparent that there is a strong relation between PoS and meaning. 
Especially "erstarrte" Verben are a strong part of this

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{./assets/analysis/run_pca.png}
  \caption{1a}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{./assets/analysis/run_umap.png}
  \caption{1b}
  \label{fig:sfig2}
\end{subfigure}
\caption{plots of....}
\label{fig:fig}
\end{figure}



\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{./assets/analysis/block_pca.png}
  \caption{1a}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{./assets/analysis/block_umap.png}
  \caption{1b}
  \label{fig:sfig2}
\end{subfigure}
\caption{plots of....}
\label{fig:fig}
\end{figure}



\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{./assets/analysis/cold_pca.png}
  \caption{1a}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{./assets/analysis/cold_umap.png}
  \caption{1b}
  \label{fig:sfig2}
\end{subfigure}
\caption{plots of....}
\label{fig:fig}
\end{figure}

%TODO make another plot which is more coarse

%\chapter{Evaluation} 
\chapter{Exploiting subspace organization of semantics of BERT embeddings}

The general pipeline of how BERT processes sentences is as follows.
We have a sentence $s$ which includes a set of words $w_1$, \ldots, $w_n$.
This sequence of words is now given as input to the BERT model.
First, this sequence of words is split into a set of tokens $t_1$, \ldots, $t_m$, where $m \geq n$.
Now these tokens, which are in the vocabulary of the BERT tokenizer, are converted to indicies, which correspond to index of each individual embedding inside BERT.

%TODO properly define which symbols etc. you want to use ... this should be formally rigorous enough

%TODO Talk about the tokenizer here again

\begin{figure}[h]
	\center
  \includegraphics[width=\linewidth]{./assets/experiments/pipeline_vanilla_BERT.png}
  \caption{The BERT model takes as input a sentence $s$. The sentence $s$ is converted to a sequence of BERT tokens $t_1$, \ldots, $t_m$ as defined in a given vocabulary $V$.
Each item in the vocabulary $V$ has a corresponding embedding vector inside the embedding layer of the transformer.
This embedding vector is used by the intermediate layers of the transformer, and thus affects the downstream pipeline of the transformer for any subsequent layers of the transformer.
}
  \label{fig:BERT_vanilla_pipeline}
\end{figure}


We introduce \textit{split-words}, for which we will be generating more specific embeddings.
In particular, 
The idea behind this is that introducing more specialized embeddings for certain tokens will allow to model more complex distributions.

Because the non-modified (vanilla) BERT model uses a certain workflow, we will shortly introduce BERTs pipeline.

\subsection{BERnie PoS} \label{bernie_pos}

\subsubsection{Motivation}

We want to start with a simple model first.
Because we have seen in the above section, that there is a strong correlation between semantics and part-of-speech, the initial idea is to introduce additional embedding vectors which are able to capture semantics (rather than just purely word tokens) better.
In this case, part-of-speech is used as a proxy for semantics.

\subsubsection{Experiment setup}

BERnie PoS introduces one new embedding vector for each possible PoS configuration of the split-words.

As an example, instead of having a single embedding vector for the word \textit{run}, we introduce two new embeddings \textit{run\_  VERB} and \textit{run\_ NOUN}, which both replace the intial \textit{run}-embedding. 
We will refer to \textit{run\_ VERB} and \textit{run\_ NOUN} as \textit{sub-embeddings}, as these exploit the subspace structure of the context embeddings sampled for the word \textit{run}.
As for the tokenizer, we also introduce a mechanism which turns any occurrence of \textit{run} into one of the sub-embeddings.

Specifically, our desired pipeline would not look as follows.

\begin{figure}[H]
	\center
  \includegraphics[width=\linewidth]{./assets/experiments/pipeline_tokenizer_BERnie_POS_input.png}
  \caption{The modified pipeline. 
  The BERnie model takes as input a sentence $s$. The sentence $s$ is converted to a sequence of BERT tokens $t_1$, \ldots, $t_m$ as defined in a given vocabulary $V$.
For each target token $t_{\text{target}}$, we make the token more specific by converting the token to a more specialized token-representation, which specifies the part-of-speech information as part of the token.
In this case, $run$ becomes $run\_ VERB$.
Again, each item in the vocabulary $V$ has a corresponding embedding vector inside the embedding layer of the transformer.
This embedding vector is used by the intermediate layers of the transformer, and thus affects the downstream pipeline of the transformer for any subsequent layers of the transformer.
}
  \label{fig:BERnie_POS_pipeline}
\end{figure}


To identify whether the occurring \textit{run} in an example sentence is a verb or a noun, we use the spaCy part-of-speech tagger \cite{spacyb}, which claims 92.6 \% accuracy for this task.

\begin{figure}[h]
	\center
  \includegraphics[width=\linewidth]{./assets/experiments/pipeline_model_BERnie_POS_initialization.png}
  \caption{Inside the embedding layer of BERT, we introduce more specific embeddings \textit{run\_ VERB} and \textit{run\_ NOUN}. The BERT model should intuitively now capture more expressiveness, as the model size increased. The original \textit{run} embedding is removed.}.
  \label{fig:BERnie_POS_pipeline}
\end{figure}




\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./assets/experiments/pipeline_tokenizer_BERnie_POS_sentence.png}
  \caption{1a: The part-of-speech modified tokenizer pipeline}
  \label{fig:sfig1}
\end{subfigure}%
\quad
\begin{subfigure}{.65\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./assets/experiments/pipeline_model_BERnie_POS.png}
  \caption{1b: The part-of-speech modified BERT embedding}
  \label{fig:sfig2}
\end{subfigure}
\caption{plots of....}
\label{fig:fig}
\end{figure}

\subsection{BERnie Meaning}\label{experiment_bernie_meaning}

\subsubsection{Motivation}

Similar to the above model, we are introducing new BERT embeddings.
This time however, we do not introduce new tokens by part-of-speech, as we did in the previous section, but rather by semantics.
For this, we use the a similar methodology as the model from the section \ref{bernie_pos}.
However, instead of replacing word by their part-of-speech specizalization, we replace words by some semantic specialization.
%TODO how to cite sections
The idea here is to adhere more strongly to the subspace structure as seen in section \ref{correlation_pos_context}.

Thus, we introduce the clustering methodology introduced in \ref{experiment_BERT_clusterability}.

\subsubsection{Experiment setup}

Again, we start with the standard BERT model, whose pipeline looks as follows.
\begin{figure}[H]
	\center
  \includegraphics[width=\linewidth]{./assets/experiments/pipeline_tokenizer_BERnie_meaning_.png}
  \caption{}
  \label{fig:cbow_skipgram}
\end{figure}

We now modify the tokenizer in such a way, that ambigious and polysemous words are replaced with a more specific token.
As an example, we want to replace the word $bank$ with a token, which captures whether or not the $bank$ that we refer to implies a 1) financial institution, or 2) a river bank.

We use the our clustering approach from \ref{experiment_BERT_clusterability} as the intermediate model to distinguish on which bank the sentence refers to.
We also introduce new embedding vectors that the new tokens correspond to.

\begin{figure}[H]
\center
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./assets/experiments/pipeline_tokenizer_BERnie_meaning.png}
  \caption{1a: The part-of-speech modified tokenizer pipeline}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.55\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./assets/experiments/pipeline_model_BERnie_meaning_embedding.png}
  \caption{1b: The part-of-speech modified BERT embedding}
  \label{fig:sfig2}
\end{subfigure}
\caption{plots of....}
\label{fig:fig}
\end{figure}

After we have concluded these changed, we arrive at the following model, which has a modified tokenizer, and a modified BERT model.
We call the corresponding model \textbf{BERnie}.

\begin{figure}[H]
	\center
  \includegraphics[width=\linewidth]{./assets/experiments/pipeline_model_BERnie_meaning.png}
  \caption{The BERT model takes as input a sentence $s$. The sentence $s$ is converted to a sequence of BERT tokens $t_1$, \ldots, $t_m$ as defined in a given vocabulary $V$.
Each item in the vocabulary $V$ has a corresponding embedding vector inside the embedding layer of the transformer.
This embedding vector is used by the intermediate layers of the transformer, and thus affects the downstream pipeline of the transformer for any subsequent layers of the transformer.}
  \label{fig:cbow_skipgram}
\end{figure}

%TODO Show results here...

\subsubsection{Results}

We now do a short evaluation of the introduced BERnie PoS and BERnie model.
To understand on a wide scale, we use the GLUE benchmarks to understand what effect the extension of the embedding layer on points of high variance has.

%TODO Write section for GLUE tasks
\begin{table}
\center
\scalebox{0.9}{
\begin{tabular}{
 l % left aligned column
 l % left aligned column
 c
 c
 c
 c
 c
 % *{5}{S[table-format=4.0]} % three columns with numeric data       
}
\toprule
%TODO MRPC has one aligned, and one unaligned test
&&\textbf{BERT} & \textbf{BERnie PoS} & \textbf{BERnie} \\
\midrule
CoLA  & Accuracy  &  \textbf{0.5739} & 0.5263 &  0.5457 \\

MRPC & Accuracy &  \textbf{0.8223} & 0.8199 &  0.8064 \\
          & F1           &  \textbf{0.8778} & 0.8614  &  0.8684 \\
          & Mixed     &  0.8501 & \textbf{0.8579}  &  0.8374 \\
          
SST-2 & Accuracy  &  0.9214 & 0.9203 &  \textbf{0.9266} \\

STS-B & Correlation &  \textbf{0.8841} & 0.8615 &  0.8574 \\
      & Pearson & \textbf{0.8860} & 0.8621         & 0.8587 \\
      & Spearman & \textbf{0.8822} & 0.8601      & 0.8567 \\
      
QNLI & Accuracy &  \textbf{0.9126} & 0.9090    &  0.9020 \\

RTE & Accuracy &  \textbf{0.6462} & 0.6083     &  \textit{0.5722} \\

WNLI & Accuracy &  \textit{0.35915} & 0.3947 &  \textbf{0.4649} \\

MNLI & Accuracy &  0.8453 &              &  0.8334 \\

SNLI & Accuracy &  \textbf{0.8461} &              &  0.8367 \\

QQP & Accuracy &  \textbf{0.9113} &              &  0.9042 \\
      & F1     & \textbf{0.8809} &                       & 0.8732 \\
      & Mixed & \textbf{0.8961} &                    & 0.8887 \\
      
\bottomrule

\end{tabular}
}
\caption{asj}
\end{table}


One can see that in general, the BERnie models performs slightly worse.


All values are the average of two runs. 
Weights which are instantiated specifically for the GLUE tasks are once instantiated with a random seed of 42, and once with a random seed of 101.

Although most experimental results are similar between standard BERT and the modified BERnie PoS and BERnie models, the WNLI and RTE experiments are considerably deviating in performance through the addition of the additional embedding vectors.
For the BERnie model, WNLI performs \textbf{10.56\%} better than standard BERT, and RTE performs  \textbf{-7.40\%} worse than the standard BERT implementation.
Similar results are observable for the difference between the unmodified BERT and the BERnie PoS model with \textbf{3.52\%} improvement for WLNI and \textbf{-3.79\%} decline in performance for the RTE task.

Also, compared to BERnie PoS, the standard BERnie model amplifies the performence-difference to BERT in both the negative and positive direction.


\subsection{BERnie Meaning with additional pre-training}

\subsubsection{Motivation}
In section \ref{experiment_bernie_meaning}, we can see that extending the BERT model generally leads to worse performance.

We assume that the BERnie require additional pre-training, as we are adding more specific embedding vectors, without fine-tuning them.
We assume that this is necessary, as this is how the weights of BERT are also trained in the original paper \cite{devlin18} through the masked language model approach.

\subsubsection{Experiment setup}

We evaluate the models of the previous experiments.

\begin{center}
\captionof{table}{Mean and standard deviation of the accuracy of a linear classifier trained on the the 4 most common classes of WordNet meanings for the word \textit{was}.}
\begin{tabular}{SSSSSSSS} \toprule
    {dimensionality} & {variance kept} & {accuracy (mean / $\pm$ stddev)}  \\ \midrule
       2  & 0.08 & 0.38 / $\pm$ 0.03 \\ \midrule
       3  & 0.11 & 0.38 / $\pm$ 0.04 \\ \midrule
     10  & 0.28 & 0.65 / $\pm$ 0.03  \\ \midrule
     20  & 0.43 & 0.76 / $\pm$ 0.04  \\ \midrule
     30  & 0.53 & 0.83 / $\pm$ 0.03  \\ \midrule
     50  & 0.67 & 0.93 / $\pm$ 0.01 \\ \midrule
     75  & 0.77 & 0.95 / $\pm$ 0.01 \\ \midrule
    100 & 0.83 & 0.95 / $\pm$ 0.01  \\ \midrule
\end{tabular}
\end{center}

%TODO Perhaps leave out the very bad results, or focus on debugging?

%TODO Link above section to this ...
WNLI is a comprehension task where a model must read a sentence with a pronoun, and select the referent of that pronoun form a list of choices.

Each one is contingent on contextual information provided by a single word or phrase in the sentence. To convert the problem into sentence pair classification, we construct sentence pairs by replacing the ambiguous pronoun with each possible referent.
%TODO : Describe what kind of task WNLI is
%TODO : Describe what kind of task RTE is

%TODO we use the GLUE tasks to understanding how introducing more embedding vectors affects the performance of our search models.


To train an ablation study, whether or not additional pre-training improves the instantiated word-vectors, we conduct the following experiment.
We instantiate the additional embeddings as described in the previous section.
We then run one full epoch of training with the same training parameters as used for GLUE on the news.corpus.2007 dataset using a masked languag emodel methodology .
%TODO cite BERT training methodology
We save this model and load it for the GLUE tasks.
BERnie full Pre-training trains the entire embeddings, whereas BERnie partial Pre-Training fixates all embedding vectors except the ones for the ones that were newly added.

%TODO Write section for GLUE tasks
\captionof{table}{Mean of mulitple experiments for BERnie with addiitonally trained embeddings and weights.}
\scalebox{0.9}{
\begin{tabular}{
 l % left aligned column
 l % left aligned column
 c
 c
 c
 %*{3}{S[table-format=4.0]} % three columns with numeric data       
}
\toprule
%TODO MRPC has one aligned, and one unaligned test
&& \textbf{BERnie} & \textbf{BERnie full\- Pre}  & \textbf{BERnie partial\- Pre}\\
\midrule
CoLA  & Accuracy  &  0.5457 & 0.5418 & 0.5731 \\
MRPC & Accuracy &  0.8064 & 0.3824 & 0.3162\\
          & F1           &  0.8684 & 0.1720 & 0.0000 \\
          & Mixed     &  0.8374 & 0.2775 & 0.1581 \\          
SST-2 & Accuracy  &  0.9266 & 0.9169 & 0.9266 \\
QNLI & Accuracy & 0.9020 & 0.5374 & 0.6700 \\
RTE & Accuracy &  0.5722 & 0.4784 & 0.4729 \\
WNLI & Accuracy & 0.4649 & 0.4225 & 0.4507 \\
\bottomrule
\end{tabular}
}

\section{Compressing the non-lexical out}
%TODO Need to run some more experiments ...
\subsubsection{Motivation}
\subsubsection{Experiment setup}

%TODO Project back (using the inverse) to "segment out" the bias and sentiment for downstream tasks

We conduct three experiments.

First, we use a simple algorithm to what extent we can disentangle the semantic space within a single sampled word.

The, we use a simple algorithm to what extent we can disentangle the semantic space within multiple sampled words, where we assume that the underlying semantics should again be mutually exclusive (i.e. the sampled words are not independent towards each other.

Finally, we use a simple algorithm to what extent we can disentangle the semantic space within multiple sampled words, where we don't inject any assumptions upon the underlying semantics of the words, i.e. the words are independent towards each other.


\subsubsection{Results}

%TODO These experiments are not final yet ...

%TODO Should I include the experimental data from the individual runs in a table in the Appendix?

\chapter{Conclusion}

\appendix
\singlespacing

% \bibliographystyle{unsrt} 
\bibliographystyle{plain}
\bibliography{dissertation} 

\end{document}
