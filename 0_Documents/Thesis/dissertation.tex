%% 
%% ACS project dissertation template. 
%% 
%% Currently designed for printing two-sided, but if you prefer to 
%% print single-sided just remove ",twoside,openright" from the 
%% \documentclass[] line below. 
%%
%%
%%   SMH, May 2010. 


\documentclass[a4paper,12pt,twoside,openright]{report}


%%
%% EDIT THE BELOW TO CUSTOMIZE
%%

\def\authorname{David Yenicelik \xspace}
\def\authorcollege{ETH ZÃ¼rich\xspace}
\def\authoremail{yedavid@ethz.ch}
\def\dissertationtitle{}
\def\wordcount{14,235}


%\usepackage[dvips]{epsfig,graphics} 
\usepackage{epsfig,graphicx,verbatim,parskip,tabularx,setspace,xspace}
\usepackage{amsmath, amsfonts}


%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing

%% START OF MAIN TEXT 

%\chapter{Introduction}
\chapter{Introduction}
\pagenumbering{arabic} 
\setcounter{page}{1} 

In Natural Language Processing (NLP), bilingual lexical induction (BLI) is a problem of inferring word-to-word mapping between two languages.
While supervised BLI may be learned trivially from a dictionary, unsupervised BLI is highly non-trivial, and serves as a backbone to many unsupervised Neural Machine Translation (NMT) systems, without which the overall MT performance drastically drops \cite{dropping_perf1} \cite{dropping_perf2}.
In addition to the unsupervised setting, words in a source language A often do not carry a one-to-one correspondence with words in a target language B.
This further increases the difficulty of finding a (bijective) mapping between the two embedding spaces.

\section{Scope of Work}


Continuing where \cite{density_matching} left off, we want to investigate the performance of using normalising flows to model unsupervised lexicon matching between two probability distributions, which are defined by Gaussian emebddings, which have a one-to-one correspondence to tokens in the respective languages.
We aim to use Gaussian embeddings for the robustness, and better integration into the probabilistic perspective.
The method discussed in the paper also relies a lot on the (semi-)supervised loss component.
We aim to investigate why this is the case, and would like to revise a method which is more robust in the fully unsupervised setting.\\

\textbf{Minimal goals:} We propose the following steps on achieving this goal.


\begin{enumerate}
    \item Replicate the algorithms and models which can generate through Gaussian embedding \cite{gaussian_embedding}. Do one sanity check by doing a sanity check on one of (SimLex / WordSim)
    \item Replicate "Density matching for bilingual word embedding" to setup a baseline normalising flow between vector-word-embeddings \cite{density_matching}.
    \item Define loss between predicted and target embeddings (if change in definition is necessary)
    \item Change the embeddings in point 2. to use Gaussian embeddings. Implement Loss functions found in point 3.
\end{enumerate}

\textbf{Extended goals:} If the above points provide good performance , we would like to expand on the below points.

\begin{enumerate}
    \item Implement a fully unsupervised extension by investigating the shortcomings of \cite{density_matching}.
    \item Investigate "deeper" normalising flows than the linear flow in \cite{density_matching} as such \cite{flowpp} \cite{neuralsplineflow}.
\end{enumerate}


\textbf{Contingency Plan / Further extended goals:} Implementing the following points would allow for an applied perspective of this approach, showing that this methodology allows for more robust mapping, also outside the field of NLP.


\begin{enumerate}
    \item Train embeddings for job-systems ESCO and AUGOV using Skip-Gram or Co-Occurence-matrix based. Do a sanity check.
    \item Train Gaussian embeddings for job-systems. Do a sanity check.
    \item Generate a small validation dataset between the European- and Australian job-system.
    \item Setup some baseline algorithms based on NLP, graph-matching, colinear-PCA for matching as a non-NLP benchmark environment. Compare against above-proposed methods.
    \item Find a normlising flow model to transform one job system into another.
\end{enumerate}


%\chapter{Background} 
\chapter{Background} 

We provide a short introduction to the background work.


\section{Gaussian Embeddings}


The above two problems can be regarded as finding an invertible transformation from one (embedding) space $\mathcal{X} \in \mathbf{R}^d$ into another $\mathcal{\hat{X}} \in \mathbf{R}^d$. 
Normalising flows \cite{variational_inference_using_normalized_flows} \cite{nvp} have proven to be a powerful tool in modeling such relations.
As such, the aim of this project is to find a model based on normalising flows which is able to find an an invertible mapping $f$ from $\mathcal{X}$ to $\mathcal{\hat{X}}$.
To keep the discussion focused, this thesis deals with the problem of finding a model for bilingual lexicon matching.

\subsubsection{Gaussian Embeddings}

\section{Normalising Flows}

(Rezende 2015) A simple distribution is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained.\\

In the above equation, the KL-divergence is the loss between the approximate posterior and the prior distribution (which acts as a regualizer). The second term, which is the expected log-likelihood is the reconstruction error.\\

Inference with normalizing flows provides a tighter, modified variational lower bound with additional terms that only add terms with linear time complexity.\\

in the asymptotic regine, this is able to recover the true posterioir distribution \\

Approximate posterior distribution of the latent variables $q_\phi(z|x)$ 
\begin{align}
\text{log} p_{\theta}(x) 
&= \text{log} \int{ p_{\theta}(x|z)p(z) dz } \\
&= \text{log} \int{ \frac{q_{\theta}(z|x)}{q_{\theta}(z|x)}p_{\theta}(x|z) p(z) dz } \\
&= \mathbb{D}_{KL} \left[ q_{\phi}(z|x) || p(z) \right] + \mathbb{E}_q \left[ \text{log} p_{\theta}(x|z) \right] \\
&\geq -\mathcal{F}(x)
\end{align}
where $-\mathbf{F}(x)$ is a free energy function, and where we used Jensen's inequality to obtain the final equation (through $log E_q\left[ \frac{p(x, z)}{q(z)} \right] \geq log E_q\left[ \text{log} \frac{p(x, z)}{q(z)} \right] $).
$p_\theta(x|z)$ is a likelihood function and $p(z)$ is a prior over latent variables, and $q_\theta(z|x)$ is a (simple) model distribution with which we want to approximate $p(x|z)$.
item We try to minimize this loss \\

To train the model, we need to efficiently (1) compute the derivatives of the expected log-likelihood  $ \nabla \phi \mathbb{E}_{q_\phi(z)}\left[ p_\theta(x|z) \right]$ and (2) choose the richest, computationally-feasible approximate posterior distribution $q(\dot)$. \\

Using Monte Carlo gradient estimation and inference networks, which (when used together), they call \textit{amortized variational inference}.
TODO: TIMO SAID THAT THIS IS WRONGLY APPROXIMATING THE TRUE DISTRIBUTION? WHAT ABOUT PSI? \\

For stochastic backpropagation, we make use of two techniques:

\begin{itemize}
\item Reparameterization: The latent variable is reparametrized in terms of a known base distribution and a differentiable transofmration.
As an example, if $q_\phi(z)$ is a Gaussian distribution $\mathbf{N}(z| \mu, \sigma^2)$ with trainable parameters $\phi = {\mu, \sigma^2}$, then we can reparametrize the variable $z$ as 

\begin{equation}
	z \sim \mathbf{N}(z | \mu, \sigma^2) \iff z = \sigma + \sigma \epsilon, \epsilon \sim \mathbf{N}(0, 1)
\end{equation}

\item Backpropagation with Monte Carlo. We backpropagated w.r.t. the parameters $\phi$ of the variational distribution using a Monte Carlo apprxoimation. 
This is our samples batch.

\begin{equation}
\nabla\phi \mathbb{E}_{q_\phi(z)}\left[ f_\theta(z) \right] 
\iff 
\nabla\phi \mathbb{E}_{\mathbf{N} (\epsilon | 0, 1)}\left[ \nabla_{\phi} f_\theta (\mu +  \sigma \epsilon) \right]
\end{equation}

where we have simply rewritten z using the reparametrization trick.

\end{itemize}

For continuous latent variables, it has the lowest variance among competing estimators [CITEEE].

Normalizing flows are most often used in the context of inference network. 
An inference network learns an inverse map from observations to latent variables.
The simplest latent variable is a Gaussian distribution. \\

Rezende 2015 propose a \textit{Deep Latent Gaussian Model}, which consists of a hierarchy of $L$ layers of Gaussian latent variables $z_l$ for layer $l$.

\begin{equation}
p(x, z_1, \ldots, z_L) = p(x | f_0(z_1)) \prod_{l=1}^{L} p(z_l | f_l(z_{l+1}))
\end{equation}

We induce priors over each latent variable $p(z_l) = \mathbf{N}(0, I)$, and the observatoin distribution $p_\theta(x|z)$ is any distribution that is conditioned on $z_1$ and is parametrized by a neural network. \\

This model is very general, and captures includes other models, such as factor analysis and PCA; non-linear factor analyiss, and non-linear Gaussian belif networks as special cases (Rezende 2014, cited in Rezende 2015) \\

We know that $\mathbf{D}_{KL}\left[ q || p\right] = 0$ is achived when $q_\phi(z|x) = p_\theta(z|x)$ (i.e. the approximate q matches the true posterior distribution). IS THIS THE ONLY TIME THIS CAN BE ACHIEVED? \\

A normalising flow captures more flexibel distributions.

Summary normalising flows:

A normalizing flow is a set of basic rules for transofmration of densiities, considers an invertible, smooth mapping

$$
f : \mathbf{R}^d \rightarrow \mathbf{R}^d
$$

where the input dimensioinality $d_0$ and output dimensionalities  $d_L$ match.
The inverse is denoted $f^{-1} = g$, i.e. the composition $g \circ f(z) = z$.
When we want to transform a random variable $z^\prime = f(z)$, we receive the following distribution:


%%\begin{equation}
%%q(z^\prime) = q(z) \left| \text{ \frac{\delta %%f^{-1}}{\delta z^\prime} } \right| = q(z) \left| det \frac{\delta f}{\delta z}^{-1} \right|
%%\end{equation}

This stems from the Identity that the probability mass must be conversed amongst operations:

\begin{equation}
p_x(x) = \frac{p_z(z) V_Z}{V_X}
\end{equation}

\begin{align}
	z^\prime &= f(z) \\
	\frac{z^\prime}{ q(z^\prime) } &= \frac{f(z)}{ q(z) }
\end{align}

WHERE DOES THE PRIME COME FROM

We can then construct arbitrarily complex densities by composing several simple maps and successively applying the above mass-preserving operation.

\begin{equation}
z_K = f_K \circ \ldots \circ f_2 \circ f_1 (z_0)
\end{equation}

We then apply the log-function to have tractable probability functions.

Probability mass must be conserved.
From the change of variable formula, we know that taking infinitesimal c:hanges toward the direction of the final probabiliity distribution, we get

\begin{align}
\int z^\prime q(z^\prime) dz^\prime &= \int f(z) q(z) dz \\
q(z^\prime) &= q(z) \text{ln} \left| det \frac{\delta f(z)}{\delta z^\prime} \right|
\end{align} \\

There are different ways to use normalising flows:

Although computing the above gradient is expensive ($O(n^3)$ complexity), we can use the Matrix determinant lemma, and construct out mapping matrices in a special way as follows
(https://blog.evjang.com/2018/01/nf1.htm)\\

\begin{align}
det(A) & = det(L + V \dot D \dot V^T) \\
		& = det(L) (1 + (VD^{\frac{1}{2}})^T (L^{-1} D^{\frac{1}{2}}V))
\end{align}

where we have used the Matrix Determiinant Lemma for the second equality, with $L$ is a lower triangular matrix, and $D$ is a diagonal matrix.


subsection{Extensions on Normalising Flows}

(https://www.shakirm.com/slides/DeepGenModelsTutorial.pdf)

Ever since Normalising flows were proposed, a number of modified versions were presented by [CITE].

Each one calculates the consecutive flow based on a different composition of the previous latent variables $z_k$.

\subsubsection{Planar Flow}

The planar flow is a simple sequential flow with invertible piecewise linear transformations.

\begin{equation}
z_k = z_{k-1} + uh (w^T z_{k-1} + b)
\end{equation}

\subsubsection{Real NVP}

The input for each step is split into two parts.
The intuition behind this is similar to a residual network, which includes logic that allows the gradient to flow back easier.

\begin{align}
y_{1:d} &= z_{k-1, 1:d} \\
y_{d+1:D} &= t(z_{k-1, 1:d}) + z_{d+1:D} \odot \text{exp}(s(z_{k-1, 1:d}))
\end{align}

Here, $t$ and $s$ are arbitrary functions, such as deep neural networks or simple linear transforms.

\subsubsection{Inverse AR Flow}

At each timestep, all prior variables are taken into consideration.

\begin{equation}
z_k = \frac{z_{k-1} - \mu_k (z_{\lq k}, x) }{\sigma_k(z_{\lq k}, x)}
\end{equation}

\subsubsection{Non-Linear Independent Components Estimation (NICE)}

Another volume-preserving flow is the NICE flow.
First, we arbitrarily partition the latent vector into two components $z = (z_A, z_B)$.

\begin{align}
f(z)  &= (z_A, z_B + h_\lambda(z_A) ) \\
g(z^\prime) &= (z^\prime_A, z^\prime_B + h_\lambda(z^\prime_A) )
\end{align}

Here, $h_\lambda$ can be chosen in such a way that $h$ is a deep neural network with parameters $\lambda$

LOL, COULD WE JUST CREATE A NEW ONE WITH MORE PROPERTIES??


Multiple

Normalising flows \cite{variational_inference_using_normalized_flows}, \cite{normalising_flows} 
are a statistical technique where a series of invertible transformations $f_t$ are applied to a simple distribution $z_0 \sim q_0(z)$, to yield increasingly complex distributions $z_t = f_t(z_{t-1})$, s.t. the last iterate $z_T$ has the desired and more flexible distribution.
As long as we can efficiently compute the Jacobian determinant of the transformation bijection $f_t$, we can both (1) evaluate the density of our data (by applying an inverse transformation and computing the density in the base distribution), and 
(2) sample from our complex distribution (by sampling from the base distribution and applying the forward transformation)
These can be used for classification and clustering \cite{normalising_flows},  [variational inference tasks \cite{iaf} such as image-generation \cite{nvp}], enriching the posterior (and prior!) \cite{variational_inference_using_normalized_flows}, and density estimation \cite{glow}.

\subsubsection{Glow: Generative Flows with Invertible 1x1 Convolutions}

We use 1x1 convolutional layers as a generalization of the permutation in sequential flows with lower triangular learnable rotation matrices.

TODO: Convolution operators and reduction to permutation operators

\begin{equation}
\text{log} \left| \text{def} \left( \frac{d \text{conv2D}(h; W) }{dh} \right) \right| = h  w  
\end{equation}

% \text{log} = \dot \text{log}  \text{log}  = \dot \text{log} |\text{det} (W) |


(read the paper a bit more before citing this, but cite this: https://arxiv.org/pdf/1901.08624.pdf) 


\subsubsection{Flow++}



The coupling layers can be described as

\begin{align}
y_1 &= x_1 
y_2 &= x_2 \dot exp( a_\theta( x_1 ) ) + b_\theta(x_1)
\end{align}

where the functions $a_\theta$ and $b_\theta$ are learnable functions (possibly normalizing flows which ). 
These have to be invertible affine transformations




%\chapter{Related Work} 
\chapter{Related Work}

\section{Unsupervised Biliingual Lexicon Matching}

%\chapter{Design and Implementation}
\chapter{Analysis of the current state of the art}

%\chapter{Evaluation} 
\chapter{Our Method}
\
%\chapter{Summary and Conclusions} 
\chapter{Evaluation}

\chapter{Conclusion}

\appendix
\singlespacing

\bibliographystyle{unsrt} 
%\bibliography{dissertation} 

\end{document}
