\contentsline {chapter}{\numberline {1}Introduction}{3}% 
\contentsline {subsection}{\numberline {1.0.1}Main Contributions}{4}% 
\contentsline {chapter}{\numberline {2}Background}{5}% 
\contentsline {section}{\numberline {2.1}Linguistic Features}{6}% 
\contentsline {paragraph}{Polysemy}{6}% 
\contentsline {paragraph}{Part of Speech \textit {PoS}}{6}% 
\contentsline {paragraph}{Other linguistic features}{7}% 
\contentsline {subsection}{\numberline {2.1.1}Tokens and n-grams}{7}% 
\contentsline {paragraph}{Character}{8}% 
\contentsline {paragraph}{Byte pair encodings}{8}% 
\contentsline {paragraph}{WordPiece tokenizer}{9}% 
\contentsline {section}{\numberline {2.2}Word Embeddings}{9}% 
\contentsline {paragraph}{Word-Embeddings}{9}% 
\contentsline {paragraph}{Distance:}{10}% 
\contentsline {paragraph}{Learning a distance}{10}% 
\contentsline {paragraph}{Exploiting the distributional structure of language:}{11}% 
\contentsline {paragraph}{Loss Objective}{12}% 
\contentsline {subsection}{\numberline {2.2.1}Static Word Embeddings}{14}% 
\contentsline {subsubsection}{Basic Model}{14}% 
\contentsline {subsubsection}{Word2Vec}{15}% 
\contentsline {subsubsection}{GloVe}{16}% 
\contentsline {subsubsection}{Gaussian Embeddings}{17}% 
\contentsline {subsection}{\numberline {2.2.2}Context Embeddings}{21}% 
\contentsline {subsubsection}{ELMo}{22}% 
\contentsline {subsubsection}{The Transformer Architecture}{24}% 
\contentsline {subsubsection}{BERT}{26}% 
\contentsline {subsubsection}{Other language models}{29}% 
\contentsline {section}{\numberline {2.3}GLUE benchmark dataset}{30}% 
\contentsline {subsubsection}{Single Sentence Tasks}{31}% 
\contentsline {subsubsection}{Similarity and paraphrase tasks}{32}% 
\contentsline {subsubsection}{Inference Tasks}{33}% 
\contentsline {section}{\numberline {2.4}WordNet}{35}% 
\contentsline {paragraph}{Lexical matrix:}{35}% 
\contentsline {subsection}{\numberline {2.4.1}SemCor dataset}{38}% 
\contentsline {chapter}{\numberline {3}Related Work}{41}% 
\contentsline {section}{\numberline {3.1}Clustering for Semantics}{41}% 
\contentsline {paragraph}{Affinity Propagation}{42}% 
\contentsline {paragraph}{DBScan}{42}% 
\contentsline {paragraph}{MeanShift}{42}% 
\contentsline {paragraph}{Chinese Whispers}{42}% 
\contentsline {section}{\numberline {3.2}Structure inside BERT}{44}% 
\contentsline {subsection}{\numberline {3.2.1}Static word embeddings from context embeddings}{50}% 
\contentsline {subsection}{\numberline {3.2.2}Other methods}{59}% 
\contentsline {subsubsection}{Generating "static" word-embeddings through contextual embeddings}{59}% 
\contentsline {subsection}{\numberline {3.2.3}From token-vectors to word-vectors}{61}% 
\contentsline {subsection}{\numberline {3.2.4}Bias in BERT vectors}{61}% 
\contentsline {subsection}{\numberline {3.2.5}Change of meanings over time}{61}% 
\contentsline {subsection}{\numberline {3.2.6}Clinical concept extration}{62}% 
\contentsline {subsection}{\numberline {3.2.7}Discovering for semantics}{62}% 
\contentsline {subsection}{\numberline {3.2.8}Embeddings for translation}{62}% 
\contentsline {chapter}{\numberline {4}Understanding the semantic subspace organization in BERT}{65}% 
\contentsline {section}{\numberline {4.1}On the Linear Separability of meaning within sampled BERT vectors}{65}% 
\contentsline {subsection}{\numberline {4.1.1}Experiment setup}{66}% 
\contentsline {subsection}{\numberline {4.1.2}Results}{68}% 
\contentsline {section}{\numberline {4.2}On the Clusterability of meaning within sampled BERT vectors}{70}% 
\contentsline {subsection}{\numberline {4.2.1}Experiment setup}{71}% 
\contentsline {subsection}{\numberline {4.2.2}Results}{75}% 
\contentsline {subsubsection}{Qualitative evaluation}{77}% 
\contentsline {section}{\numberline {4.3}Correlation between Part of Speech and Context within BERT}{80}% 
\contentsline {subsection}{\numberline {4.3.1}Experiment setup}{81}% 
\contentsline {subsection}{\numberline {4.3.2}Results}{83}% 
\contentsline {subsubsection}{Quantitative Evaluation}{83}% 
\contentsline {subsubsection}{Qualitative Evaluation}{83}% 
\contentsline {chapter}{\numberline {5}Exploiting subspace organization of semantics of BERT embeddings}{87}% 
\contentsline {section}{\numberline {5.1}BERnie: Making the variances across BERT embeddings more equal}{89}% 
\contentsline {subsection}{\numberline {5.1.1}BERnie PoS}{89}% 
\contentsline {subsubsection}{Experiment setup}{89}% 
\contentsline {subsection}{\numberline {5.1.2}BERnie Meaning}{92}% 
\contentsline {subsubsection}{Experiment setup}{92}% 
\contentsline {subsubsection}{Results}{94}% 
\contentsline {subsection}{\numberline {5.1.3}BERnie Meaning with additional pre-training}{96}% 
\contentsline {subsection}{\numberline {5.1.4}Experiment setup}{96}% 
\contentsline {subsection}{\numberline {5.1.5}Results}{96}% 
\contentsline {chapter}{\numberline {6}Conclusion}{99}% 
\contentsline {chapter}{\numberline {A}Other lines of work}{111}% 
\contentsline {section}{\numberline {A.1}Metric Learning and Disentanglement}{111}% 
\contentsline {subsubsection}{Non-Deep Metric Learning}{112}% 
\contentsline {paragraph}{Dimensionality Reduction}{113}% 
\contentsline {subsubsection}{Deep Metric Learning}{114}% 
\contentsline {section}{\numberline {A.2}Compressing the non-lexical out}{116}% 
\contentsline {subsubsection}{Motivation}{116}% 
\contentsline {subsubsection}{Experiment setup}{116}% 
\contentsline {subsubsection}{Results}{116}% 
\contentsline {chapter}{\numberline {B}More Results}{117}% 
\contentsline {section}{\numberline {B.1}Finding the best clustering model}{117}% 
\contentsline {subsubsection}{Quantiative Evaluation}{117}% 
\contentsline {subsubsection}{Qualitative Evaluation}{118}% 
\contentsline {section}{\numberline {B.2}Frozen Verbs}{121}% 
\contentsline {chapter}{\numberline {C}Using word vectors in translation}{123}% 
\contentsline {subsubsection}{Word2Vec}{125}% 
