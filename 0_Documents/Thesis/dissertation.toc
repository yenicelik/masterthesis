\contentsline {chapter}{\numberline {1}Introduction}{3}% 
\contentsline {subsection}{\numberline {1.0.1}Main Contributions}{4}% 
\contentsline {chapter}{\numberline {2}Background}{5}% 
\contentsline {section}{\numberline {2.1}Linguistic Features}{6}% 
\contentsline {paragraph}{Polysemy}{6}% 
\contentsline {paragraph}{Part of Speech \textit {PoS}}{6}% 
\contentsline {paragraph}{Other linguistic features}{7}% 
\contentsline {subsection}{\numberline {2.1.1}Tokens and n-grams}{7}% 
\contentsline {paragraph}{Character}{8}% 
\contentsline {paragraph}{Byte pair encodings}{8}% 
\contentsline {paragraph}{WordPiece tokenizer}{9}% 
\contentsline {section}{\numberline {2.2}Word Embeddings}{9}% 
\contentsline {paragraph}{Word-Embeddings}{9}% 
\contentsline {paragraph}{Distance:}{10}% 
\contentsline {paragraph}{Learning a distance}{10}% 
\contentsline {paragraph}{Exploiting the distributional structure of language:}{11}% 
\contentsline {paragraph}{Loss Objective}{12}% 
\contentsline {subsection}{\numberline {2.2.1}Static Word Embeddings}{15}% 
\contentsline {subsubsection}{Basic Model}{15}% 
\contentsline {subsubsection}{Word2Vec}{16}% 
\contentsline {subsubsection}{GloVe}{17}% 
\contentsline {subsubsection}{Gaussian Embeddings}{18}% 
\contentsline {subsection}{\numberline {2.2.2}Context Embeddings}{22}% 
\contentsline {subsubsection}{ELMo}{23}% 
\contentsline {subsubsection}{The Transformer Architecture}{25}% 
\contentsline {subsubsection}{BERT}{27}% 
\contentsline {subsubsection}{GPT and GPT-2}{30}% 
\contentsline {paragraph}{Unsupervised pre-training}{30}% 
\contentsline {paragraph}{Supervised fine-tuning}{31}% 
\contentsline {subsubsection}{GLUE benchmark dataset}{32}% 
\contentsline {subsubsection}{Single Sentence Tasks}{33}% 
\contentsline {subsubsection}{Similarity and paraphrase tasks}{33}% 
\contentsline {subsubsection}{Inference Tasks}{34}% 
\contentsline {section}{\numberline {2.3}Metric Learning and Disentanglement}{36}% 
\contentsline {subsubsection}{Non-Deep Metric Learning}{38}% 
\contentsline {paragraph}{Dimensionality Reduction}{39}% 
\contentsline {subsubsection}{Deep Metric Learning}{40}% 
\contentsline {section}{\numberline {2.4}Clustering Algorithms}{42}% 
\contentsline {paragraph}{Affinity Propagation}{42}% 
\contentsline {paragraph}{Chinese Whispers}{42}% 
\contentsline {paragraph}{DBScan}{42}% 
\contentsline {paragraph}{HDBScan}{42}% 
\contentsline {paragraph}{MeanShift}{43}% 
\contentsline {paragraph}{Optics}{43}% 
\contentsline {chapter}{\numberline {3}Related Work}{45}% 
\contentsline {section}{\numberline {3.1}Structure inside BERT}{45}% 
\contentsline {subsection}{\numberline {3.1.1}WordNet}{47}% 
\contentsline {paragraph}{Lexical matrix:}{47}% 
\contentsline {subsection}{\numberline {3.1.2}SemCor dataset}{50}% 
\contentsline {subsection}{\numberline {3.1.3}Other methods}{51}% 
\contentsline {subsubsection}{Generating "static" word-embeddings through contextual embeddings}{52}% 
\contentsline {subsection}{\numberline {3.1.4}Attention mechanism}{54}% 
\contentsline {subsection}{\numberline {3.1.5}From token-vectors to word-vectors}{54}% 
\contentsline {subsection}{\numberline {3.1.6}Bias in BERT vectors}{55}% 
\contentsline {subsection}{\numberline {3.1.7}Change of meanings over time}{55}% 
\contentsline {subsection}{\numberline {3.1.8}Clinical concept extration}{55}% 
\contentsline {subsection}{\numberline {3.1.9}Discovering for semantics}{55}% 
\contentsline {subsection}{\numberline {3.1.10}Embeddings for translation}{55}% 
\contentsline {chapter}{\numberline {4}Analysing the current state of the art}{57}% 
\contentsline {section}{\numberline {4.1}On the Linear Separability of meaning within sampled BERT vectors}{57}% 
\contentsline {subsection}{\numberline {4.1.1}Motivation}{57}% 
\contentsline {subsection}{\numberline {4.1.2}Experiment setup}{57}% 
\contentsline {subsection}{\numberline {4.1.3}Results}{59}% 
\contentsline {section}{\numberline {4.2}On the Clusterability of meaning within sampled BERT vectors}{61}% 
\contentsline {subsection}{\numberline {4.2.1}Motivation}{61}% 
\contentsline {subsection}{\numberline {4.2.2}Experiment setup}{61}% 
\contentsline {subsection}{\numberline {4.2.3}Results}{64}% 
\contentsline {subsubsection}{Qualitative evaluation}{65}% 
\contentsline {section}{\numberline {4.3}Correlation between Part of Speech and Context within BERT}{75}% 
\contentsline {subsection}{\numberline {4.3.1}Motivation}{75}% 
\contentsline {subsection}{\numberline {4.3.2}Experiment setup}{75}% 
\contentsline {subsection}{\numberline {4.3.3}Results}{75}% 
\contentsline {chapter}{\numberline {5}Exploiting subspace organization of semantics of BERT embeddings}{77}% 
\contentsline {subsection}{\numberline {5.0.1}BERnie PoS}{78}% 
\contentsline {subsubsection}{Motivation}{78}% 
\contentsline {subsubsection}{Experiment setup}{78}% 
\contentsline {subsection}{\numberline {5.0.2}BERnie Meaning}{79}% 
\contentsline {subsubsection}{Motivation}{79}% 
\contentsline {subsubsection}{Experiment setup}{80}% 
\contentsline {subsubsection}{Results}{82}% 
\contentsline {subsection}{\numberline {5.0.3}BERnie Meaning with additional pre-training}{84}% 
\contentsline {subsubsection}{Motivation}{84}% 
\contentsline {subsubsection}{Experiment setup}{84}% 
\contentsline {section}{\numberline {5.1}Compressing the non-lexical out}{85}% 
\contentsline {subsubsection}{Motivation}{85}% 
\contentsline {subsubsection}{Experiment setup}{85}% 
\contentsline {subsubsection}{Results}{86}% 
\contentsline {chapter}{\numberline {6}Conclusion}{87}% 
\contentsline {chapter}{\numberline {A}Using word vectors in translation}{95}% 
\contentsline {subsubsection}{Word2Vec}{97}% 
