\contentsline {chapter}{\numberline {1}Introduction}{3}% 
\contentsline {subsection}{\numberline {1.0.1}Main Contributions}{4}% 
\contentsline {chapter}{\numberline {2}Background}{5}% 
\contentsline {section}{\numberline {2.1}Linguistic Features}{6}% 
\contentsline {paragraph}{Polysemy}{6}% 
\contentsline {paragraph}{Part of Speech \textit {PoS}}{6}% 
\contentsline {paragraph}{Other linguistic features}{7}% 
\contentsline {subsection}{\numberline {2.1.1}Tokens and n-grams}{7}% 
\contentsline {paragraph}{Character}{8}% 
\contentsline {paragraph}{Byte pair encodings}{8}% 
\contentsline {paragraph}{WordPiece tokenizer}{9}% 
\contentsline {section}{\numberline {2.2}Word Embeddings}{9}% 
\contentsline {paragraph}{Word-Embeddings}{9}% 
\contentsline {paragraph}{Distance:}{10}% 
\contentsline {paragraph}{Learning a distance}{10}% 
\contentsline {paragraph}{Exploiting the distributional structure of language:}{11}% 
\contentsline {paragraph}{Loss Objective}{12}% 
\contentsline {subsection}{\numberline {2.2.1}Static Word Embeddings}{14}% 
\contentsline {subsubsection}{Basic Model}{14}% 
\contentsline {subsubsection}{Word2Vec}{15}% 
\contentsline {subsubsection}{GloVe}{16}% 
\contentsline {subsubsection}{Gaussian Embeddings}{17}% 
\contentsline {subsection}{\numberline {2.2.2}Context Embeddings}{21}% 
\contentsline {subsubsection}{ELMo}{22}% 
\contentsline {subsubsection}{The Transformer Architecture}{24}% 
\contentsline {subsubsection}{BERT}{26}% 
\contentsline {subsubsection}{GPT and GPT-2}{29}% 
\contentsline {paragraph}{Unsupervised pre-training}{29}% 
\contentsline {paragraph}{Supervised fine-tuning}{30}% 
\contentsline {section}{\numberline {2.3}GLUE benchmark dataset}{31}% 
\contentsline {subsubsection}{Single Sentence Tasks}{32}% 
\contentsline {subsubsection}{Similarity and paraphrase tasks}{32}% 
\contentsline {subsubsection}{Inference Tasks}{33}% 
\contentsline {section}{\numberline {2.4}WordNet}{35}% 
\contentsline {paragraph}{Lexical matrix:}{36}% 
\contentsline {subsection}{\numberline {2.4.1}SemCor dataset}{38}% 
\contentsline {section}{\numberline {2.5}Metric Learning and Disentanglement}{39}% 
\contentsline {subsubsection}{Non-Deep Metric Learning}{41}% 
\contentsline {paragraph}{Dimensionality Reduction}{43}% 
\contentsline {subsubsection}{Deep Metric Learning}{43}% 
\contentsline {chapter}{\numberline {3}Related Work}{47}% 
\contentsline {section}{\numberline {3.1}Clustering for Semantics}{47}% 
\contentsline {paragraph}{Affinity Propagation}{48}% 
\contentsline {paragraph}{DBScan}{48}% 
\contentsline {paragraph}{MeanShift}{48}% 
\contentsline {paragraph}{The Chinese Whispers}{48}% 
\contentsline {section}{\numberline {3.2}Structure inside BERT}{49}% 
\contentsline {subsection}{\numberline {3.2.1}Other methods}{50}% 
\contentsline {subsubsection}{Generating "static" word-embeddings through contextual embeddings}{50}% 
\contentsline {subsection}{\numberline {3.2.2}Attention mechanism}{53}% 
\contentsline {subsection}{\numberline {3.2.3}From token-vectors to word-vectors}{53}% 
\contentsline {subsection}{\numberline {3.2.4}Bias in BERT vectors}{54}% 
\contentsline {subsection}{\numberline {3.2.5}Change of meanings over time}{54}% 
\contentsline {subsection}{\numberline {3.2.6}Clinical concept extration}{54}% 
\contentsline {subsection}{\numberline {3.2.7}Discovering for semantics}{54}% 
\contentsline {subsection}{\numberline {3.2.8}Embeddings for translation}{54}% 
\contentsline {chapter}{\numberline {4}Understanding the semantic subspace organization in BERT}{57}% 
\contentsline {section}{\numberline {4.1}On the Linear Separability of meaning within sampled BERT vectors}{58}% 
\contentsline {subsection}{\numberline {4.1.1}Experiment setup}{58}% 
\contentsline {subsection}{\numberline {4.1.2}Results}{60}% 
\contentsline {section}{\numberline {4.2}On the Clusterability of meaning within sampled BERT vectors}{63}% 
\contentsline {subsection}{\numberline {4.2.1}Experiment setup}{64}% 
\contentsline {subsection}{\numberline {4.2.2}Results}{67}% 
\contentsline {subsubsection}{Qualitative evaluation}{70}% 
\contentsline {section}{\numberline {4.3}Correlation between Part of Speech and Context within BERT}{78}% 
\contentsline {subsection}{\numberline {4.3.1}Motivation}{78}% 
\contentsline {subsection}{\numberline {4.3.2}Experiment setup}{78}% 
\contentsline {subsection}{\numberline {4.3.3}Results}{79}% 
\contentsline {chapter}{\numberline {5}Exploiting subspace organization of semantics of BERT embeddings}{81}% 
\contentsline {subsection}{\numberline {5.0.1}BERnie PoS}{82}% 
\contentsline {subsubsection}{Motivation}{82}% 
\contentsline {subsubsection}{Experiment setup}{82}% 
\contentsline {subsection}{\numberline {5.0.2}BERnie Meaning}{83}% 
\contentsline {subsubsection}{Motivation}{83}% 
\contentsline {subsubsection}{Experiment setup}{84}% 
\contentsline {subsubsection}{Results}{86}% 
\contentsline {subsection}{\numberline {5.0.3}BERnie Meaning with additional pre-training}{87}% 
\contentsline {subsubsection}{Motivation}{87}% 
\contentsline {subsubsection}{Experiment setup}{87}% 
\contentsline {section}{\numberline {5.1}Compressing the non-lexical out}{89}% 
\contentsline {subsubsection}{Motivation}{89}% 
\contentsline {subsubsection}{Experiment setup}{89}% 
\contentsline {subsubsection}{Results}{89}% 
\contentsline {chapter}{\numberline {6}Conclusion}{91}% 
\contentsline {chapter}{\numberline {A}More Results}{99}% 
\contentsline {section}{\numberline {A.1}Finding the best clustering model}{99}% 
\contentsline {chapter}{\numberline {B}Using word vectors in translation}{101}% 
\contentsline {subsubsection}{Word2Vec}{103}% 
