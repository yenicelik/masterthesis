\contentsline {chapter}{\numberline {1}Introduction}{3}% 
\contentsline {subsection}{\numberline {1.0.1}Main Contributions}{4}% 
\contentsline {chapter}{\numberline {2}Background}{7}% 
\contentsline {section}{\numberline {2.1}Linguistic Features}{8}% 
\contentsline {paragraph}{Polysemy}{8}% 
\contentsline {paragraph}{Part of Speech \textit {PoS}}{8}% 
\contentsline {subsection}{\numberline {2.1.1}Tokens and n-grams}{9}% 
\contentsline {paragraph}{Character}{9}% 
\contentsline {paragraph}{Byte pair encodings}{10}% 
\contentsline {paragraph}{WordPiece tokenizer}{10}% 
\contentsline {section}{\numberline {2.2}Word Embeddings}{11}% 
\contentsline {paragraph}{Word-Embeddings}{11}% 
\contentsline {paragraph}{Distance:}{11}% 
\contentsline {paragraph}{Learning a distance}{12}% 
\contentsline {paragraph}{Exploiting the distributional structure of language:}{13}% 
\contentsline {paragraph}{Loss Objective}{14}% 
\contentsline {subsection}{\numberline {2.2.1}Static Word Embeddings}{16}% 
\contentsline {subsubsection}{Basic Model}{16}% 
\contentsline {subsubsection}{Word2Vec}{17}% 
\contentsline {subsubsection}{GloVe}{18}% 
\contentsline {subsubsection}{Gaussian Embeddings}{19}% 
\contentsline {subsection}{\numberline {2.2.2}Context Embeddings}{23}% 
\contentsline {subsubsection}{ELMo}{24}% 
\contentsline {subsubsection}{The Transformer Architecture}{26}% 
\contentsline {subsubsection}{BERT}{28}% 
\contentsline {subsubsection}{Other language models}{31}% 
\contentsline {section}{\numberline {2.3}GLUE benchmark dataset}{32}% 
\contentsline {subsubsection}{Single Sentence Tasks}{33}% 
\contentsline {subsubsection}{Similarity and paraphrase tasks}{34}% 
\contentsline {subsubsection}{Inference Tasks}{35}% 
\contentsline {section}{\numberline {2.4}WordNet}{37}% 
\contentsline {paragraph}{Lexical matrix:}{37}% 
\contentsline {subsection}{\numberline {2.4.1}SemCor dataset}{40}% 
\contentsline {chapter}{\numberline {3}Related Work}{43}% 
\contentsline {section}{\numberline {3.1}Clustering for Semantics}{43}% 
\contentsline {paragraph}{Affinity Propagation}{44}% 
\contentsline {paragraph}{DBScan}{44}% 
\contentsline {paragraph}{MeanShift}{44}% 
\contentsline {paragraph}{Chinese Whispers}{44}% 
\contentsline {section}{\numberline {3.2}Structure inside BERT}{46}% 
\contentsline {subsection}{\numberline {3.2.1}Static word embeddings from context embeddings}{52}% 
\contentsline {subsection}{\numberline {3.2.2}Other methods}{60}% 
\contentsline {subsection}{\numberline {3.2.3}Bias in BERT vectors}{60}% 
\contentsline {subsection}{\numberline {3.2.4}Change of meanings over time}{61}% 
\contentsline {subsection}{\numberline {3.2.5}Clinical concept extration}{61}% 
\contentsline {subsection}{\numberline {3.2.6}Discovering for semantics}{61}% 
\contentsline {subsection}{\numberline {3.2.7}Embeddings for translation}{61}% 
\contentsline {chapter}{\numberline {4}Understanding the semantic subspace organization in BERT}{63}% 
\contentsline {section}{\numberline {4.1}On the Linear Separability of meaning within sampled BERT vectors}{63}% 
\contentsline {subsection}{\numberline {4.1.1}Experiment setup}{64}% 
\contentsline {subsection}{\numberline {4.1.2}Results}{66}% 
\contentsline {section}{\numberline {4.2}On the Clusterability of meaning within sampled BERT vectors}{68}% 
\contentsline {subsection}{\numberline {4.2.1}Experiment setup}{69}% 
\contentsline {subsection}{\numberline {4.2.2}Results}{73}% 
\contentsline {subsubsection}{Qualitative evaluation}{75}% 
\contentsline {section}{\numberline {4.3}Correlation between Part of Speech and Context within BERT}{78}% 
\contentsline {subsection}{\numberline {4.3.1}Experiment setup}{79}% 
\contentsline {subsection}{\numberline {4.3.2}Results}{81}% 
\contentsline {subsubsection}{Quantitative Evaluation}{81}% 
\contentsline {subsubsection}{Qualitative Evaluation}{81}% 
\contentsline {chapter}{\numberline {5}Exploiting subspace organization of semantics of BERT embeddings}{85}% 
\contentsline {section}{\numberline {5.1}BERnie: Making the variances across BERT embeddings more equal}{87}% 
\contentsline {subsection}{\numberline {5.1.1}BERnie PoS}{87}% 
\contentsline {subsubsection}{Experiment setup}{87}% 
\contentsline {subsection}{\numberline {5.1.2}BERnie Meaning}{90}% 
\contentsline {subsubsection}{Experiment setup}{90}% 
\contentsline {subsubsection}{Results}{92}% 
\contentsline {subsection}{\numberline {5.1.3}BERnie Meaning with additional pre-training}{94}% 
\contentsline {subsection}{\numberline {5.1.4}Experiment setup}{94}% 
\contentsline {subsection}{\numberline {5.1.5}Results}{94}% 
\contentsline {chapter}{\numberline {6}Conclusion}{97}% 
\contentsline {chapter}{\numberline {A}Background}{109}% 
\contentsline {section}{\numberline {A.1}Linguistic features}{109}% 
\contentsline {paragraph}{Other linguistic features}{109}% 
\contentsline {chapter}{\numberline {B}Other lines of work}{111}% 
\contentsline {section}{\numberline {B.1}Metric Learning and Disentanglement}{111}% 
\contentsline {subsubsection}{Non-Deep Metric Learning}{112}% 
\contentsline {paragraph}{Dimensionality Reduction}{113}% 
\contentsline {subsubsection}{Deep Metric Learning}{114}% 
\contentsline {section}{\numberline {B.2}Compressing the non-lexical out}{116}% 
\contentsline {subsubsection}{Motivation}{116}% 
\contentsline {subsubsection}{Experiment setup}{116}% 
\contentsline {subsubsection}{Results}{116}% 
\contentsline {chapter}{\numberline {C}More Results}{117}% 
\contentsline {section}{\numberline {C.1}Finding the best clustering model}{117}% 
\contentsline {subsubsection}{Quantiative Evaluation}{117}% 
\contentsline {subsubsection}{Qualitative Evaluation}{118}% 
\contentsline {section}{\numberline {C.2}Frozen Verbs}{121}% 
\contentsline {chapter}{\numberline {D}Using word vectors in translation}{123}% 
\contentsline {subsubsection}{Word2Vec}{125}% 
