\contentsline {chapter}{\numberline {1}Introduction}{1}% 
\contentsline {chapter}{\numberline {2}Motivation}{3}% 
\contentsline {chapter}{\numberline {3}Background}{5}% 
\contentsline {section}{\numberline {3.1}Linguistic Features}{6}% 
\contentsline {paragraph}{Lexical features}{6}% 
\contentsline {paragraph}{}{7}% 
\contentsline {section}{\numberline {3.2}Word Embeddings}{7}% 
\contentsline {paragraph}{Word-Embeddings}{7}% 
\contentsline {paragraph}{Distance}{8}% 
\contentsline {paragraph}{Learning a distance}{8}% 
\contentsline {paragraph}{Going from the distributional structure of sentences to learning distances between words.}{9}% 
\contentsline {paragraph}{However, we do not need to only look at only the previous words}{10}% 
\contentsline {subsection}{\numberline {3.2.1}Static Word Embeddings}{13}% 
\contentsline {subsubsection}{Basic Model}{13}% 
\contentsline {subsubsection}{Word2Vec}{13}% 
\contentsline {subsubsection}{GloVe}{15}% 
\contentsline {subsubsection}{Gaussian Embeddings}{15}% 
\contentsline {paragraph}{Symmetric similarity: expected likelihood or probability product kernel}{16}% 
\contentsline {paragraph}{Asymmetric divergence: KL-Divergence}{17}% 
\contentsline {paragraph}{Uncertainty calculation:}{17}% 
\contentsline {subsection}{\numberline {3.2.2}Context Embeddings}{19}% 
\contentsline {subsubsection}{ELMo}{20}% 
\contentsline {subsubsection}{The Transformer Architecture}{21}% 
\contentsline {subsubsection}{BERT}{21}% 
\contentsline {subsubsection}{GPT and GPT-2}{23}% 
\contentsline {paragraph}{Unsupervised pre-training}{23}% 
\contentsline {paragraph}{Supervised fine-tuning}{24}% 
\contentsline {subsection}{\numberline {3.2.3}Other methods}{25}% 
\contentsline {subsubsection}{Generating "static" word-embeddings through contextual embeddings}{25}% 
\contentsline {section}{\numberline {3.3}Resources and Datasets}{28}% 
\contentsline {subsubsection}{WordNet}{28}% 
\contentsline {paragraph}{Lexical matrix:}{28}% 
\contentsline {subsubsection}{SemCor dataset}{31}% 
\contentsline {subsubsection}{News dataset}{32}% 
\contentsline {subsubsection}{GLUE benchmark dataset}{32}% 
\contentsline {subsubsection}{Single Sentence Tasks}{33}% 
\contentsline {subsubsection}{Similarity and paraphrase tasks}{34}% 
\contentsline {subsubsection}{Inference Tasks}{35}% 
\contentsline {chapter}{\numberline {4}Related Work}{37}% 
\contentsline {section}{\numberline {4.1}Structure inside BERT}{37}% 
\contentsline {subsection}{\numberline {4.1.1}Attention mechanism}{39}% 
\contentsline {subsection}{\numberline {4.1.2}From token-vectors to word-vectors}{39}% 
\contentsline {subsection}{\numberline {4.1.3}Bias in BERT vectors}{39}% 
\contentsline {subsection}{\numberline {4.1.4}Change of meanings over time}{39}% 
\contentsline {subsection}{\numberline {4.1.5}Clinical concept extration}{40}% 
\contentsline {subsection}{\numberline {4.1.6}Discovering for semantics}{40}% 
\contentsline {subsection}{\numberline {4.1.7}Embeddings for translation}{40}% 
\contentsline {section}{\numberline {4.2}Metric Learning and Disentanglement}{41}% 
\contentsline {section}{\numberline {4.3}Clustering Algorithms}{41}% 
\contentsline {section}{\numberline {4.4}Applications of word vector}{42}% 
\contentsline {subsubsection}{Word2Vec}{45}% 
\contentsline {chapter}{\numberline {5}Analysing the current state of the art}{47}% 
\contentsline {section}{\numberline {5.1}On the Linear Separability of meaning within sampled BERT vectors}{47}% 
\contentsline {subsection}{\numberline {5.1.1}Motivation}{47}% 
\contentsline {subsection}{\numberline {5.1.2}Experiment setup}{47}% 
\contentsline {subsection}{\numberline {5.1.3}Results}{49}% 
\contentsline {section}{\numberline {5.2}On the Clusterability of meaning within sampled BERT vectors}{51}% 
\contentsline {subsection}{\numberline {5.2.1}Motivation}{51}% 
\contentsline {subsection}{\numberline {5.2.2}Experiment setup}{51}% 
\contentsline {subsection}{\numberline {5.2.3}Results}{54}% 
\contentsline {subsubsection}{Qualitative evaluation}{55}% 
\contentsline {section}{\numberline {5.3}Correlation between Part of Speech and Context within BERT}{65}% 
\contentsline {subsection}{\numberline {5.3.1}Motivation}{65}% 
\contentsline {subsection}{\numberline {5.3.2}Experiment setup}{65}% 
\contentsline {subsection}{\numberline {5.3.3}Results}{65}% 
\contentsline {chapter}{\numberline {6}Exploiting subspace organization of semantics of BERT embeddings}{67}% 
\contentsline {subsection}{\numberline {6.0.1}BERnie PoS}{68}% 
\contentsline {subsubsection}{Motivation}{68}% 
\contentsline {subsubsection}{Experiment setup}{68}% 
\contentsline {subsection}{\numberline {6.0.2}BERnie Meaning}{69}% 
\contentsline {subsubsection}{Motivation}{69}% 
\contentsline {subsubsection}{Experiment setup}{70}% 
\contentsline {subsubsection}{Results}{72}% 
\contentsline {subsection}{\numberline {6.0.3}BERnie Meaning with additional pre-training}{74}% 
\contentsline {subsubsection}{Motivation}{74}% 
\contentsline {subsubsection}{Experiment setup}{74}% 
\contentsline {section}{\numberline {6.1}Compressing the non-lexical out}{75}% 
\contentsline {subsubsection}{Motivation}{75}% 
\contentsline {subsubsection}{Experiment setup}{75}% 
\contentsline {subsubsection}{Results}{75}% 
\contentsline {chapter}{\numberline {7}Conclusion}{77}% 
