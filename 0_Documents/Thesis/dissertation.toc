\contentsline {chapter}{\numberline {1}Introduction}{1}% 
\contentsline {section}{\numberline {1.1}Scope of Work}{1}% 
\contentsline {chapter}{\numberline {2}Background}{5}% 
\contentsline {section}{\numberline {2.1}Word Embeddings}{5}% 
\contentsline {section}{\numberline {2.2}Generating "static" word-embeddings through contextual embeddings}{6}% 
\contentsline {section}{\numberline {2.3}Gaussian Embeddings}{7}% 
\contentsline {subsubsection}{Word representations via Gaussian Embeddings}{8}% 
\contentsline {paragraph}{Symmetric similarity: expected likelihood or probability product kernel}{9}% 
\contentsline {paragraph}{Asymmetric divergence: KL-Divergence}{9}% 
\contentsline {paragraph}{Uncertainty calculation:}{10}% 
\contentsline {section}{\numberline {2.4}General random information}{10}% 
\contentsline {section}{\numberline {2.5}Normalising Flows}{12}% 
\contentsline {subsubsection}{Planar Flow}{18}% 
\contentsline {subsubsection}{Real NVP (2017)}{19}% 
\contentsline {subsubsection}{Inverse AR Flow (2017)}{19}% 
\contentsline {subsubsection}{Non-Linear Independent Components Estimation (NICE)}{19}% 
\contentsline {subsubsection}{Glow: Generative Flows with Invertible 1x1 Convolutions (2018)}{20}% 
\contentsline {subsubsection}{Flow++ (2019)}{21}% 
\contentsline {chapter}{\numberline {3}Related Work}{23}% 
\contentsline {paragraph}{Pseudo-Cross-Label}{25}% 
\contentsline {paragraph}{online data}{26}% 
\contentsline {paragraph}{offline data}{26}% 
\contentsline {section}{\numberline {3.1}Discrete Methods }{26}% 
\contentsline {subsection}{\numberline {3.1.1}Loss in Translation}{26}% 
\contentsline {subsection}{\numberline {3.1.2}MUSE Facebook using a Min-Max objective}{27}% 
\contentsline {section}{\numberline {3.2}Methods using Normalising Flows}{30}% 
\contentsline {subsection}{\numberline {3.2.1}Density Matching for Bilingual Word Embeddings (Zhou 2019)}{30}% 
\contentsline {section}{\numberline {3.3}Methods viewing this problem as an Optimal Transport (OT) Problem}{33}% 
\contentsline {subsection}{\numberline {3.3.1}Gromov-Wasserstein Alignment of Word Embedding Spaces}{33}% 
\contentsline {subsubsection}{Supervised Case: Procrustes}{34}% 
\contentsline {subsubsection}{Unsupervised Maps: Optimal Transport}{35}% 
\contentsline {subsection}{\numberline {3.3.2}Graph translation using normalizing flows}{36}% 
\contentsline {subsection}{\numberline {3.3.3}Using monolingual word-embeddings to map one to another}{37}% 
\contentsline {subsubsection}{Cross-lingual word-mappings (Artetxe 2018)}{37}% 
\contentsline {subsubsection}{Bilingual Lexicon Induction through Unsupervised Machine Translation (Artetxe 2019)}{38}% 
\contentsline {section}{\numberline {3.4}Gaussian Embeddings and Token matching in other applications}{38}% 
\contentsline {subsection}{\numberline {3.4.1}Creating Gaussian Embeddings to represent Graphs}{38}% 
\contentsline {chapter}{\numberline {4}Analysis of the current state of the art}{39}% 
\contentsline {chapter}{\numberline {5}Our Method}{41}% 
\contentsline {chapter}{\numberline {6}Further Work}{43}% 
\contentsline {chapter}{\numberline {7}Evaluation}{45}% 
\contentsline {chapter}{\numberline {8}Conclusion}{47}% 
