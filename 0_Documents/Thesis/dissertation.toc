\contentsline {chapter}{\numberline {1}Introduction}{3}% 
\contentsline {subsection}{\numberline {1.0.1}Main Contributions}{4}% 
\contentsline {chapter}{\numberline {2}Background}{7}% 
\contentsline {section}{\numberline {2.1}Linguistic Features}{8}% 
\contentsline {paragraph}{Polysemy}{8}% 
\contentsline {paragraph}{Part of Speech \textit {PoS}}{8}% 
\contentsline {subsection}{\numberline {2.1.1}Tokens and n-grams}{8}% 
\contentsline {paragraph}{Characters}{9}% 
\contentsline {paragraph}{Byte pair encodings}{9}% 
\contentsline {paragraph}{WordPiece tokenizer}{10}% 
\contentsline {section}{\numberline {2.2}Word Embeddings}{10}% 
\contentsline {paragraph}{Word-Embeddings}{11}% 
\contentsline {paragraph}{Distance:}{11}% 
\contentsline {paragraph}{Learning a distance}{12}% 
\contentsline {paragraph}{Exploiting the distributional structure of language:}{12}% 
\contentsline {paragraph}{Loss Objective}{13}% 
\contentsline {subsection}{\numberline {2.2.1}Static Word Embeddings}{16}% 
\contentsline {subsubsection}{Basic Model}{16}% 
\contentsline {subsubsection}{Word2Vec}{17}% 
\contentsline {subsubsection}{GloVe}{18}% 
\contentsline {subsection}{\numberline {2.2.2}Context Embeddings}{20}% 
\contentsline {subsubsection}{ELMo}{21}% 
\contentsline {subsubsection}{The Transformer Architecture}{22}% 
\contentsline {subsubsection}{BERT}{25}% 
\contentsline {paragraph}{The pipeline of BERT}{27}% 
\contentsline {subsubsection}{Other language models}{28}% 
\contentsline {section}{\numberline {2.3}GLUE benchmark dataset}{29}% 
\contentsline {subsubsection}{Inference Tasks}{29}% 
\contentsline {section}{\numberline {2.4}WordNet}{32}% 
\contentsline {paragraph}{Lexical matrix:}{32}% 
\contentsline {subsection}{\numberline {2.4.1}SemCor dataset}{35}% 
\contentsline {chapter}{\numberline {3}Related Work}{37}% 
\contentsline {section}{\numberline {3.1}Clustering for Synsets in Static Word Embeddings}{37}% 
\contentsline {paragraph}{The Chinese whispers algorithm}{38}% 
\contentsline {paragraph}{We will use a modified version of the Chinese Whispers algorithm.}{39}% 
\contentsline {section}{\numberline {3.2}Quantifying word sense disambiguation}{40}% 
\contentsline {section}{\numberline {3.3}Semantic subspace inside BERT}{41}% 
\contentsline {section}{\numberline {3.4}Syntactic subspace in BERT}{48}% 
\contentsline {section}{\numberline {3.5}Sentiment-subspace in BERT}{53}% 
\contentsline {chapter}{\numberline {4}Understanding the semantic subspace organization in BERT}{55}% 
\contentsline {section}{\numberline {4.1}On the Linear Separability of meaning within sampled BERT vectors}{56}% 
\contentsline {subsection}{\numberline {4.1.1}Experiment setup}{56}% 
\contentsline {subsection}{\numberline {4.1.2}Results}{58}% 
\contentsline {section}{\numberline {4.2}On the Clusterability of meaning within sampled BERT vectors}{61}% 
\contentsline {subsection}{\numberline {4.2.1}Experiment setup}{62}% 
\contentsline {subsection}{\numberline {4.2.2}Results}{65}% 
\contentsline {subsubsection}{Qualitative evaluation}{68}% 
\contentsline {section}{\numberline {4.3}Correlation between Part of Speech and Context within BERT}{71}% 
\contentsline {subsection}{\numberline {4.3.1}Experiment setup}{72}% 
\contentsline {subsection}{\numberline {4.3.2}Results}{74}% 
\contentsline {subsubsection}{Quantitative Evaluation}{74}% 
\contentsline {subsubsection}{Qualitative Evaluation}{74}% 
\contentsline {chapter}{\numberline {5}Exploiting subspace organization of semantics of BERT embeddings}{77}% 
\contentsline {section}{\numberline {5.1}BERNIE: Equalizing variances across BERT embeddings}{79}% 
\contentsline {subsection}{\numberline {5.1.1}BERNIE PoS}{80}% 
\contentsline {subsubsection}{Experiment setup}{80}% 
\contentsline {subsection}{\numberline {5.1.2}BERNIE Cluster}{83}% 
\contentsline {subsubsection}{Experiment setup}{83}% 
\contentsline {subsubsection}{Results}{85}% 
\contentsline {subsection}{\numberline {5.1.3}BERNIE Cluster with additional pre-training}{87}% 
\contentsline {subsection}{\numberline {5.1.4}Experiment setup}{87}% 
\contentsline {subsection}{\numberline {5.1.5}Results}{88}% 
\contentsline {chapter}{\numberline {6}Conclusion}{89}% 
\contentsline {subsection}{\numberline {6.0.1}Main Contributions}{89}% 
\contentsline {section}{\numberline {6.1}Future Work}{90}% 
\contentsline {paragraph}{Introducing the WiC benchmark:}{90}% 
\contentsline {paragraph}{Larger benchmarking dataset:}{91}% 
\contentsline {paragraph}{Linear subspace still not apparent:}{91}% 
\contentsline {paragraph}{Difficulty in quantifying semantics:}{91}% 
\contentsline {paragraph}{Limited coherence in lines of work}{91}% 
\contentsline {chapter}{\numberline {A}Background}{105}% 
\contentsline {section}{\numberline {A.1}Linguistic features}{105}% 
\contentsline {paragraph}{Other linguistic features}{105}% 
\contentsline {section}{\numberline {A.2}Gaussian Embeddings}{105}% 
\contentsline {section}{\numberline {A.3}Long Short-Term Memory}{108}% 
\contentsline {section}{\numberline {A.4}GLUE}{109}% 
\contentsline {subsubsection}{Single Sentence Tasks}{109}% 
\contentsline {subsubsection}{Similarity and paraphrase tasks}{109}% 
\contentsline {chapter}{\numberline {B}Other lines of work}{111}% 
\contentsline {section}{\numberline {B.1}Clustering}{111}% 
\contentsline {paragraph}{Affinity Propagation}{111}% 
\contentsline {paragraph}{DBScan}{111}% 
\contentsline {paragraph}{MeanShift}{111}% 
\contentsline {section}{\numberline {B.2}Static embeddings and context embeddings}{112}% 
\contentsline {section}{\numberline {B.3}Metric Learning and Disentanglement}{112}% 
\contentsline {subsubsection}{Non-Deep Metric Learning}{114}% 
\contentsline {paragraph}{Dimensionality Reduction}{115}% 
\contentsline {subsubsection}{Deep Metric Learning}{115}% 
\contentsline {subsection}{\numberline {B.3.1}Embeddings for translation}{118}% 
\contentsline {section}{\numberline {B.4}Compressing the non-lexical out}{119}% 
\contentsline {subsubsection}{Motivation}{119}% 
\contentsline {subsubsection}{Experiment setup}{119}% 
\contentsline {subsubsection}{Results}{119}% 
\contentsline {chapter}{\numberline {C}More Results}{121}% 
\contentsline {section}{\numberline {C.1}Finding the best clustering model}{121}% 
\contentsline {subsubsection}{Quantiative Evaluation}{121}% 
\contentsline {subsubsection}{Qualitative Evaluation}{122}% 
\contentsline {section}{\numberline {C.2}Nominalised Verbs}{125}% 
\contentsline {chapter}{\numberline {D}Applications}{127}% 
\contentsline {section}{\numberline {D.1}Using embeddings in other domains}{127}% 
\contentsline {section}{\numberline {D.2}Using embeddings in translations}{127}% 
\contentsline {subsubsection}{Word2Vec}{130}% 
