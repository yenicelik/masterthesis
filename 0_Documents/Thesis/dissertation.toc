\contentsline {chapter}{\numberline {1}Introduction}{3}% 
\contentsline {subsection}{\numberline {1.0.1}Main Contributions}{4}% 
\contentsline {chapter}{\numberline {2}Background}{5}% 
\contentsline {section}{\numberline {2.1}Linguistic Features}{6}% 
\contentsline {paragraph}{Polysemy}{6}% 
\contentsline {paragraph}{Part of Speech \textit {PoS}}{6}% 
\contentsline {paragraph}{Other linguistic features}{7}% 
\contentsline {subsection}{\numberline {2.1.1}Tokens and n-grams}{7}% 
\contentsline {paragraph}{Character}{8}% 
\contentsline {paragraph}{Byte pair encodings}{8}% 
\contentsline {paragraph}{WordPiece tokenizer}{9}% 
\contentsline {section}{\numberline {2.2}Word Embeddings}{9}% 
\contentsline {paragraph}{Word-Embeddings}{9}% 
\contentsline {paragraph}{Distance:}{10}% 
\contentsline {paragraph}{Learning a distance}{10}% 
\contentsline {paragraph}{Exploiting the distributional structure of language:}{11}% 
\contentsline {paragraph}{Loss Objective}{12}% 
\contentsline {subsection}{\numberline {2.2.1}Static Word Embeddings}{14}% 
\contentsline {subsubsection}{Basic Model}{14}% 
\contentsline {subsubsection}{Word2Vec}{15}% 
\contentsline {subsubsection}{GloVe}{16}% 
\contentsline {subsubsection}{Gaussian Embeddings}{17}% 
\contentsline {subsection}{\numberline {2.2.2}Context Embeddings}{21}% 
\contentsline {subsubsection}{ELMo}{22}% 
\contentsline {subsubsection}{The Transformer Architecture}{24}% 
\contentsline {subsubsection}{BERT}{26}% 
\contentsline {subsubsection}{GPT and GPT-2}{29}% 
\contentsline {paragraph}{Unsupervised pre-training}{29}% 
\contentsline {paragraph}{Supervised fine-tuning}{30}% 
\contentsline {section}{\numberline {2.3}GLUE benchmark dataset}{32}% 
\contentsline {subsubsection}{Single Sentence Tasks}{32}% 
\contentsline {subsubsection}{Similarity and paraphrase tasks}{33}% 
\contentsline {subsubsection}{Inference Tasks}{34}% 
\contentsline {section}{\numberline {2.4}WordNet}{36}% 
\contentsline {paragraph}{Lexical matrix:}{36}% 
\contentsline {subsection}{\numberline {2.4.1}SemCor dataset}{39}% 
\contentsline {section}{\numberline {2.5}Metric Learning and Disentanglement}{40}% 
\contentsline {subsubsection}{Non-Deep Metric Learning}{42}% 
\contentsline {paragraph}{Dimensionality Reduction}{44}% 
\contentsline {subsubsection}{Deep Metric Learning}{44}% 
\contentsline {chapter}{\numberline {3}Related Work}{49}% 
\contentsline {section}{\numberline {3.1}Clustering for Semantics}{49}% 
\contentsline {paragraph}{Affinity Propagation}{50}% 
\contentsline {paragraph}{DBScan}{50}% 
\contentsline {paragraph}{MeanShift}{50}% 
\contentsline {paragraph}{The Chinese Whispers}{50}% 
\contentsline {section}{\numberline {3.2}Structure inside BERT}{51}% 
\contentsline {subsection}{\numberline {3.2.1}Other methods}{52}% 
\contentsline {subsubsection}{Generating "static" word-embeddings through contextual embeddings}{52}% 
\contentsline {subsection}{\numberline {3.2.2}Attention mechanism}{55}% 
\contentsline {subsection}{\numberline {3.2.3}From token-vectors to word-vectors}{55}% 
\contentsline {subsection}{\numberline {3.2.4}Bias in BERT vectors}{56}% 
\contentsline {subsection}{\numberline {3.2.5}Change of meanings over time}{56}% 
\contentsline {subsection}{\numberline {3.2.6}Clinical concept extration}{56}% 
\contentsline {subsection}{\numberline {3.2.7}Discovering for semantics}{56}% 
\contentsline {subsection}{\numberline {3.2.8}Embeddings for translation}{56}% 
\contentsline {chapter}{\numberline {4}Understanding the semantic subspace organization in BERT}{59}% 
\contentsline {section}{\numberline {4.1}On the Linear Separability of meaning within sampled BERT vectors}{60}% 
\contentsline {subsection}{\numberline {4.1.1}Experiment setup}{60}% 
\contentsline {subsection}{\numberline {4.1.2}Results}{62}% 
\contentsline {section}{\numberline {4.2}On the Clusterability of meaning within sampled BERT vectors}{65}% 
\contentsline {subsection}{\numberline {4.2.1}Experiment setup}{66}% 
\contentsline {subsection}{\numberline {4.2.2}Results}{69}% 
\contentsline {subsubsection}{Qualitative evaluation}{72}% 
\contentsline {section}{\numberline {4.3}Correlation between Part of Speech and Context within BERT}{74}% 
\contentsline {subsection}{\numberline {4.3.1}Experiment setup}{75}% 
\contentsline {subsection}{\numberline {4.3.2}Results}{77}% 
\contentsline {subsubsection}{Quantitative Evaluation}{77}% 
\contentsline {subsubsection}{Qualitative Evaluation}{77}% 
\contentsline {chapter}{\numberline {5}Exploiting subspace organization of semantics of BERT embeddings}{81}% 
\contentsline {section}{\numberline {5.1}BERnie: Making the variances across BERT embeddings more equal}{83}% 
\contentsline {subsection}{\numberline {5.1.1}BERnie PoS}{83}% 
\contentsline {subsubsection}{Experiment setup}{83}% 
\contentsline {subsection}{\numberline {5.1.2}BERnie Meaning}{86}% 
\contentsline {subsubsection}{Experiment setup}{86}% 
\contentsline {subsubsection}{Results}{88}% 
\contentsline {subsection}{\numberline {5.1.3}BERnie Meaning with additional pre-training}{90}% 
\contentsline {subsection}{\numberline {5.1.4}Experiment setup}{90}% 
\contentsline {subsection}{\numberline {5.1.5}Results}{90}% 
\contentsline {section}{\numberline {5.2}Compressing the non-lexical out}{91}% 
\contentsline {subsubsection}{Motivation}{91}% 
\contentsline {subsubsection}{Experiment setup}{91}% 
\contentsline {subsubsection}{Results}{92}% 
\contentsline {chapter}{\numberline {6}Conclusion}{93}% 
\contentsline {chapter}{\numberline {A}More Results}{103}% 
\contentsline {section}{\numberline {A.1}Finding the best clustering model}{103}% 
\contentsline {subsubsection}{Quantiative Evaluation}{103}% 
\contentsline {subsubsection}{Qualitative Evaluation}{104}% 
\contentsline {section}{\numberline {A.2}Frozen Verbs}{107}% 
\contentsline {chapter}{\numberline {B}Using word vectors in translation}{109}% 
\contentsline {subsubsection}{Word2Vec}{111}% 
