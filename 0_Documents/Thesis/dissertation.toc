\contentsline {chapter}{\numberline {1}Introduction}{1}% 
\contentsline {paragraph}{Our main contributions are the following:}{2}% 
\contentsline {chapter}{\numberline {2}Background}{4}% 
\contentsline {section}{\numberline {2.1}Linguistic Features}{5}% 
\contentsline {paragraph}{Polysemy}{5}% 
\contentsline {paragraph}{Part of Speech \textit {PoS}}{5}% 
\contentsline {subsection}{\numberline {2.1.1}Tokens and n-grams}{5}% 
\contentsline {paragraph}{Byte pair encodings}{6}% 
\contentsline {paragraph}{WordPiece tokenizer}{7}% 
\contentsline {section}{\numberline {2.2}Word Embeddings}{8}% 
\contentsline {subsection}{\numberline {2.2.1}Static Word Embeddings}{12}% 
\contentsline {subsubsection}{Basic Model}{13}% 
\contentsline {subsubsection}{Word2Vec}{14}% 
\contentsline {subsubsection}{GloVe}{15}% 
\contentsline {subsection}{\numberline {2.2.2}Contextual Word Embeddings}{16}% 
\contentsline {subsection}{\numberline {2.2.3}ELMo}{18}% 
\contentsline {subsection}{\numberline {2.2.4}The Transformer Architecture}{19}% 
\contentsline {subsection}{\numberline {2.2.5}BERT: Bidirectional Encoder Representations from Transformers}{21}% 
\contentsline {paragraph}{The pipeline of BERT}{23}% 
\contentsline {subsubsection}{Other language models}{24}% 
\contentsline {section}{\numberline {2.3}GLUE benchmark dataset}{25}% 
\contentsline {subsubsection}{Inference Tasks}{25}% 
\contentsline {section}{\numberline {2.4}WordNet}{27}% 
\contentsline {paragraph}{Lexical matrix:}{27}% 
\contentsline {subsection}{\numberline {2.4.1}SemCor dataset}{29}% 
\contentsline {chapter}{\numberline {3}Related Work}{30}% 
\contentsline {section}{\numberline {3.1}Creating Synsets from Static Word Embeddings}{30}% 
\contentsline {section}{\numberline {3.2}Quantifying word sense disambiguation}{32}% 
\contentsline {section}{\numberline {3.3}Semantic subspace inside BERT}{33}% 
\contentsline {subsection}{\numberline {3.3.1}Geometric analysis}{33}% 
\contentsline {subsection}{\numberline {3.3.2}Probing for semantics}{36}% 
\contentsline {subsection}{\numberline {3.3.3}Word sense disambiguation in specialized domains}{39}% 
\contentsline {section}{\numberline {3.4}Syntactic subspace in BERT}{40}% 
\contentsline {subsection}{\numberline {3.4.1}Extracting Parse-Trees}{40}% 
\contentsline {subsection}{\numberline {3.4.2}Subspace representing part-of-speech, verbs and arguments}{43}% 
\contentsline {section}{\numberline {3.5}Sentiment subspace in BERT}{44}% 
\contentsline {subsection}{\numberline {3.5.1}Bias}{44}% 
\contentsline {subsection}{\numberline {3.5.2}Change of language over time}{45}% 
\contentsline {chapter}{\numberline {4}How is semantics captured through contextual word embeddings produced by BERT?}{46}% 
\contentsline {section}{\numberline {4.1}On the Linear Separability of meaning within sampled BERT vectors}{47}% 
\contentsline {subsection}{\numberline {4.1.1}Experiment setup}{47}% 
\contentsline {subsection}{\numberline {4.1.2}Results}{50}% 
\contentsline {section}{\numberline {4.2}On the Clusterability of meaning within sampled BERT vectors}{52}% 
\contentsline {subsection}{\numberline {4.2.1}Experiment setup}{53}% 
\contentsline {subsection}{\numberline {4.2.2}Results}{58}% 
\contentsline {subsubsection}{Qualitative evaluation}{60}% 
\contentsline {section}{\numberline {4.3}Correlation between Part of Speech and Context within BERT}{64}% 
\contentsline {subsection}{\numberline {4.3.1}Experiment setup}{65}% 
\contentsline {subsection}{\numberline {4.3.2}Results}{66}% 
\contentsline {subsubsection}{Quantitative Evaluation}{66}% 
\contentsline {subsubsection}{Qualitative Evaluation}{67}% 
\contentsline {chapter}{\numberline {5}Exploiting subspace organization of semantics of BERT embeddings}{69}% 
\contentsline {section}{\numberline {5.1}BERNIE: Equalizing variances across BERT embeddings}{71}% 
\contentsline {subsection}{\numberline {5.1.1}BERNIE PoS}{72}% 
\contentsline {subsubsection}{Experiment setup}{72}% 
\contentsline {subsection}{\numberline {5.1.2}BERNIE Cluster}{74}% 
\contentsline {subsubsection}{Experiment setup}{75}% 
\contentsline {subsubsection}{Results}{76}% 
\contentsline {subsection}{\numberline {5.1.3}BERNIE Cluster with additional pre-training}{78}% 
\contentsline {subsection}{\numberline {5.1.4}Experiment setup}{78}% 
\contentsline {subsection}{\numberline {5.1.5}Results}{78}% 
\contentsline {chapter}{\numberline {6}Conclusion}{80}% 
\contentsline {section}{\numberline {6.1}Main Contributions}{80}% 
\contentsline {section}{\numberline {6.2}Future Work}{81}% 
\contentsline {paragraph}{Introducing the WiC benchmark:}{81}% 
\contentsline {paragraph}{Larger benchmarking dataset:}{82}% 
\contentsline {paragraph}{Semantic subspace not apparent:}{82}% 
\contentsline {paragraph}{Formalizing semantics:}{82}% 
\contentsline {paragraph}{Limited coherence in lines of work}{82}% 
\contentsline {chapter}{\numberline {A}Background}{95}% 
\contentsline {section}{\numberline {A.1}Linguistic features}{95}% 
\contentsline {paragraph}{Other linguistic features}{95}% 
\contentsline {section}{\numberline {A.2}Gaussian Embeddings}{95}% 
\contentsline {section}{\numberline {A.3}Long Short-Term Memory}{98}% 
\contentsline {section}{\numberline {A.4}GLUE}{99}% 
\contentsline {subsection}{\numberline {A.4.1}Single Sentence Tasks}{99}% 
\contentsline {subsection}{\numberline {A.4.2}Similarity and paraphrase tasks}{99}% 
\contentsline {subsection}{\numberline {A.4.3}Inference tasks}{101}% 
\contentsline {section}{\numberline {A.5}WordNet}{102}% 
\contentsline {chapter}{\numberline {B}Other lines of work}{103}% 
\contentsline {section}{\numberline {B.1}Clustering}{103}% 
\contentsline {paragraph}{Affinity Propagation}{103}% 
\contentsline {paragraph}{DBScan}{103}% 
\contentsline {paragraph}{MeanShift}{103}% 
\contentsline {section}{\numberline {B.2}Static word embeddings and contextual word embeddings}{104}% 
\contentsline {section}{\numberline {B.3}Metric Learning and Disentanglement}{104}% 
\contentsline {subsubsection}{Non-Deep Metric Learning}{106}% 
\contentsline {subsubsection}{Deep Metric Learning}{107}% 
\contentsline {subsection}{\numberline {B.3.1}Embeddings for translation}{110}% 
\contentsline {chapter}{\numberline {C}More Results}{112}% 
\contentsline {section}{\numberline {C.1}Linear Separability}{112}% 
\contentsline {section}{\numberline {C.2}Finding the best clustering model}{112}% 
\contentsline {subsubsection}{Quantiative Evaluation}{112}% 
\contentsline {subsubsection}{Qualitative Evaluation}{113}% 
\contentsline {subsection}{\numberline {C.2.1}More qualitative evaluation}{119}% 
\contentsline {section}{\numberline {C.3}Nominalised Verbs}{119}% 
\contentsline {chapter}{\numberline {D}Applications}{120}% 
\contentsline {section}{\numberline {D.1}Using embeddings in other domains}{120}% 
\contentsline {section}{\numberline {D.2}Using embeddings in translations}{120}% 
